{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# HIT Files: 119\n",
      "# Passed Test: 43 / 76 = 56.58 %\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "task_dir = '/Users/ethanperez/research/ParlAI/parlai/mturk/core/run_data/live/context_evaluator_'\n",
    "\n",
    "### RACE: Unfiltered Workers\n",
    "# prompt_type, task_id = 'question', '1553982706'  # Q-only\n",
    "# prompt_type, task_id = 'context_question', 1553790696  # TFIDF\n",
    "# prompt_type, task_id = 'context_question', 1553901953  # FastText\n",
    "### RACE: Filtered Workers\n",
    "# prompt_type, task_id = 'question', '1554052233'  # Q-only\n",
    "# prompt_type, task_id = 'quote and question', 1554006689  # TFIDF-QA\n",
    "# prompt_type, task_id = 'quote and question', 1554130485  # TFIDF-A\n",
    "# prompt_type, task_id = 'quote and question', 1554069931  # Cross-Ranker\n",
    "# prompt_type, task_id = 'quote and question', 1554072277  # Predicting Search\n",
    "# prompt_type, task_id = 'quote and question', 1554132868  # Predicting ΔP(A)\n",
    "\n",
    "### RACE Test\n",
    "## Convinced\n",
    "# prompt_type, task_id = 'quote and question', 1556671432  # TFIDF-QA\n",
    "# prompt_type, task_id = 'quote and question', 1556725767  # TFIDF-A\n",
    "# prompt_type, task_id = 'quote and question', 1556739336  # BoW-A\n",
    "# prompt_type, task_id = 'quote and question', 1556756789  # ΔP(A) (race.m=sl-sents.i.best.e)\n",
    "# prompt_type, task_id = 'quote and question', 1556809031  # P(A)\n",
    "# prompt_type, task_id = 'quote and question', 1556832343  # Cross-Ranker (Almost complete)\n",
    "# prompt_type, task_id = 'quote and question', 1556892630  # Cross-Ranker\n",
    "## Acc on Summary\n",
    "# prompt_type, task_id = 'passage and question', 1555823963  # Full Passage\n",
    "# prompt_type, task_id = 'quotes and question', 1555946909  # FastText\n",
    "# prompt_type, task_id = 'quotes and question', 1555952058  # Cross-Ranker (Best Epoch. 6-10 sentence incorrectly placed at end)\n",
    "# prompt_type, task_id = 'quotes and question', 1556939750  # Predicting ΔP(A)\n",
    "# prompt_type, task_id = 'quotes and question', 1556977072  # Predicting P(A)\n",
    "prompt_type, task_id = 'quotes and question', 1556987177  # Predicting Search\n",
    "dataset = 'race'\n",
    "\n",
    "### DREAM\n",
    "# prompt_type, task_id = 'question', 1554582693  # Q-only\n",
    "# prompt_type, task_id = 'quote and question', 1554596686  # TFIDF-QA\n",
    "# prompt_type, task_id = 'quote and question', 1554587404  # TFIDF-A\n",
    "# prompt_type, task_id = 'quote and question', 1554662280  # BoW-A\n",
    "# prompt_type, task_id = 'quote and question', 1556670413  # Bi-Ranker\n",
    "# prompt_type, task_id = 'quote and question', 1554675304  # Cross-Ranker\n",
    "# prompt_type, task_id = 'quote and question', 1554685131  # SL\n",
    "# prompt_type, task_id = 'quote and question', 1554692472  # SL-Sents\n",
    "# prompt_type, task_id = 'quote and question', 1554729998  # SL-Sents-Influence\n",
    "# prompt_type, task_id = 'quote and question', 1555333992  # SL-Theory-of-Mind\n",
    "## All answers at once\n",
    "# prompt_type, task_id = 'question, answers, and quotes', 1555707929  # TFIDF(O): 64.21%: (Less filter / no feedback)\n",
    "# prompt_type, task_id = 'question, answers, and quotes', 1555722489  # Search: 65.38%: (Less filter / no feedback)\n",
    "# prompt_type, task_id = 'question and quotes', 1555789302  # SL: 75.17% (4/5 filter)\n",
    "# prompt_type, task_id = 'question and quotes', 1555812443  # SL: 79.32% Actually: quotes and question (4/5 filter)\n",
    "# prompt_type, task_id = 'passage and question', 1555804551  # Full Passage: 92.97%\n",
    "# prompt_type, task_id = 'quotes and question', 1555823257  # FastText (5/5 filter) (77.33%)\n",
    "# prompt_type, task_id = 'quotes and question', 1555946647  # RACE Cross-Ranker (4 sentences incorrectly placed at end) (80.84%)\n",
    "# prompt_type, task_id = 'quotes and question', 1556727396  # Cross-Ranker\n",
    "# prompt_type, task_id = 'quotes and question', 1556740293  # Bi-Ranker\n",
    "# prompt_type, task_id = 'quotes and question', 1556757043  # TFIDF-A\n",
    "# prompt_type, task_id = 'quotes and question', 1556811067  # TFIDF-QA\n",
    "# prompt_type, task_id = 'quotes and question', 1556832115  # Predicting Search\n",
    "# prompt_type, task_id = 'quotes and question', 1556892896  # Predicting P(A)\n",
    "# prompt_type, task_id = 'quotes and question', 1556938429  # Predicting ΔP(A)\n",
    "# dataset = 'dream'\n",
    "\n",
    "\n",
    "split = None  # 'middle', 'high', None\n",
    "\n",
    "\n",
    "# Set useful variables\n",
    "task_dir += str(task_id)\n",
    "if dataset != 'race':\n",
    "    split = None\n",
    "num_options = 3 if dataset == 'dream' else 4\n",
    "options = ['A', 'B', 'C', 'D'][:num_options]\n",
    "debate_mode_to_option = {'Ⅰ': 'A', 'Ⅱ': 'B', 'Ⅲ': 'C', 'Ⅳ': 'D', 'ⅰ': 'A', 'ⅱ': 'B', 'ⅲ': 'C', 'ⅳ': 'D', None: None}\n",
    "question_type_labels = ['a', 'c', 'l', 'm', 's'] if dataset == 'dream' else ['a', 'b', 'c', 'd', 'e']\n",
    "\n",
    "# Read HIT data\n",
    "print('# HIT Files:', len(os.listdir(task_dir)))\n",
    "hit_results = []\n",
    "num_passed_test = 0\n",
    "for hit_dir in os.listdir(task_dir):\n",
    "    if hit_dir.startswith('o_'):\n",
    "        continue\n",
    "    num_passed_test += 1\n",
    "    with open(os.path.join(task_dir, hit_dir, 'custom/data.json'), 'r') as file:\n",
    "        hit_results.append(json.load(file))\n",
    "        file.close()\n",
    "if len(os.listdir(task_dir)) != num_passed_test:\n",
    "    num_total_tested = len(os.listdir(task_dir)) - num_passed_test\n",
    "    print('# Passed Test:', num_passed_test, '/', num_total_tested, '=', round((100. * num_passed_test) / num_total_tested, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Time: 1176.2 | Acc: 60 | Max Freq: 45.0 | Rate: 10 | Feedback: I guess try not of have those quotes where, well it was hard because it could have been something else? | Quote Rating: 5 | Quote Desc: Not sure what you mean by that? The original quotes about which I had to pick an answer, were fine. Your explanations of my incorrect answers, left something to be desired, in my mind. I only felt I had one wrong and that was because I didn't read all the options (all of the above). Thank you for the opportunity! FUN \n",
      "| Time: 1636.9 | Acc: 60 | Max Freq: 35.0 | Rate: 10 | Feedback: By making some of the passage more clearer and easier to find the answers. | Quote Rating: 8 | Quote Desc: They gave helpful hints to the answers in most cases but some were hard to understand.\n",
      "| Time: 1695.7 | Acc: 75 | Max Freq: 25.0 | Rate: 8 | Feedback: by giving more detailed passage  | Quote Rating: 7 | Quote Desc: helpful but can be better\n",
      "| Time: 1636.1 | Acc: 85 | Max Freq: 40.0 | Rate: 8 | Feedback: some too abstract | Quote Rating: 9 | Quote Desc: fun\n",
      "| Time: 600.4 | Acc: 70 | Max Freq: 40.0 | Rate: 10 | Feedback: if its possible add an all of the above tab to certain questions | Quote Rating: 6 | Quote Desc: slightly informative but sporadic\n",
      "| Time: 784.7 | Acc: 50 | Max Freq: 35.0 | Rate: 10 | Feedback: Make the passages clearer | Quote Rating: 3 | Quote Desc: confusing\n",
      "| Time: 628.7 | Acc: 45 | Max Freq: 30.0 | Rate: 6 | Feedback: Was fine - just very challenging today | Quote Rating: 1 | Quote Desc: Very difficult to determine\n",
      "| Time: 865.9 | Acc: 85 | Max Freq: 40.0 | Rate: 8 | Feedback: Some of the passages definitely need to be clearer. | Quote Rating: 5 | Quote Desc: The English is very poor in most of them, to the point where it made no sense in some cases, meaning I was unable to answer the question correctly. (I'm really hoping this doesn't mean I won't be paid, but I want to be honest so you can achieve your goal.) Question 9 was particularly bad in this regard. )\n",
      "| Time: 1571.5 | Acc: 70 | Max Freq: 30.0 | Rate: 10 | Feedback: I cant think of anything | Quote Rating: 4 | Quote Desc: Broken, missing context\n",
      "| Time: 1059.4 | Acc: 80 | Max Freq: 35.0 | Rate: 10 | Feedback: 1) stop the display from shaking (I don't know if you know it, but your dialog box is constantly vibrating up and down. (2) Provide a progress bar, it is very helpful and reassuring to know where you are in a task so we can manage our time.  | Quote Rating: 8 | Quote Desc: For the most part, I was able to discern the answer from the quotes. My only complaint is that the quotes are very \"choppy\" and can be difficult to read. However, I can see how you could get used to reading them and become very efficient at it. Of the few questions I got wrong, I only didn't understand or agree with the \"correct\" answer of one of them; the others I could see the reasoning of and why my answer was wrong. All in all, it was a fun activity: kind of like being a detective or cryptographer. Thanks and good luck with your research. \n",
      "| Time: 768.9 | Acc: 60 | Max Freq: 40.0 | Rate: 10 | Feedback: Let the user give a reason for their answer when the answer is chosen as incorrect. | Quote Rating: 8 | Quote Desc: They were small snippets or condensed paragraphs that were sometimes difficult to understand due to spelling or grammatical mistakes.\n",
      "| Time: 474.7 | Acc: 85 | Max Freq: 40.0 | Rate: 5 | Feedback: Alternate the size of passages more, getting multiple large paragraphs in a row is frustrating | Quote Rating: 7 | Quote Desc: At some points really jumbled with parts omitted which made it more difficult to understand and try and figure out the underlying meaning.\n",
      "| Time: 1115.6 | Acc: 70 | Max Freq: 30.0 | Rate: 9 | Feedback: Instead of choosing an answer from a separate drop-down list, it might be easier to directly click on the answer. | Quote Rating: 7 | Quote Desc: Most of them were helpful, but some were missing key information needed to answer questions.\n",
      "| Time: 833.7 | Acc: 70 | Max Freq: 35.0 | Rate: 10 | Feedback: it's good | Quote Rating: 6 | Quote Desc: some were helpful and on topic, some weren't\n",
      "| Time: 505.0 | Acc: 25 | Max Freq: 35.0 | Rate: 2 | Feedback: Make the passages more clear | Quote Rating: 3 | Quote Desc: Confusing and needed more information \n",
      "| Time: 868.5 | Acc: 75 | Max Freq: 35.0 | Rate: 8 | Feedback: Make some of the passages a little more clear. | Quote Rating: 8 | Quote Desc: I found some interesting to have to guess what was all being said when it was not provided.\n",
      "| Time: 838.9 | Acc: 80 | Max Freq: 40.0 | Rate: 8 | Feedback: I'm not sure. I like the task how it is. | Quote Rating: 8 | Quote Desc: Pretty clear if you read them really carefully\n",
      "| Time: 592.4 | Acc: 75 | Max Freq: 40.0 | Rate: 9 | Feedback: N/A | Quote Rating: 9 | Quote Desc: Seemed like they were taken from longer passages\n",
      "| Time: 993.8 | Acc: 80 | Max Freq: 40.0 | Rate: 6 | Feedback: I have no recommendations, other than providing more relevant information in the passages. | Quote Rating: 4 | Quote Desc: Some were very straightforward in helping answer questions, but others were completely irrelevant\n",
      "| Time: 725.4 | Acc: 55 | Max Freq: 35.0 | Rate: 6 | Feedback: Nothing really, any failure was on my end | Quote Rating: 4 | Quote Desc: Fairly accurate but some seemed to be able to be taken multiple ways and it marked my answer incorrect even though I thought the answer could be perfectly answered by two of the options but I understand your intentions\n",
      "| Time: 832.6 | Acc: 85 | Max Freq: 35.0 | Rate: 7 | Feedback: I find the scrolling a bit disorienting, but it may be the size of the frame on this web page. | Quote Rating: 7 | Quote Desc: Some were skeletal or misleading, but most of them had ample clues.\n",
      "| Time: 626.5 | Acc: 65 | Max Freq: 35.0 | Rate: 10 | Feedback: let us know how many questions there are in total.  Let us know that there are some answers we might have to make guesses.   I enjoyed this a lot.   | Quote Rating: 8 | Quote Desc: very interesting but some didn't contain enough information to answer the questions that were asked after.  \n",
      "| Time: 495.2 | Acc: 40 | Max Freq: 40.0 | Rate: 10 | Feedback: SOme of the answers seemed like they couldn't possibly be right.  | Quote Rating: 3 | Quote Desc: Some were very clear and some were very difficult to understand. \n",
      "| Time: 968.3 | Acc: 65 | Max Freq: 30.0 | Rate: 8 | Feedback: Add slightly more details. | Quote Rating: 3 | Quote Desc: Many of them were fine, but some were far too vague to make a correct answer anything other than a guess.\n",
      "| Time: 1291.9 | Acc: 85 | Max Freq: 50.0 | Rate: 6 | Feedback: The task is good the way it is. | Quote Rating: 8 | Quote Desc: vague.\n",
      "| Time: 577.5 | Acc: 85 | Max Freq: 40.0 | Rate: 10 | Feedback: I really liked the format!  | Quote Rating: 6 | Quote Desc: I would say they were partial, because it seemed like some of the context was missing from them. \n",
      "| Time: 317.3 | Acc: 70 | Max Freq: 35.0 | Rate: 10 | Feedback: no improvements needed | Quote Rating: 9 | Quote Desc: easy to understand\n",
      "| Time: 2493.9 | Acc: 60 | Max Freq: 45.0 | Rate: 3 | Feedback: shorten it  | Quote Rating: 9 | Quote Desc: informative\n",
      "| Time: 860.6 | Acc: 75 | Max Freq: 30.0 | Rate: 10 | Feedback: Making the quotes longer | Quote Rating: 10 | Quote Desc: A little detailed\n",
      "| Time: 480.4 | Acc: 55 | Max Freq: 35.0 | Rate: 5 | Feedback: The vagueness of the quotes have to be reduced. | Quote Rating: 6 | Quote Desc: Some of them were just too vague to understand\n",
      "| Time: 944.4 | Acc: 45 | Max Freq: 30.0 | Rate: 8 | Feedback: I WOULD LIKE TO TAKE IT MULTIPLE TIMES TO SEE IF I IMPROVED | Quote Rating: 6 | Quote Desc: VAGUE IN SOME, CONFUSING IN SOME, INCOMPLETE IN SOME, CLEAR IN SOME\n",
      "| Time: 359.8 | Acc: 55 | Max Freq: 35.0 | Rate: 0 | Feedback: more moeney | Quote Rating: 2 | Quote Desc: random\n",
      "| Time: 1078.4 | Acc: 65 | Max Freq: 35.0 | Rate: 10 | Feedback: I'm not sure. I enjoyed thinking about these.  | Quote Rating: 7 | Quote Desc: Snippets\n",
      "| Time: 761.9 | Acc: 55 | Max Freq: 45.0 | Rate: 7 | Feedback: This round lacked logic compared to the previous round. | Quote Rating: 3 | Quote Desc: I could not make the assumptions you need for answers from most of the passages.\n",
      "| Time: 844.6 | Acc: 65 | Max Freq: 35.0 | Rate: 10 | Feedback: I think everything was as good as it could be. | Quote Rating: 6 | Quote Desc: I thought they were a little bit scattered, there was quite a bit of useless information and quite a bit that was needed but not included.\n",
      "| Time: 752.9 | Acc: 80 | Max Freq: 30.0 | Rate: 10 | Feedback: I think it's fine the way it is. | Quote Rating: 8 | Quote Desc: They were helpful in most cases but lacked details in some.\n",
      "| Time: 495.5 | Acc: 80 | Max Freq: 35.0 | Rate: 7 | Feedback: More relevant quotes on ones like those I missed. | Quote Rating: 7 | Quote Desc: Kind of random. Sometime effective.\n",
      "REJECTED: 0\n",
      "INCOMPLETE: 6\n",
      "VALID: 37\n",
      "Median Question Duration: 34.661\n",
      "Mean Question Duration: 38.74482601351352\n",
      "Min/Median/Mean/Max Worker Duration: 5.29 / 13.89 / 15.11 / 41.56\n",
      "Min/Median/Mean/Max Good Worker Durations: 7.91 / 14.34 / 15.37 / 28.26\n",
      "Median Worker Accuracy: 0.7\n",
      "Median Max Response Freq: 0.35\n",
      "Quote Rating: | Mean: 6.16 | Median: 7.0 | Std: 2.26\n"
     ]
    }
   ],
   "source": [
    "num_valid_hits, num_rejected_hits, num_incomplete_hits = 0, 0, 0\n",
    "metrics = {}\n",
    "task_ratings = {i: 0 for i in range(11)}\n",
    "quote_ratings = []\n",
    "durations = []\n",
    "worker_durations = []\n",
    "accuracy_by_worker = {}\n",
    "max_response_freqs = []\n",
    "worker_ids = []\n",
    "hits_by_qid = {}\n",
    "\n",
    "for hit_result in hit_results:\n",
    "    if ((len(hit_result['reject_reasons']) > 0) or\n",
    "        (len(hit_result['block_reasons']) > 0)):\n",
    "        num_rejected_hits += 1\n",
    "        print(hit_result['worker_id'], hit_result['assignment_id'],\n",
    "              '| reject_reasons:', hit_result['reject_reasons'],\n",
    "              '| block_reasons:', hit_result['block_reasons'],\n",
    "              '| bonus_reasons: ' + str(hit_result['bonus_reasons']) if 'bonus_reasons' in hit_result else '')\n",
    "    elif hit_result['feedback'] is None:\n",
    "        num_incomplete_hits += 1\n",
    "        continue\n",
    "    \n",
    "    worker_ids.append(hit_result['worker_id'])\n",
    "    num_valid_hits += 1\n",
    "    if (hit_result['task_rating'] is not None) and (hit_result['task_rating'].isdigit()):\n",
    "        task_ratings[int(hit_result['task_rating'])] += 1\n",
    "    if (hit_result.get('quote_rating') is not None) and (hit_result['quote_rating'].isdigit()):\n",
    "        quote_ratings.append(int(hit_result['quote_rating']))\n",
    "    for qtype, qtype_accuracy in hit_result['accuracy'].items():\n",
    "        accuracy_by_worker[qtype] = accuracy_by_worker.get(qtype, []) + [qtype_accuracy]\n",
    "    \n",
    "    hit_durations = []\n",
    "    response_option_counts = {option: 0 for option in options}\n",
    "    responses = []\n",
    "    for prompt in hit_result['data']:\n",
    "        qid = prompt['sample']['qid']\n",
    "        if (split is not None) and (split not in qid):\n",
    "            continue\n",
    "        hits_by_qid[qid] = hits_by_qid.get(qid, [])\n",
    "        hits_by_qid[qid].append(prompt)\n",
    "        model_stance = debate_mode_to_option[prompt['sample']['debate_mode']]\n",
    "        answer = prompt['sample']['eval_labels'][0]\n",
    "        human_correct = (prompt['response'] == answer)\n",
    "        assert answer in options, 'Answer must be in options.'\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if qid not in metrics:\n",
    "            metrics[qid] = {\n",
    "                option: {\n",
    "                    'num': 0,\n",
    "                    'num_correct': 0,\n",
    "                    'num_correct_debate_mode': 0,\n",
    "                    'num_incorrect_debate_mode': 0,\n",
    "                    'num_correct_with_correct_debate_mode': 0,\n",
    "                    'num_correct_with_incorrect_debate_mode': 0,\n",
    "                    'num_debate_mode_responses': 0,\n",
    "                }\n",
    "                for option in ([None] if model_stance is None else options)\n",
    "            }\n",
    "            for qtype in question_type_labels:\n",
    "                metrics[qid][qtype] = {\n",
    "                    'num': 0,\n",
    "                    'num_correct': 0,\n",
    "                }\n",
    "            metrics[qid]['answer'] = answer\n",
    "        for qtype in set(''.join(prompt['sample'].get('question_type_labels', []))):\n",
    "            qtype = qtype.lower()\n",
    "            metrics[qid][qtype]['num'] += 1\n",
    "            metrics[qid][qtype]['num_correct'] += human_correct\n",
    "        prompt_metrics = metrics[qid][model_stance]\n",
    "        prompt_metrics['num'] += 1\n",
    "        prompt_metrics['num_correct'] += human_correct\n",
    "        if model_stance == answer:\n",
    "            prompt_metrics['num_correct_with_correct_debate_mode'] += human_correct\n",
    "            prompt_metrics['num_correct_debate_mode'] += 1\n",
    "        else:\n",
    "            prompt_metrics['num_correct_with_incorrect_debate_mode'] += human_correct\n",
    "            prompt_metrics['num_incorrect_debate_mode'] += 1\n",
    "        prompt_metrics['num_debate_mode_responses'] += (prompt['response'] == model_stance)\n",
    "        \n",
    "        hit_durations.append(prompt['duration'] / 1000.)\n",
    "        response_option_counts[prompt['response']] += 1\n",
    "        responses.append(prompt['response'])\n",
    "    duration = np.sum(np.array(hit_durations))\n",
    "    worker_durations.append(duration)\n",
    "    durations += hit_durations\n",
    "    response_options_array = np.array(list(response_option_counts.values()))\n",
    "    response_options_array = response_options_array / response_options_array.sum()\n",
    "    max_response_freq = response_options_array.max()\n",
    "    max_response_freqs.append(max_response_freq)\n",
    "    acc = round(100 * hit_result['accuracy'][prompt_type])\n",
    "    print('| Time:', round(duration, 1),\n",
    "          '| Acc:', acc,\n",
    "          '| Max Freq:', round(100 * max_response_freq, 1),\n",
    "          '| Rate:', hit_result['task_rating'],\n",
    "          '| Feedback:', hit_result['feedback'],\n",
    "          '| Quote Rating:', None if 'quote_rating' not in hit_result else hit_result['quote_rating'], \n",
    "          '| Quote Desc:', None if 'quote_description' not in hit_result else hit_result['quote_description'])\n",
    "\n",
    "good_worker_durations = []\n",
    "assert len(worker_durations) == len(accuracy_by_worker[prompt_type])\n",
    "for worker_duration, worker_accuracy in zip(worker_durations, accuracy_by_worker[prompt_type]):\n",
    "    if worker_accuracy > np.median(np.array(accuracy_by_worker[prompt_type])):\n",
    "        good_worker_durations.append(worker_duration)\n",
    "\n",
    "quote_ratings = np.array(quote_ratings)\n",
    "durations = np.array(durations)\n",
    "worker_durations = np.array(worker_durations)\n",
    "good_worker_durations = np.array(good_worker_durations)\n",
    "max_response_freqs = np.array(max_response_freqs)\n",
    "\n",
    "quote_ratings.sort()\n",
    "durations.sort()\n",
    "worker_durations.sort()\n",
    "good_worker_durations.sort()\n",
    "max_response_freqs.sort()\n",
    "\n",
    "for qtype in accuracy_by_worker:\n",
    "    accuracy_by_worker[qtype] = np.array(accuracy_by_worker[qtype])\n",
    "    accuracy_by_worker[qtype].sort()\n",
    "print('REJECTED:', num_rejected_hits)\n",
    "print('INCOMPLETE:', num_incomplete_hits)\n",
    "print('VALID:', num_valid_hits)\n",
    "print('Median Question Duration:', np.median(durations))\n",
    "print('Mean Question Duration:', np.mean(durations[int(durations.shape[0] / 10.):int(9. * durations.shape[0] / 10.)]))\n",
    "print('Min/Median/Mean/Max Worker Duration:',\n",
    "      round(np.min(worker_durations / 60.), 2), '/',\n",
    "      round(np.median(worker_durations / 60.), 2), '/',\n",
    "      round(np.mean(worker_durations / 60.), 2), '/',\n",
    "      round(np.max(worker_durations / 60.), 2))\n",
    "print('Min/Median/Mean/Max Good Worker Durations:',\n",
    "      round(np.min(good_worker_durations / 60.), 2), '/',\n",
    "      round(np.median(good_worker_durations / 60.), 2),'/',\n",
    "      round(np.mean(good_worker_durations / 60.), 2), '/',\n",
    "      round(np.max(good_worker_durations / 60.), 2))\n",
    "print('Median Worker Accuracy:', np.median(accuracy_by_worker[prompt_type]))\n",
    "print('Median Max Response Freq:', np.median(max_response_freqs))\n",
    "print('Quote Rating:',\n",
    "      '| Mean:', round(quote_ratings.mean(), 2),\n",
    "      '| Median:', round(np.median(quote_ratings), 2),\n",
    "      '| Std:', round(np.std(quote_ratings), 2))\n",
    "# pprint(hit_results[0]['data'][0])\n",
    "# pprint(hit_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evals per sample: 7.4\n",
      "Fraction insuffient evals: 0.0\n",
      "Convinced: nan %\n",
      "- Correct debater: nan %\n",
      "- Incorrect debater: nan %\n",
      "Accuracy: 66.5 %\n",
      "- Correct debater: nan %\n",
      "- Incorrect debater: nan %\n",
      "Extra Evals: 32.43 %\n",
      "Evals per sample distribution: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 9 9 9 9 9 9 9 9 9 9 9 9 9 9\n",
      " 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9]\n",
      "Accuracy/Num-Samples by Q Type:\n",
      "{'a': (0.7941798941798942, 24),\n",
      " 'b': (0.6032738095238095, 32),\n",
      " 'c': (0.678718400940623, 54),\n",
      " 'd': (0.6529499850254568, 53),\n",
      " 'e': (0.5375000000000001, 16)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:52: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:54: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:56: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:61: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:63: RuntimeWarning: Mean of empty slice.\n"
     ]
    }
   ],
   "source": [
    "accuracy_by_qtype = {qtype: [] for qtype in question_type_labels}\n",
    "accuracy_by_sample = []\n",
    "accuracy_by_sample_correct_debate_mode = []\n",
    "accuracy_by_sample_incorrect_debate_mode = []\n",
    "convinced_freqs = []\n",
    "convinced_freqs_with_correct_debate_mode = []\n",
    "convinced_freqs_with_incorrect_debate_mode = []\n",
    "num_evals_by_sample = []\n",
    "for qid, qid_metrics in metrics.items():\n",
    "    answer = metrics[qid]['answer']\n",
    "    for qid_metric_key, prompt in qid_metrics.items():\n",
    "        if qid_metric_key in question_type_labels:\n",
    "            qtype = qid_metric_key\n",
    "            if qid_metrics[qtype]['num'] > 0:\n",
    "                accuracy_by_qtype[qtype].append(qid_metrics[qtype]['num_correct'] / qid_metrics[qtype]['num'])\n",
    "            continue\n",
    "        if not (qid_metric_key in [None] + options):\n",
    "            continue\n",
    "        model_stance = qid_metric_key\n",
    "\n",
    "        # Q-only stats\n",
    "        prompt_metrics = metrics[qid][model_stance]\n",
    "        num_evals_by_sample.append(prompt_metrics['num'])\n",
    "        accuracy_by_sample.append(prompt_metrics['num_correct'] / prompt_metrics['num'])\n",
    "        if model_stance is None:\n",
    "            continue\n",
    "        \n",
    "        # Debater stats\n",
    "#         if 'num_debate_mode_responses' not in prompt_metrics:\n",
    "#             print(qid_metric_key, prompt_metrics)\n",
    "        convinced_freq = prompt_metrics['num_debate_mode_responses'] / prompt_metrics['num']\n",
    "        if model_stance == answer:\n",
    "            convinced_freqs_with_correct_debate_mode.append(convinced_freq)\n",
    "            accuracy_by_sample_correct_debate_mode.append(\n",
    "                prompt_metrics['num_correct_with_correct_debate_mode'] /\n",
    "                prompt_metrics['num_correct_debate_mode'])\n",
    "        else:\n",
    "            convinced_freqs_with_incorrect_debate_mode.append(convinced_freq)\n",
    "            accuracy_by_sample_incorrect_debate_mode.append(\n",
    "                prompt_metrics['num_correct_with_incorrect_debate_mode'] /\n",
    "                prompt_metrics['num_incorrect_debate_mode'])\n",
    "        convinced_freqs.append(convinced_freq)\n",
    "\n",
    "accuracy_by_qtype = {qtype: (np.array(accuracy_by_qtype[qtype]).mean(), len(accuracy_by_qtype[qtype])) for qtype in question_type_labels}\n",
    "worker_ids = set(worker_ids)\n",
    "        \n",
    "num_evals_by_sample = np.array(num_evals_by_sample)\n",
    "print('Evals per sample:', num_evals_by_sample.mean())\n",
    "print('Fraction insuffient evals:', (num_evals_by_sample < 5).mean())\n",
    "\n",
    "convinced_freqs = np.array(convinced_freqs)\n",
    "print('Convinced:', round(100 * convinced_freqs.mean(), 2), '%')\n",
    "convinced_freqs_with_correct_debate_mode = np.array(convinced_freqs_with_correct_debate_mode)\n",
    "print('- Correct debater:', round(100 * convinced_freqs_with_correct_debate_mode.mean(), 2), '%')\n",
    "convinced_freqs_with_incorrect_debate_mode = np.array(convinced_freqs_with_incorrect_debate_mode)\n",
    "print('- Incorrect debater:', round(100 * convinced_freqs_with_incorrect_debate_mode.mean(), 2), '%')\n",
    "\n",
    "accuracy_by_sample = np.array(accuracy_by_sample)\n",
    "print('Accuracy:', round(100 * accuracy_by_sample.mean(), 2), '%')\n",
    "accuracy_by_sample_correct_debate_mode = np.array(accuracy_by_sample_correct_debate_mode)\n",
    "print('- Correct debater:', round(100 * accuracy_by_sample_correct_debate_mode.mean(), 2), '%')\n",
    "accuracy_by_sample_incorrect_debate_mode = np.array(accuracy_by_sample_incorrect_debate_mode)\n",
    "print('- Incorrect debater:', round(100 * accuracy_by_sample_incorrect_debate_mode.mean(), 2), '%')\n",
    "\n",
    "num_target_evals = 5\n",
    "print('Extra Evals:', round(((100. * (num_evals_by_sample - num_target_evals).sum()) / num_evals_by_sample.sum()), 2), '%')\n",
    "num_evals_by_sample.sort()\n",
    "print('Evals per sample distribution:', num_evals_by_sample)\n",
    "\n",
    "print('Accuracy/Num-Samples by Q Type:')\n",
    "pprint(accuracy_by_qtype)\n",
    "# 1.5*3.1*60/(917.5684545454544*26/(20*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPS, Mean: (24.32, 7.95)\n"
     ]
    }
   ],
   "source": [
    "def nps(task_ratings):\n",
    "    num_ratings = sum(list(task_ratings.values()))\n",
    "    if num_ratings == 0:\n",
    "        return None\n",
    "\n",
    "    nps_sum_ratings = 0\n",
    "    sum_ratings = 0\n",
    "    for score, num_raters in task_ratings.items():\n",
    "        sum_ratings += num_raters * score\n",
    "        if score >= 9:\n",
    "            nps_sum_ratings += num_raters\n",
    "        elif score <= 6:\n",
    "            nps_sum_ratings -= num_raters\n",
    "    return round(100 * (nps_sum_ratings / float(num_ratings)), 2), round((sum_ratings / float(num_ratings)), 2)\n",
    "\n",
    "print('NPS, Mean:', nps(task_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(ttest_ind(convinced_freqs, convinced_freqs2, equal_var=False))\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 616.371,  713.065,  819.15 ,  824.231,  956.543, 1006.474,\n",
       "       1008.615, 1020.517, 1633.065, 1741.471])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_worker_durations[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

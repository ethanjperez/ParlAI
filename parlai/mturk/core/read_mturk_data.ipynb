{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# HIT Files: 121\n",
      "# Passed Test: 31 / 90 = 34.44 %\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "task_dir = '/Users/ethanperez/research/ParlAI/parlai/mturk/core/run_data/live/context_evaluator_'\n",
    "\n",
    "### RACE: Unfiltered Workers\n",
    "# prompt_type, task_id = 'question', '1553982706'  # Q-only\n",
    "# prompt_type, task_id = 'context_question', 1553790696  # TFIDF\n",
    "# prompt_type, task_id = 'context_question', 1553901953  # FastText\n",
    "### RACE: Filtered Workers\n",
    "# prompt_type, task_id = 'question', '1554052233'  # Q-only\n",
    "# prompt_type, task_id = 'quote and question', 1554006689  # TFIDF-QA\n",
    "# prompt_type, task_id = 'quote and question', 1554130485  # TFIDF-A\n",
    "# prompt_type, task_id = 'quote and question', 1554069931  # Cross-Ranker\n",
    "# prompt_type, task_id = 'quote and question', 1554072277  # SL\n",
    "# prompt_type, task_id = 'quote and question', 1554132868  # SL-Influence\n",
    "\n",
    "### RACE Test\n",
    "## Convinced\n",
    "# prompt_type, task_id = 'quote and question', 1556671432  # TFIDF-QA\n",
    "# prompt_type, task_id = 'quote and question', 1556725767  # TFIDF-A\n",
    "# prompt_type, task_id = 'quote and question', 1556739336  # BoW-A\n",
    "# prompt_type, task_id = 'quote and question', 1556756789  # ΔP(A) (race.m=sl-sents.i.best.e)\n",
    "## Acc on Summary\n",
    "# prompt_type, task_id = 'passage and question', 1555823963  # Full Passage\n",
    "# prompt_type, task_id = 'quotes and question', 1555946909  # FastText\n",
    "# prompt_type, task_id = 'quotes and question', 1555952058  # Cross-Ranker (6-10 sentence incorrectly placed at end)\n",
    "# dataset = 'race'\n",
    "\n",
    "### DREAM\n",
    "# prompt_type, task_id = 'question', 1554582693  # Q-only\n",
    "# prompt_type, task_id = 'quote and question', 1554596686  # TFIDF-QA\n",
    "# prompt_type, task_id = 'quote and question', 1554587404  # TFIDF-A\n",
    "# prompt_type, task_id = 'quote and question', 1554662280  # BoW-A\n",
    "# prompt_type, task_id = 'quote and question', 1556670413  # Bi-Ranker\n",
    "# prompt_type, task_id = 'quote and question', 1554675304  # Cross-Ranker\n",
    "# prompt_type, task_id = 'quote and question', 1554685131  # SL\n",
    "# prompt_type, task_id = 'quote and question', 1554692472  # SL-Sents\n",
    "# prompt_type, task_id = 'quote and question', 1554729998  # SL-Sents-Influence\n",
    "# prompt_type, task_id = 'quote and question', 1555333992  # SL-Theory-of-Mind\n",
    "## All answers at once\n",
    "# prompt_type, task_id = 'question, answers, and quotes', 1555707929  # TFIDF(O): 64.21%: (Less filter / no feedback)\n",
    "# prompt_type, task_id = 'question, answers, and quotes', 1555722489  # Search: 65.38%: (Less filter / no feedback)\n",
    "# prompt_type, task_id = 'question and quotes', 1555789302  # SL: 75.17% (4/5 filter)\n",
    "# prompt_type, task_id = 'question and quotes', 1555812443  # SL: 79.32% Actually: quotes and question (4/5 filter)\n",
    "# prompt_type, task_id = 'passage and question', 1555804551  # Full Passage: 92.97%\n",
    "# prompt_type, task_id = 'quotes and question', 1555823257  # FastText (5/5 filter) (77.33%)\n",
    "# prompt_type, task_id = 'quotes and question', 1555946647  # RACE Cross-Ranker (4 sentences incorrectly placed at end) (80.84%)\n",
    "# prompt_type, task_id = 'quotes and question', 1556727396  # Cross-Ranker\n",
    "# prompt_type, task_id = 'quotes and question', 1556740293  # Bi-Ranker\n",
    "prompt_type, task_id = 'quotes and question', 1556757043  # TFIDF-A\n",
    "dataset = 'dream'\n",
    "\n",
    "\n",
    "split = None  # 'middle', 'high', None\n",
    "\n",
    "\n",
    "# Set useful variables\n",
    "task_dir += str(task_id)\n",
    "if dataset != 'race':\n",
    "    split = None\n",
    "num_options = 3 if dataset == 'dream' else 4\n",
    "options = ['A', 'B', 'C', 'D'][:num_options]\n",
    "debate_mode_to_option = {'Ⅰ': 'A', 'Ⅱ': 'B', 'Ⅲ': 'C', 'Ⅳ': 'D', 'ⅰ': 'A', 'ⅱ': 'B', 'ⅲ': 'C', 'ⅳ': 'D', None: None}\n",
    "question_type_labels = ['a', 'c', 'l', 'm', 's'] if dataset == 'dream' else ['a', 'b', 'c', 'd', 'e']\n",
    "\n",
    "# Read HIT data\n",
    "print('# HIT Files:', len(os.listdir(task_dir)))\n",
    "hit_results = []\n",
    "num_passed_test = 0\n",
    "for hit_dir in os.listdir(task_dir):\n",
    "    if hit_dir.startswith('o_'):\n",
    "        continue\n",
    "    num_passed_test += 1\n",
    "    with open(os.path.join(task_dir, hit_dir, 'custom/data.json'), 'r') as file:\n",
    "        hit_results.append(json.load(file))\n",
    "        file.close()\n",
    "if len(os.listdir(task_dir)) != num_passed_test:\n",
    "    num_total_tested = len(os.listdir(task_dir)) - num_passed_test\n",
    "    print('# Passed Test:', num_passed_test, '/', num_total_tested, '=', round((100. * num_passed_test) / num_total_tested, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Time: 9.0 | Acc: 70 | Max Freq: 65.0 | Rate: 10 | Feedback: Nothing! Great task.  | Quote Rating: 8 | Quote Desc: Sparse, but largely intelligible. There were only a couple that didn't provide adequate context.\n",
      "| Time: 19.3 | Acc: 90 | Max Freq: 65.0 | Rate: 7 | Feedback: not sure | Quote Rating: 8 | Quote Desc: helpful\n",
      "| Time: 16.9 | Acc: 75 | Max Freq: 35.0 | Rate: 10 | Feedback: It seems pretty great already to be honest!  | Quote Rating: 8 | Quote Desc: Mostly useful, but a small handful of them were basically useless.\n",
      "| Time: 12.4 | Acc: 75 | Max Freq: 60.0 | Rate: 10 | Feedback: Maybe making the ones with no correlation between question and answer just a little easier or understandable. | Quote Rating: 7 | Quote Desc: Most of them gave enough detail to figure out the answer, however a few of them made no sense as to how I would have known the answer.\n",
      "| Time: 14.8 | Acc: 65 | Max Freq: 45.0 | Rate: 10 | Feedback: Fix the formatting on some of the passages so it is easier to tell who is speaking. | Quote Rating: 6 | Quote Desc: Some were just fine, The ones I got incorrect were too vague or didnt contain enough information to make the correct choice. Also the formatting could use some work as it was hard to tell who was speaking in some of them.\n",
      "| Time: 56.5 | Acc: 85 | Max Freq: 60.0 | Rate: 10 | Feedback: do not make a guessing game out of some of the answers. It is insulting to think that checking my ability tuns into a guessing game. | Quote Rating: 7 | Quote Desc: All were helpful except the last few which were nothing but a guessing game.\n",
      "| Time: 11.0 | Acc: 80 | Max Freq: 35.0 | Rate: 1 | Feedback: I noticed it said at the beginning only the top scorers would receive the $1.50. I'm pretty confident as I have a good understanding of vocabulary and grammar, but with questions like the \" '.' where did this take place' it is quite unfair. | Quote Rating: 3 | Quote Desc: Some of them had no descriptions at all.  In particular, one was \".\" and wanted to know if this took place at a railway, bus stop, or airport.  There was not a single word in the quote, zero context, so purely luck to guess the right answer.  This type should be avoided--there needs to be SOME context.\n",
      "| Time: 14.0 | Acc: 85 | Max Freq: 60.0 | Rate: 10 | Feedback: I can't think of a way. | Quote Rating: 9 | Quote Desc: A little confusing sometimes. Question would be for a man but ask what a woman did. I missed a couple but I se why looking back.\n",
      "| Time: 16.1 | Acc: 80 | Max Freq: 45.0 | Rate: 9 | Feedback: I think it needs to be a bit more sure. | Quote Rating: 8 | Quote Desc: I think they were pretty helpful but a couple didn't make sense to me and I have no idea how they could have gotten the answer they did. It was fun though!\n",
      "| Time: 23.8 | Acc: 75 | Max Freq: 65.0 | Rate: 6 | Feedback: Use better spacing so its easier to read. | Quote Rating: 3 | Quote Desc: A little hard to read and comprehend due to the layout of the person speaking.\n",
      "| Time: 15.4 | Acc: 65 | Max Freq: 40.0 | Rate: 6 | Feedback: You can't really, it's just the quotes. | Quote Rating: 4 | Quote Desc: Very vague and some had no correlation. \n",
      "| Time: 22.7 | Acc: 60 | Max Freq: 40.0 | Rate: 5 | Feedback: Give a little more details to some of the quotes.  | Quote Rating: Yes | Quote Desc: Some helped a lot and others were very vague and it was hard to guess what it meant.  \n",
      "| Time: 14.1 | Acc: 90 | Max Freq: 65.0 | Rate: 10 | Feedback: not sure | Quote Rating: 8 | Quote Desc: mostly ok\n",
      "| Time: 21.3 | Acc: 80 | Max Freq: 40.0 | Rate: 10 | Feedback: I don't know the ultimate objective of your research so it is difficult to offer a recommendation. That said, your task was clearly presented. | Quote Rating: 8 | Quote Desc: Most were easily answered, however a couple were fairly tricky.\n",
      "| Time: 11.8 | Acc: 80 | Max Freq: 40.0 | Rate: 6 | Feedback: Make more on the passages useful | Quote Rating: 7 | Quote Desc: They are mostly useful but some are not\n",
      "| Time: 35.1 | Acc: 85 | Max Freq: 70.0 | Rate: 9 | Feedback: improve how to evaluate key words | Quote Rating: 9 | Quote Desc: useful\n",
      "| Time: 22.9 | Acc: 65 | Max Freq: 50.0 | Rate: 8 | Feedback: If an answer is wrong explain why it is incorrect. | Quote Rating: 7 | Quote Desc: Some are too vague.\n",
      "| Time: 24.7 | Acc: 90 | Max Freq: 65.0 | Rate: 9 | Feedback: none | Quote Rating: 9 | Quote Desc: Good\n",
      "| Time: 16.6 | Acc: 60 | Max Freq: 50.0 | Rate: 9 | Feedback: I'm not sure, I think it would help to reassure us that some questions are near impossible given the lack of information. I can't learn from those without knowing what I might have missed. | Quote Rating: 6 | Quote Desc: There were a few that I could not pull any information out of at all. I'd say three quarters had something I could use to know or at least take a highly educated guess.\n",
      "| Time: 11.2 | Acc: 75 | Max Freq: 45.0 | Rate: 10 | Feedback: It's fine the way it is! | Quote Rating: 6 | Quote Desc: A lot of them didn't make sense this time!\n",
      "| Time: 13.0 | Acc: 70 | Max Freq: 35.0 | Rate: 6 | Feedback: improve pay | Quote Rating: 6 | Quote Desc: short\n",
      "| Time: 13.9 | Acc: 75 | Max Freq: 45.0 | Rate: 8 | Feedback: I would say it is fairly straight forward. | Quote Rating: 5 | Quote Desc: Besides on misread, there where many passages that said things with no indication of whats going on.\n",
      "| Time: 23.6 | Acc: 85 | Max Freq: 55.0 | Rate: 9 | Feedback: Some of the questions need more detailed quotes in order to answer the question correctly. | Quote Rating: 9 | Quote Desc: I would describe the quotes as snippets of conversations between men and women that were usually detailed and helpful.\n",
      "| Time: 9.7 | Acc: 80 | Max Freq: 45.0 | Rate: 8 | Feedback: more money, honey | Quote Rating: 7 | Quote Desc: Sometimes helpful, sometimes not helpful at all\n",
      "| Time: 17.1 | Acc: 75 | Max Freq: 55.0 | Rate: 6 | Feedback: unsure | Quote Rating: 7 | Quote Desc: Some made no sence on how they were incorrect answers or correct answers\n",
      "| Time: 17.4 | Acc: 85 | Max Freq: 60.0 | Rate: 9 | Feedback: Perhaps more explanation of why an answer was wrong | Quote Rating: 8 | Quote Desc: Some felt quite disjointed and I was surprised when what I thought was being said was different.\n",
      "| Time: 15.8 | Acc: 90 | Max Freq: 65.0 | Rate: 10 | Feedback: I enjoyed the task as it was, very engaging. | Quote Rating: 9 | Quote Desc: I enjoyed reading them very much. 18 &  19 were a little tough, but overall I believe they were very helpful.\n",
      "| Time: 10.3 | Acc: 75 | Max Freq: 35.0 | Rate: 10 | Feedback: It would be easier if the example tasks included quotes that may be misleading or give no information at all. It would make it more similar to the actual task itself. | Quote Rating: 7 | Quote Desc: Most of them were helpful, however, a few were misleading. There were also a couple that gave zero input on to what the scenario may or may not have been.\n",
      "REJECTED: 0\n",
      "INCOMPLETE: 3\n",
      "VALID: 28\n",
      "Median Question Duration: 16.035\n",
      "Mean Question Duration: 18.03568303571429\n",
      "Min/Median/Max Worker Duration: 9.014500000000002 / 15.9565 / 56.513\n",
      "Min/Median/Max Good Worker Durations: 9.715 / 16.776 / 56.513\n",
      "Median Worker Accuracy: 0.775\n",
      "Median Max Response Freq: 0.5\n",
      "Quote Rating: | Mean: 7.0 | Median: 7.0 | Std: 1.68\n"
     ]
    }
   ],
   "source": [
    "num_valid_hits, num_rejected_hits, num_incomplete_hits = 0, 0, 0\n",
    "metrics = {}\n",
    "task_ratings = {i: 0 for i in range(11)}\n",
    "quote_ratings = []\n",
    "durations = []\n",
    "worker_durations = []\n",
    "accuracy_by_worker = {}\n",
    "max_response_freqs = []\n",
    "worker_ids = []\n",
    "hits_by_qid = {}\n",
    "\n",
    "for hit_result in hit_results:\n",
    "    if ((len(hit_result['reject_reasons']) > 0) or\n",
    "        (len(hit_result['block_reasons']) > 0)):\n",
    "        num_rejected_hits += 1\n",
    "        print(hit_result['worker_id'], hit_result['assignment_id'],\n",
    "              '| reject_reasons:', hit_result['reject_reasons'],\n",
    "              '| block_reasons:', hit_result['block_reasons'],\n",
    "              '| bonus_reasons: ' + str(hit_result['bonus_reasons']) if 'bonus_reasons' in hit_result else '')\n",
    "    elif hit_result['feedback'] is None:\n",
    "        num_incomplete_hits += 1\n",
    "        continue\n",
    "    \n",
    "    worker_ids.append(hit_result['worker_id'])\n",
    "    num_valid_hits += 1\n",
    "    if (hit_result['task_rating'] is not None) and (hit_result['task_rating'].isdigit()):\n",
    "        task_ratings[int(hit_result['task_rating'])] += 1\n",
    "    if (hit_result.get('quote_rating') is not None) and (hit_result['quote_rating'].isdigit()):\n",
    "        quote_ratings.append(int(hit_result['quote_rating']))\n",
    "    for qtype, qtype_accuracy in hit_result['accuracy'].items():\n",
    "        accuracy_by_worker[qtype] = accuracy_by_worker.get(qtype, []) + [qtype_accuracy]\n",
    "    \n",
    "    hit_durations = []\n",
    "    response_option_counts = {option: 0 for option in options}\n",
    "    responses = []\n",
    "    for prompt in hit_result['data']:\n",
    "        qid = prompt['sample']['qid']\n",
    "        if (split is not None) and (split not in qid):\n",
    "            continue\n",
    "        hits_by_qid[qid] = hits_by_qid.get(qid, [])\n",
    "        hits_by_qid[qid].append(prompt)\n",
    "        model_stance = debate_mode_to_option[prompt['sample']['debate_mode']]\n",
    "        answer = prompt['sample']['eval_labels'][0]\n",
    "        human_correct = (prompt['response'] == answer)\n",
    "        assert answer in options, 'Answer must be in options.'\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if qid not in metrics:\n",
    "            metrics[qid] = {\n",
    "                option: {\n",
    "                    'num': 0,\n",
    "                    'num_correct': 0,\n",
    "                    'num_correct_debate_mode': 0,\n",
    "                    'num_incorrect_debate_mode': 0,\n",
    "                    'num_correct_with_correct_debate_mode': 0,\n",
    "                    'num_correct_with_incorrect_debate_mode': 0,\n",
    "                    'num_debate_mode_responses': 0,\n",
    "                }\n",
    "                for option in ([None] if model_stance is None else options)\n",
    "            }\n",
    "            for qtype in question_type_labels:\n",
    "                metrics[qid][qtype] = {\n",
    "                    'num': 0,\n",
    "                    'num_correct': 0,\n",
    "                }\n",
    "            metrics[qid]['answer'] = answer\n",
    "        for qtype in set(''.join(prompt['sample'].get('question_type_labels', []))):\n",
    "            qtype = qtype.lower()\n",
    "            metrics[qid][qtype]['num'] += 1\n",
    "            metrics[qid][qtype]['num_correct'] += human_correct\n",
    "        prompt_metrics = metrics[qid][model_stance]\n",
    "        prompt_metrics['num'] += 1\n",
    "        prompt_metrics['num_correct'] += human_correct\n",
    "        if model_stance == answer:\n",
    "            prompt_metrics['num_correct_with_correct_debate_mode'] += human_correct\n",
    "            prompt_metrics['num_correct_debate_mode'] += 1\n",
    "        else:\n",
    "            prompt_metrics['num_correct_with_incorrect_debate_mode'] += human_correct\n",
    "            prompt_metrics['num_incorrect_debate_mode'] += 1\n",
    "        prompt_metrics['num_debate_mode_responses'] += (prompt['response'] == model_stance)\n",
    "        \n",
    "        hit_durations.append(prompt['duration'] / 1000.)\n",
    "        response_option_counts[prompt['response']] += 1\n",
    "        responses.append(prompt['response'])\n",
    "    duration = np.median(np.array(hit_durations))\n",
    "    worker_durations.append(duration)\n",
    "    durations += hit_durations\n",
    "    response_options_array = np.array(list(response_option_counts.values()))\n",
    "    response_options_array = response_options_array / response_options_array.sum()\n",
    "    max_response_freq = response_options_array.max()\n",
    "    max_response_freqs.append(max_response_freq)\n",
    "    acc = round(100 * hit_result['accuracy'][prompt_type])\n",
    "    print('| Time:', round(duration, 1),\n",
    "          '| Acc:', acc,\n",
    "          '| Max Freq:', round(100 * max_response_freq, 1),\n",
    "          '| Rate:', hit_result['task_rating'],\n",
    "          '| Feedback:', hit_result['feedback'],\n",
    "          '| Quote Rating:', None if 'quote_rating' not in hit_result else hit_result['quote_rating'], \n",
    "          '| Quote Desc:', None if 'quote_description' not in hit_result else hit_result['quote_description'])\n",
    "\n",
    "good_worker_durations = []\n",
    "assert len(worker_durations) == len(accuracy_by_worker[prompt_type])\n",
    "for worker_duration, worker_accuracy in zip(worker_durations, accuracy_by_worker[prompt_type]):\n",
    "    if worker_accuracy > np.median(np.array(accuracy_by_worker[prompt_type])):\n",
    "        good_worker_durations.append(worker_duration)\n",
    "\n",
    "quote_ratings = np.array(quote_ratings)\n",
    "durations = np.array(durations)\n",
    "worker_durations = np.array(worker_durations)\n",
    "good_worker_durations = np.array(good_worker_durations)\n",
    "max_response_freqs = np.array(max_response_freqs)\n",
    "\n",
    "quote_ratings.sort()\n",
    "durations.sort()\n",
    "worker_durations.sort()\n",
    "good_worker_durations.sort()\n",
    "max_response_freqs.sort()\n",
    "\n",
    "for qtype in accuracy_by_worker:\n",
    "    accuracy_by_worker[qtype] = np.array(accuracy_by_worker[qtype])\n",
    "    accuracy_by_worker[qtype].sort()\n",
    "print('REJECTED:', num_rejected_hits)\n",
    "print('INCOMPLETE:', num_incomplete_hits)\n",
    "print('VALID:', num_valid_hits)\n",
    "print('Median Question Duration:', np.median(durations))\n",
    "print('Mean Question Duration:', np.mean(durations[int(durations.shape[0] / 10.):int(9. * durations.shape[0] / 10.)]))\n",
    "print('Min/Median/Max Worker Duration:', np.min(worker_durations), '/', np.median(worker_durations), '/', np.max(worker_durations))\n",
    "print('Min/Median/Max Good Worker Durations:', np.min(good_worker_durations), '/', np.median(good_worker_durations), '/', np.max(good_worker_durations))\n",
    "print('Median Worker Accuracy:', np.median(accuracy_by_worker[prompt_type]))\n",
    "print('Median Max Response Freq:', np.median(max_response_freqs))\n",
    "print('Quote Rating:',\n",
    "      '| Mean:', round(quote_ratings.mean(), 2),\n",
    "      '| Median:', round(np.median(quote_ratings), 2),\n",
    "      '| Std:', round(np.std(quote_ratings), 2))\n",
    "# pprint(hit_results[0]['data'][0])\n",
    "# pprint(hit_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evals per sample: 5.6\n",
      "Fraction insuffient evals: 0.0\n",
      "Convinced: nan %\n",
      "- Correct debater: nan %\n",
      "- Incorrect debater: nan %\n",
      "Accuracy: 77.0 %\n",
      "- Correct debater: nan %\n",
      "- Incorrect debater: nan %\n",
      "Extra Evals: 10.71 %\n",
      "Evals per sample distribution: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5\n",
      " 5 5 5 5 5 5 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "Accuracy/Num-Samples by Q Type:\n",
      "{'a': (0.0, 2),\n",
      " 'c': (0.6771428571428572, 35),\n",
      " 'l': (0.7900000000000001, 70),\n",
      " 'm': (0.7999999999999999, 12),\n",
      " 's': (0.6937500000000001, 16)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:52: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:54: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:56: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:61: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:63: RuntimeWarning: Mean of empty slice.\n"
     ]
    }
   ],
   "source": [
    "accuracy_by_qtype = {qtype: [] for qtype in question_type_labels}\n",
    "accuracy_by_sample = []\n",
    "accuracy_by_sample_correct_debate_mode = []\n",
    "accuracy_by_sample_incorrect_debate_mode = []\n",
    "convinced_freqs = []\n",
    "convinced_freqs_with_correct_debate_mode = []\n",
    "convinced_freqs_with_incorrect_debate_mode = []\n",
    "num_evals_by_sample = []\n",
    "for qid, qid_metrics in metrics.items():\n",
    "    answer = metrics[qid]['answer']\n",
    "    for qid_metric_key, prompt in qid_metrics.items():\n",
    "        if qid_metric_key in question_type_labels:\n",
    "            qtype = qid_metric_key\n",
    "            if qid_metrics[qtype]['num'] > 0:\n",
    "                accuracy_by_qtype[qtype].append(qid_metrics[qtype]['num_correct'] / qid_metrics[qtype]['num'])\n",
    "            continue\n",
    "        if not (qid_metric_key in [None] + options):\n",
    "            continue\n",
    "        model_stance = qid_metric_key\n",
    "\n",
    "        # Q-only stats\n",
    "        prompt_metrics = metrics[qid][model_stance]\n",
    "        num_evals_by_sample.append(prompt_metrics['num'])\n",
    "        accuracy_by_sample.append(prompt_metrics['num_correct'] / prompt_metrics['num'])\n",
    "        if model_stance is None:\n",
    "            continue\n",
    "        \n",
    "        # Debater stats\n",
    "#         if 'num_debate_mode_responses' not in prompt_metrics:\n",
    "#             print(qid_metric_key, prompt_metrics)\n",
    "        convinced_freq = prompt_metrics['num_debate_mode_responses'] / prompt_metrics['num']\n",
    "        if model_stance == answer:\n",
    "            convinced_freqs_with_correct_debate_mode.append(convinced_freq)\n",
    "            accuracy_by_sample_correct_debate_mode.append(\n",
    "                prompt_metrics['num_correct_with_correct_debate_mode'] /\n",
    "                prompt_metrics['num_correct_debate_mode'])\n",
    "        else:\n",
    "            convinced_freqs_with_incorrect_debate_mode.append(convinced_freq)\n",
    "            accuracy_by_sample_incorrect_debate_mode.append(\n",
    "                prompt_metrics['num_correct_with_incorrect_debate_mode'] /\n",
    "                prompt_metrics['num_incorrect_debate_mode'])\n",
    "        convinced_freqs.append(convinced_freq)\n",
    "\n",
    "accuracy_by_qtype = {qtype: (np.array(accuracy_by_qtype[qtype]).mean(), len(accuracy_by_qtype[qtype])) for qtype in question_type_labels}\n",
    "worker_ids = set(worker_ids)\n",
    "        \n",
    "num_evals_by_sample = np.array(num_evals_by_sample)\n",
    "print('Evals per sample:', num_evals_by_sample.mean())\n",
    "print('Fraction insuffient evals:', (num_evals_by_sample < 5).mean())\n",
    "\n",
    "convinced_freqs = np.array(convinced_freqs)\n",
    "print('Convinced:', round(100 * convinced_freqs.mean(), 2), '%')\n",
    "convinced_freqs_with_correct_debate_mode = np.array(convinced_freqs_with_correct_debate_mode)\n",
    "print('- Correct debater:', round(100 * convinced_freqs_with_correct_debate_mode.mean(), 2), '%')\n",
    "convinced_freqs_with_incorrect_debate_mode = np.array(convinced_freqs_with_incorrect_debate_mode)\n",
    "print('- Incorrect debater:', round(100 * convinced_freqs_with_incorrect_debate_mode.mean(), 2), '%')\n",
    "\n",
    "accuracy_by_sample = np.array(accuracy_by_sample)\n",
    "print('Accuracy:', round(100 * accuracy_by_sample.mean(), 2), '%')\n",
    "accuracy_by_sample_correct_debate_mode = np.array(accuracy_by_sample_correct_debate_mode)\n",
    "print('- Correct debater:', round(100 * accuracy_by_sample_correct_debate_mode.mean(), 2), '%')\n",
    "accuracy_by_sample_incorrect_debate_mode = np.array(accuracy_by_sample_incorrect_debate_mode)\n",
    "print('- Incorrect debater:', round(100 * accuracy_by_sample_incorrect_debate_mode.mean(), 2), '%')\n",
    "\n",
    "num_target_evals = 5\n",
    "print('Extra Evals:', round(((100. * (num_evals_by_sample - num_target_evals).sum()) / num_evals_by_sample.sum()), 2), '%')\n",
    "num_evals_by_sample.sort()\n",
    "print('Evals per sample distribution:', num_evals_by_sample)\n",
    "\n",
    "print('Accuracy/Num-Samples by Q Type:')\n",
    "pprint(accuracy_by_qtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPS, Mean: (36.36, 8.18)\n"
     ]
    }
   ],
   "source": [
    "def nps(task_ratings):\n",
    "    num_ratings = sum(list(task_ratings.values()))\n",
    "    if num_ratings == 0:\n",
    "        return None\n",
    "\n",
    "    nps_sum_ratings = 0\n",
    "    sum_ratings = 0\n",
    "    for score, num_raters in task_ratings.items():\n",
    "        sum_ratings += num_raters * score\n",
    "        if score >= 9:\n",
    "            nps_sum_ratings += num_raters\n",
    "        elif score <= 6:\n",
    "            nps_sum_ratings -= num_raters\n",
    "    return round(100 * (nps_sum_ratings / float(num_ratings)), 2), round((sum_ratings / float(num_ratings)), 2)\n",
    "\n",
    "print('NPS, Mean:', nps(task_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(ttest_ind(convinced_freqs, convinced_freqs2, equal_var=False))\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1-5-q0',\n",
      " '1-5-q1',\n",
      " '1-5-q2',\n",
      " '1-5-q3',\n",
      " '1-5-q4',\n",
      " '10-18-q0',\n",
      " '11-133-q0',\n",
      " '12-127-q0',\n",
      " '12-317-q0',\n",
      " '12-482-q0',\n",
      " '12-509-q0',\n",
      " '12-604-q0',\n",
      " '12-68-q0',\n",
      " '13-179-q0',\n",
      " '13-55-q0',\n",
      " '14-150-q0',\n",
      " '14-175-q0',\n",
      " '14-175-q1',\n",
      " '14-181-q0',\n",
      " '14-184-q0',\n",
      " '14-184-q1',\n",
      " '14-184-q2',\n",
      " '14-203-q0',\n",
      " '14-203-q1',\n",
      " '14-203-q2',\n",
      " '14-208-q0',\n",
      " '14-314-q0',\n",
      " '14-314-q1',\n",
      " '15-111-q0',\n",
      " '15-111-q1',\n",
      " '15-111-q2',\n",
      " '15-111-q3',\n",
      " '15-28-q0',\n",
      " '16-78-q0',\n",
      " '17-150-q0',\n",
      " '17-277-q0',\n",
      " '17-96-q0',\n",
      " '17-99-q0',\n",
      " '2-41-q0',\n",
      " '2-41-q1',\n",
      " '2-41-q2',\n",
      " '21-140-q0',\n",
      " '21-162-q0',\n",
      " '21-162-q1',\n",
      " '21-182-q0',\n",
      " '21-233-q0',\n",
      " '21-274-q0',\n",
      " '21-274-q1',\n",
      " '3-218-q0',\n",
      " '3-218-q1',\n",
      " '3-218-q2',\n",
      " '3-220-q0',\n",
      " '3-24-q0',\n",
      " '3-240-q0',\n",
      " '3-240-q1',\n",
      " '3-240-q2',\n",
      " '4-1083-q0',\n",
      " '4-1083-q1',\n",
      " '4-1083-q2',\n",
      " '4-1083-q3',\n",
      " '4-1105-q0',\n",
      " '4-199-q0',\n",
      " '4-329-q0',\n",
      " '4-329-q1',\n",
      " '4-329-q2',\n",
      " '4-366-q0',\n",
      " '4-433-q0',\n",
      " '4-481-q0',\n",
      " '4-510-q0',\n",
      " '4-510-q1',\n",
      " '4-597-q0',\n",
      " '4-713-q0',\n",
      " '4-823-q0',\n",
      " '4-883-q0',\n",
      " '4-90-q0',\n",
      " '4-90-q1',\n",
      " '4-90-q2',\n",
      " '4-920-q0',\n",
      " '5-129-q0',\n",
      " '5-129-q1',\n",
      " '5-129-q2',\n",
      " '5-142-q0',\n",
      " '5-249-q0',\n",
      " '5-258-q0',\n",
      " '5-555-q0',\n",
      " '5-644-q0',\n",
      " '5-736-q0',\n",
      " '5-806-q0',\n",
      " '5-806-q1',\n",
      " '5-806-q2',\n",
      " '5-847-q0',\n",
      " '7-140-q0',\n",
      " '7-140-q1',\n",
      " '7-140-q2',\n",
      " '7-296-q0',\n",
      " '7-297-q0',\n",
      " '7-307-q0',\n",
      " '7-386-q0',\n",
      " '7-84-q0',\n",
      " '8-33-q0']\n"
     ]
    }
   ],
   "source": [
    "qids = list(metrics.keys())\n",
    "qids.sort()\n",
    "pprint(qids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dev/1-5/0',\n",
      " 'dev/1-5/1',\n",
      " 'dev/1-5/2',\n",
      " 'dev/1-5/3',\n",
      " 'dev/1-5/4',\n",
      " 'dev/10-18/0',\n",
      " 'dev/11-133/0',\n",
      " 'dev/12-127/0',\n",
      " 'dev/12-317/0',\n",
      " 'dev/12-482/0',\n",
      " 'dev/12-509/0',\n",
      " 'dev/12-604/0',\n",
      " 'dev/12-68/0',\n",
      " 'dev/13-179/0',\n",
      " 'dev/13-55/0',\n",
      " 'dev/14-150/0',\n",
      " 'dev/14-175/0',\n",
      " 'dev/14-175/1',\n",
      " 'dev/14-181/0',\n",
      " 'dev/14-184/0',\n",
      " 'dev/14-184/1',\n",
      " 'dev/14-184/2',\n",
      " 'dev/14-203/0',\n",
      " 'dev/14-203/1',\n",
      " 'dev/14-203/2',\n",
      " 'dev/14-208/0',\n",
      " 'dev/14-314/0',\n",
      " 'dev/14-314/1',\n",
      " 'dev/15-111/0',\n",
      " 'dev/15-111/1',\n",
      " 'dev/15-111/2',\n",
      " 'dev/15-111/3',\n",
      " 'dev/15-28/0',\n",
      " 'dev/16-78/0',\n",
      " 'dev/17-150/0',\n",
      " 'dev/17-277/0',\n",
      " 'dev/17-96/0',\n",
      " 'dev/17-99/0',\n",
      " 'dev/2-41/0',\n",
      " 'dev/2-41/1',\n",
      " 'dev/2-41/2',\n",
      " 'dev/21-140/0',\n",
      " 'dev/21-162/0',\n",
      " 'dev/21-162/1',\n",
      " 'dev/21-182/0',\n",
      " 'dev/21-233/0',\n",
      " 'dev/21-274/0',\n",
      " 'dev/21-274/1',\n",
      " 'dev/3-218/0',\n",
      " 'dev/3-218/1',\n",
      " 'dev/3-218/2',\n",
      " 'dev/3-220/0',\n",
      " 'dev/3-24/0',\n",
      " 'dev/3-240/0',\n",
      " 'dev/3-240/1',\n",
      " 'dev/3-240/2',\n",
      " 'dev/4-1083/0',\n",
      " 'dev/4-1083/1',\n",
      " 'dev/4-1083/2',\n",
      " 'dev/4-1083/3',\n",
      " 'dev/4-1105/0',\n",
      " 'dev/4-199/0',\n",
      " 'dev/4-329/0',\n",
      " 'dev/4-329/1',\n",
      " 'dev/4-329/2',\n",
      " 'dev/4-366/0',\n",
      " 'dev/4-433/0',\n",
      " 'dev/4-481/0',\n",
      " 'dev/4-510/0',\n",
      " 'dev/4-510/1',\n",
      " 'dev/4-597/0',\n",
      " 'dev/4-713/0',\n",
      " 'dev/4-823/0',\n",
      " 'dev/4-883/0',\n",
      " 'dev/4-90/0',\n",
      " 'dev/4-90/1',\n",
      " 'dev/4-90/2',\n",
      " 'dev/4-920/0',\n",
      " 'dev/5-129/0',\n",
      " 'dev/5-129/1',\n",
      " 'dev/5-129/2',\n",
      " 'dev/5-142/0',\n",
      " 'dev/5-249/0',\n",
      " 'dev/5-258/0',\n",
      " 'dev/5-555/0',\n",
      " 'dev/5-644/0',\n",
      " 'dev/5-736/0',\n",
      " 'dev/5-806/0',\n",
      " 'dev/5-806/1',\n",
      " 'dev/5-806/2',\n",
      " 'dev/5-847/0',\n",
      " 'dev/7-140/0',\n",
      " 'dev/7-140/1',\n",
      " 'dev/7-140/2',\n",
      " 'dev/7-296/0',\n",
      " 'dev/7-297/0',\n",
      " 'dev/7-307/0',\n",
      " 'dev/7-386/0',\n",
      " 'dev/7-84/0',\n",
      " 'dev/8-33/0']\n"
     ]
    }
   ],
   "source": [
    "qids2 = list(metrics.keys())\n",
    "qids2.sort()\n",
    "pprint(qids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

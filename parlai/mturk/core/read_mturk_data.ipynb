{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# HIT Files: 81\n",
      "# Passed Test: 32 / 49 = 65.31 %\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "### RACE: Unfiltered Workers\n",
    "# prompt_type, task_dir = 'question', '/Users/ethanperez/research/ParlAI/parlai/mturk/core/run_data/live/context_evaluator_1553982706'  # Q-only\n",
    "# prompt_type, task_dir = 'context_question', '/Users/ethanperez/research/ParlAI/parlai/mturk/core/run_data/live/context_evaluator_1553790696'  # TFIDF\n",
    "# prompt_type, task_dir = 'context_question', '/Users/ethanperez/research/ParlAI/parlai/mturk/core/run_data/live/context_evaluator_1553901953'  # FastText\n",
    "### RACE: Filtered Workers\n",
    "# prompt_type, task_dir = 'question', '/Users/ethanperez/research/ParlAI/parlai/mturk/core/run_data/live/context_evaluator_1554052233'  # Q-only\n",
    "# prompt_type, task_dir = 'quote and question', '/Users/ethanperez/research/ParlAI/parlai/mturk/core/run_data/live/context_evaluator_1554006689'  # TFIDF(Q+O)\n",
    "# prompt_type, task_dir = 'quote and question', '/Users/ethanperez/research/ParlAI/parlai/mturk/core/run_data/live/context_evaluator_1554130485'  # TFIDF(O)\n",
    "# prompt_type, task_dir = 'quote and question', '/Users/ethanperez/research/ParlAI/parlai/mturk/core/run_data/live/context_evaluator_1554069931'  # Oracle\n",
    "# prompt_type, task_dir = 'quote and question', '/Users/ethanperez/research/ParlAI/parlai/mturk/core/run_data/live/context_evaluator_1554072277'  # SL\n",
    "# prompt_type, task_dir = 'quote and question', '/Users/ethanperez/research/ParlAI/parlai/mturk/core/run_data/live/context_evaluator_1554132868'  # SL-Influence\n",
    "\n",
    "### DREAM\n",
    "prompt_type, task_dir = 'question', '/Users/ethanperez/research/ParlAI/parlai/mturk/core/run_data/live/context_evaluator_1554582693'  # Q-only\n",
    "\n",
    "# split = 'middle'\n",
    "split = None\n",
    "\n",
    "# Set useful variables\n",
    "num_options = 4\n",
    "options = ['A', 'B', 'C', 'D'][:num_options]\n",
    "debate_mode_to_option = {'Ⅰ': 'A', 'Ⅱ': 'B', 'Ⅲ': 'C', 'Ⅳ': 'D', 'ⅰ': 'A', 'ⅱ': 'B', 'ⅲ': 'C', 'ⅳ': 'D', None: None}\n",
    "\n",
    "# Read HIT data\n",
    "print('# HIT Files:', len(os.listdir(task_dir)))\n",
    "hit_results = []\n",
    "num_passed_test = 0\n",
    "for hit_dir in os.listdir(task_dir):\n",
    "    if hit_dir.startswith('o_'):\n",
    "        continue\n",
    "    num_passed_test += 1\n",
    "    with open(os.path.join(task_dir, hit_dir, 'custom/data.json'), 'r') as file:\n",
    "        hit_results.append(json.load(file))\n",
    "        file.close()\n",
    "if len(os.listdir(task_dir)) != num_passed_test:\n",
    "    num_total_tested = len(os.listdir(task_dir)) - num_passed_test\n",
    "    print('# Passed Test:', num_passed_test, '/', num_total_tested, '=', round((100. * num_passed_test) / num_total_tested, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Time: 9.5 | Acc: 25 | Max Freq: 40.0 | Rate: 6 | Feedback: Let successful workers skip the pretest questions.\n",
      "| Time: 5.7 | Acc: 30 | Max Freq: 45.0 | Rate: 6 | Feedback: N/A\n",
      "| Time: 7.7 | Acc: 40 | Max Freq: 50.0 | Rate: 6 | Feedback: No improvements.\n",
      "| Time: 5.6 | Acc: 40 | Max Freq: 40.0 | Rate: 10 | Feedback: No comment\n",
      "| Time: 12.8 | Acc: 30 | Max Freq: 50.0 | Rate: 7 | Feedback: Its already good\n",
      "| Time: 18.7 | Acc: 65 | Max Freq: 40.0 | Rate: 5 | Feedback: It's very long for the pay amount. I'd say pay more.\n",
      "| Time: 8.9 | Acc: 60 | Max Freq: 45.0 | Rate: 10 | Feedback: I would make the instructions more clear so that the worker knows what they are doing. \n",
      "| Time: 10.7 | Acc: 65 | Max Freq: 40.0 | Rate: 9 | Feedback: Some of the questions seem impossible to guess right without context. I suppose that's part of the design but it's pretty hard that way.\n",
      "| Time: 3.0 | Acc: 30 | Max Freq: 35.0 | Rate: 10 | Feedback: Nothing everything went smooth! \n",
      "| Time: 8.7 | Acc: 35 | Max Freq: 40.0 | Rate: 10 | Feedback: No need\n",
      "| Time: 6.8 | Acc: 20 | Max Freq: 55.0 | Rate: 10 | Feedback: Nothing comes to mind.  I really enjoyed it as is!\n",
      "| Time: 6.3 | Acc: 55 | Max Freq: 35.0 | Rate: 9 | Feedback: moar money! j/k thanx\n",
      "| Time: 9.5 | Acc: 45 | Max Freq: 40.0 | Rate: 8 | Feedback: none\n",
      "| Time: 7.9 | Acc: 35 | Max Freq: 45.0 | Rate: 8 | Feedback: I have no suggestions for improvement.\n",
      "| Time: 8.2 | Acc: 50 | Max Freq: 40.0 | Rate: 10 | Feedback: It doesnt need improved.\n",
      "| Time: 7.2 | Acc: 50 | Max Freq: 45.0 | Rate: 10 | Feedback: I think it's fine as it is. Perhaps better pay?\n",
      "| Time: 6.2 | Acc: 55 | Max Freq: 55.0 | Rate: 10 | Feedback: No improvement\n",
      "| Time: 9.0 | Acc: 45 | Max Freq: 40.0 | Rate: 9 | Feedback: Give a little more context.\n",
      "| Time: 6.6 | Acc: 45 | Max Freq: 50.0 | Rate: 8 | Feedback: No improvements, very well done. \n",
      "| Time: 9.7 | Acc: 50 | Max Freq: 50.0 | Rate: 6 | Feedback: Too subjective. We're risking a rejection if we don't answer correctly when there's no information to have an accurate opinion or answer.\n",
      "| Time: 4.5 | Acc: 35 | Max Freq: 35.0 | Rate: 7 | Feedback: More off the wall questions.\n",
      "| Time: 5.6 | Acc: 35 | Max Freq: 40.0 | Rate: 7 | Feedback: [DISCONNECT]\n",
      "| Time: 7.7 | Acc: 50 | Max Freq: 45.0 | Rate: 9 | Feedback: Fewer questions would be nice or possible a slightly faster interface. If I could select the Letter, ABCD  on a button versus having to go to a drop down menu.\n",
      "| Time: 5.3 | Acc: 50 | Max Freq: 40.0 | Rate: 10 | Feedback: It's perfect how it is\n",
      "| Time: 7.4 | Acc: 50 | Max Freq: 40.0 | Rate: 6 | Feedback: Increase the pay.\n",
      "| Time: 9.0 | Acc: 35 | Max Freq: 35.0 | Rate: 9 | Feedback: Give the correct answer and why it is correct\n",
      "| Time: 11.8 | Acc: 45 | Max Freq: 45.0 | Rate: 10 | Feedback: I think it's fine the way it is.\n",
      "| Time: 7.9 | Acc: 50 | Max Freq: 40.0 | Rate: 4 | Feedback: Nothing. I like the training part, since the task is hard to understand. This task is just confusing in general, and I think some people might not like it.\n",
      "| Time: 7.3 | Acc: 40 | Max Freq: 45.0 | Rate: 4 | Feedback: Better layout\n",
      "| Time: 8.1 | Acc: 50 | Max Freq: 40.0 | Rate: 8 | Feedback: some questions are impossible to answer, each should have some sort of none luck factor\n",
      "REJECTED: 0\n",
      "INCOMPLETE: 2\n",
      "VALID: 30\n",
      "Median Question Duration: 7.8855\n",
      "Median Worker Duration: 7.82425\n",
      "Median Worker Accuracy: 0.45\n",
      "Median Max Response Freq: 0.4\n",
      "'A1P6OXEJ86HQRM'\n"
     ]
    }
   ],
   "source": [
    "num_valid_hits, num_rejected_hits, num_incomplete_hits = 0, 0, 0\n",
    "metrics = {}\n",
    "task_ratings = {i: 0 for i in range(11)}\n",
    "durations = []\n",
    "durations_by_worker = []\n",
    "accuracy_by_worker = {}\n",
    "max_response_freqs = []\n",
    "worker_ids = []\n",
    "\n",
    "for hit_result in hit_results:\n",
    "    if ((len(hit_result['reject_reasons']) > 0) or\n",
    "        (len(hit_result['block_reasons']) > 0)):\n",
    "        num_rejected_hits += 1\n",
    "        print('| reject_reasons:', hit_result['reject_reasons'], '| block_reasons:', hit_result['block_reasons'])\n",
    "#         continue\n",
    "    elif hit_result['feedback'] is None:\n",
    "        num_incomplete_hits += 1\n",
    "        continue\n",
    "    \n",
    "    worker_ids.append(hit_result['worker_id'])\n",
    "    num_valid_hits += 1\n",
    "    if (hit_result['task_rating'] is not None) and (hit_result['task_rating'].isdigit()):\n",
    "        task_ratings[int(hit_result['task_rating'])] += 1\n",
    "    for qtype, qtype_accuracy in hit_result['accuracy'].items():\n",
    "        accuracy_by_worker[qtype] = accuracy_by_worker.get(qtype, []) + [qtype_accuracy]\n",
    "    \n",
    "    hit_durations = []\n",
    "    response_option_counts = {option: 0 for option in options}\n",
    "    responses = []\n",
    "    for prompt in hit_result['data']:\n",
    "        qid = prompt['sample']['qid']\n",
    "        if (split is not None) and (split not in qid):\n",
    "            continue\n",
    "        model_stance = debate_mode_to_option[prompt['sample']['debate_mode']]\n",
    "        answer = prompt['sample']['eval_labels'][0]\n",
    "        human_correct = (prompt['response'] == answer)\n",
    "        assert answer in options, 'Answer must be in options.'\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if qid not in metrics:\n",
    "            metrics[qid] = {\n",
    "                option: {\n",
    "                    'num': 0,\n",
    "                    'num_correct': 0,\n",
    "                    'num_correct_debate_mode': 0,\n",
    "                    'num_incorrect_debate_mode': 0,\n",
    "                    'num_correct_with_correct_debate_mode': 0,\n",
    "                    'num_correct_with_incorrect_debate_mode': 0,\n",
    "                    'num_debate_mode_responses': 0,\n",
    "                }\n",
    "                for option in ([None] if model_stance is None else options)\n",
    "            }\n",
    "            metrics[qid]['answer'] = answer\n",
    "        prompt_metrics = metrics[qid][model_stance]\n",
    "        prompt_metrics['num'] += 1\n",
    "        prompt_metrics['num_correct'] += human_correct\n",
    "        if model_stance == answer:\n",
    "            prompt_metrics['num_correct_with_correct_debate_mode'] += human_correct\n",
    "            prompt_metrics['num_correct_debate_mode'] += 1\n",
    "        else:\n",
    "            prompt_metrics['num_correct_with_incorrect_debate_mode'] += human_correct\n",
    "            prompt_metrics['num_incorrect_debate_mode'] += 1\n",
    "        prompt_metrics['num_debate_mode_responses'] += (prompt['response'] == model_stance)\n",
    "        \n",
    "        hit_durations.append(prompt['duration'] / 1000.)\n",
    "        response_option_counts[prompt['response']] += 1\n",
    "        responses.append(prompt['response'])\n",
    "    duration = np.median(np.array(hit_durations))\n",
    "    durations_by_worker.append(duration)\n",
    "    durations += hit_durations\n",
    "    response_options_array = np.array(list(response_option_counts.values()))\n",
    "    response_options_array = response_options_array / response_options_array.sum()\n",
    "    max_response_freq = response_options_array.max()\n",
    "    max_response_freqs.append(max_response_freq)\n",
    "    print('| Time:', round(duration, 1),\n",
    "          '| Acc:', round(100 * hit_result['accuracy'][prompt_type]),\n",
    "          '| Max Freq:', round(100 * max_response_freq, 1),\n",
    "          '| Rate:', hit_result['task_rating'],\n",
    "          '| Feedback:', hit_result['feedback'])\n",
    "\n",
    "durations = np.array(durations)\n",
    "durations_by_worker = np.array(durations_by_worker)\n",
    "max_response_freqs = np.array(max_response_freqs)\n",
    "durations.sort()\n",
    "durations_by_worker.sort()\n",
    "max_response_freqs.sort()\n",
    "for qtype in accuracy_by_worker:\n",
    "    accuracy_by_worker[qtype] = np.array(accuracy_by_worker[qtype])\n",
    "    accuracy_by_worker[qtype].sort()\n",
    "print('REJECTED:', num_rejected_hits)\n",
    "print('INCOMPLETE:', num_incomplete_hits)\n",
    "print('VALID:', num_valid_hits)\n",
    "print('Median Question Duration:', np.median(durations))\n",
    "print('Median Worker Duration:', np.median(durations_by_worker))\n",
    "print('Median Worker Accuracy:', np.median(accuracy_by_worker[prompt_type]))\n",
    "print('Median Max Response Freq:', np.median(max_response_freqs))\n",
    "# pprint(hit_results[0]['data'][0])\n",
    "pprint(hit_results[0]['worker_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evals per sample: 6.0\n",
      "Fraction insuffient evals: 0.0\n",
      "Convinced: nan %\n",
      "- Correct debater: nan %\n",
      "- Incorrect debater: nan %\n",
      "Accuracy: 43.28 %\n",
      "- Correct debater: nan %\n",
      "- Incorrect debater: nan %\n",
      "Extra Evals: 16.67 %\n",
      "Evals per sample distribution: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ethanperez/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:42: RuntimeWarning: Mean of empty slice.\n",
      "/Users/ethanperez/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/ethanperez/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:44: RuntimeWarning: Mean of empty slice.\n",
      "/Users/ethanperez/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:46: RuntimeWarning: Mean of empty slice.\n",
      "/Users/ethanperez/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:51: RuntimeWarning: Mean of empty slice.\n",
      "/Users/ethanperez/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:53: RuntimeWarning: Mean of empty slice.\n"
     ]
    }
   ],
   "source": [
    "accuracy_by_sample = []\n",
    "accuracy_by_sample_correct_debate_mode = []\n",
    "accuracy_by_sample_incorrect_debate_mode = []\n",
    "convinced_freqs = []\n",
    "convinced_freqs_with_correct_debate_mode = []\n",
    "convinced_freqs_with_incorrect_debate_mode = []\n",
    "num_evals_by_sample = []\n",
    "for qid, qid_metrics in metrics.items():\n",
    "    answer = metrics[qid]['answer']\n",
    "    for model_stance, prompt in qid_metrics.items():\n",
    "        if not (model_stance in [None] + options):\n",
    "            continue\n",
    "\n",
    "        # Q-only stats\n",
    "        prompt_metrics = metrics[qid][model_stance]\n",
    "        num_evals_by_sample.append(prompt_metrics['num'])\n",
    "        accuracy_by_sample.append(prompt_metrics['num_correct'] / prompt_metrics['num'])\n",
    "        if model_stance is None:\n",
    "            continue\n",
    "        \n",
    "        # Debater stats\n",
    "        convinced_freq = prompt_metrics['num_debate_mode_responses'] / prompt_metrics['num']\n",
    "        if model_stance == answer:\n",
    "            convinced_freqs_with_correct_debate_mode.append(convinced_freq)\n",
    "            accuracy_by_sample_correct_debate_mode.append(\n",
    "                prompt_metrics['num_correct_with_correct_debate_mode'] /\n",
    "                prompt_metrics['num_correct_debate_mode'])\n",
    "        else:\n",
    "            convinced_freqs_with_incorrect_debate_mode.append(convinced_freq)\n",
    "            accuracy_by_sample_incorrect_debate_mode.append(\n",
    "                prompt_metrics['num_correct_with_incorrect_debate_mode'] /\n",
    "                prompt_metrics['num_incorrect_debate_mode'])\n",
    "        convinced_freqs.append(convinced_freq)\n",
    "\n",
    "worker_ids = set(worker_ids)\n",
    "        \n",
    "num_evals_by_sample = np.array(num_evals_by_sample)\n",
    "print('Evals per sample:', num_evals_by_sample.mean())\n",
    "print('Fraction insuffient evals:', (num_evals_by_sample < 5).mean())\n",
    "\n",
    "convinced_freqs = np.array(convinced_freqs)\n",
    "print('Convinced:', round(100 * convinced_freqs.mean(), 2), '%')\n",
    "convinced_freqs_with_correct_debate_mode = np.array(convinced_freqs_with_correct_debate_mode)\n",
    "print('- Correct debater:', round(100 * convinced_freqs_with_correct_debate_mode.mean(), 2), '%')\n",
    "convinced_freqs_with_incorrect_debate_mode = np.array(convinced_freqs_with_incorrect_debate_mode)\n",
    "print('- Incorrect debater:', round(100 * convinced_freqs_with_incorrect_debate_mode.mean(), 2), '%')\n",
    "\n",
    "accuracy_by_sample = np.array(accuracy_by_sample)\n",
    "print('Accuracy:', round(100 * accuracy_by_sample.mean(), 2), '%')\n",
    "accuracy_by_sample_correct_debate_mode = np.array(accuracy_by_sample_correct_debate_mode)\n",
    "print('- Correct debater:', round(100 * accuracy_by_sample_correct_debate_mode.mean(), 2), '%')\n",
    "accuracy_by_sample_incorrect_debate_mode = np.array(accuracy_by_sample_incorrect_debate_mode)\n",
    "print('- Incorrect debater:', round(100 * accuracy_by_sample_incorrect_debate_mode.mean(), 2), '%')\n",
    "\n",
    "num_target_evals = 5\n",
    "print('Extra Evals:', round(((100. * (num_evals_by_sample - num_target_evals).sum()) / num_evals_by_sample.sum()), 2), '%')\n",
    "num_evals_by_sample.sort()\n",
    "print('Evals per sample distribution:', num_evals_by_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "NPS, Mean: (23.33, 8.03)\n"
     ]
    }
   ],
   "source": [
    "def nps(task_ratings):\n",
    "    num_ratings = sum(list(task_ratings.values()))\n",
    "    print(num_ratings)\n",
    "    if num_ratings == 0:\n",
    "        return None\n",
    "\n",
    "    nps_sum_ratings = 0\n",
    "    sum_ratings = 0\n",
    "    for score, num_raters in task_ratings.items():\n",
    "        sum_ratings += num_raters * score\n",
    "        if score >= 9:\n",
    "            nps_sum_ratings += num_raters\n",
    "        elif score <= 6:\n",
    "            nps_sum_ratings -= num_raters\n",
    "    return round(100 * (nps_sum_ratings / float(num_ratings)), 2), round((sum_ratings / float(num_ratings)), 2)\n",
    "\n",
    "print('NPS, Mean:', nps(task_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.0275,  4.4515,  5.2565,  5.6405,  5.6475,  5.703 ,  6.156 ,\n",
       "        6.2705,  6.6475,  6.8395,  7.2245,  7.317 ,  7.4135,  7.7045,\n",
       "        7.7315,  7.917 ,  7.932 ,  8.148 ,  8.1855,  8.6555,  8.87  ,\n",
       "        8.9845,  8.9915,  9.451 ,  9.4655,  9.6525, 10.696 , 11.7755,\n",
       "       12.808 , 18.698 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "durations_by_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4366666666666667"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_by_worker[prompt_type].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

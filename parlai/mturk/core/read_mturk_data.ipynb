{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "num_evals = 5\n",
    "evals = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# HIT Files: 116\n",
      "# Passed Test: 39 / 77 = 50.65 %\n"
     ]
    }
   ],
   "source": [
    "task_dir = '/Users/ethanperez/research/ParlAI/parlai/mturk/core/run_data/live/context_evaluator_'\n",
    "\n",
    "### RACE: Unfiltered Workers\n",
    "# prompt_type, task_id = 'question', '1553982706'  # Q-only\n",
    "# prompt_type, task_id = 'context_question', 1553790696  # TFIDF\n",
    "# prompt_type, task_id = 'context_question', 1553901953  # FastText\n",
    "### RACE: Filtered Workers\n",
    "# prompt_type, task_id = 'question', '1554052233'  # Q-only\n",
    "# prompt_type, task_id = 'quote and question', 1554006689  # TFIDF-QA\n",
    "# prompt_type, task_id = 'quote and question', 1554130485  # TFIDF-A\n",
    "# prompt_type, task_id = 'quote and question', 1554069931  # Cross-Ranker\n",
    "# prompt_type, task_id = 'quote and question', 1554072277  # Predicting Search\n",
    "# prompt_type, task_id = 'quote and question', 1554132868  # Predicting ΔP(A)\n",
    "\n",
    "### RACE Test\n",
    "## Persuading\n",
    "# prompt_type, task_id, name = 'quote and question', 1556832343, 'Pred Search (Almost complete)'\n",
    "# prompt_type, task_id, name = 'quote and question', 1556671432, 'TFIDF-QA'\n",
    "# prompt_type, task_id, name = 'quote and question', 1556725767, 'TFIDF-A'\n",
    "# prompt_type, task_id, name = 'quote and question', 1556739336, 'BoW-A'\n",
    "# prompt_type, task_id, name = 'quote and question', 1557155351, 'Bi-Ranker'\n",
    "# prompt_type, task_id, name = 'quote and question', 1557144204, 'Cross-Ranker'\n",
    "# prompt_type, task_id, name = 'quote and question', 1556892630, 'Pred Search'\n",
    "# prompt_type, task_id, name = 'quote and question', 1556809031, 'Pred P(A)'\n",
    "# prompt_type, task_id, name = 'quote and question', 1556756789, 'Pred ΔP(A)'  # (race.m=sl-sents.i.best.e)\n",
    "## Acc on Summary\n",
    "# prompt_type, task_id, name = 'passage and question', 1555823963, 'Full Passage'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1555946909, 'BoW-A'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1555952058, 'Cross-Ranker Best Epoch'  # (6-10 sentence incorrectly placed at end)\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556939750, 'Pred ΔP(A) (lower pay)'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556977072, 'Pred P(A)'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556987177, 'Pred Search'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556999857, 'Pred ΔP(A)'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1557085110, 'Cross-Ranker'  # Last Epoch\n",
    "# prompt_type, task_id, name = 'quotes and question', 1557093902, 'Bi-Ranker'\n",
    "prompt_type, task_id, name = 'quotes and question', 1557164593, 'TFIDF-A'\n",
    "# prompt_type, task_id, name = 'quotes and question', ?, 'TFIDF-QA'\n",
    "dataset = 'race'\n",
    "\n",
    "### DREAM\n",
    "## Persuading\n",
    "# prompt_type, task_id, name = 'question', 1554582693, 'Q-only'\n",
    "# prompt_type, task_id, name = 'quote and question', 1554596686, 'TFIDF-QA'\n",
    "# prompt_type, task_id, name = 'quote and question', 1554587404, 'TFIDF-A'\n",
    "# prompt_type, task_id, name = 'quote and question', 1554662280, 'BoW-A'\n",
    "# prompt_type, task_id, name = 'quote and question', 1556670413, 'Bi-Ranker'\n",
    "# prompt_type, task_id, name = 'quote and question', 1554675304, 'Cross-Ranker'\n",
    "# prompt_type, task_id, name = 'quote and question', 1554685131, 'Pred Search'\n",
    "# prompt_type, task_id, name = 'quote and question', 1554692472, 'Pred P(A)'\n",
    "# prompt_type, task_id, name = 'quote and question', 1554729998, 'Pred ΔP(A)'\n",
    "# prompt_type, task_id, name = 'quote and question', 1555333992, 'Pred Search (ToM)'\n",
    "## Acc on Summary\n",
    "# prompt_type, task_id, name = 'question, answers, and quotes', 1555707929, 'TFIDF-A'  # 64.21%: (Less filter / no feedback)\n",
    "# prompt_type, task_id, name = 'question, answers, and quotes', 1555722489, 'Cross-Ranker'  # 65.38%: (Less filter / no feedback)\n",
    "# prompt_type, task_id, name = 'question and quotes', 1555789302, 'Pred Search'  # 75.17% (4/5 filter)\n",
    "# prompt_type, task_id, name = 'question and quotes', 1555812443, 'Pred Search'  # 79.32% Actually: quotes and question (4/5 filter)\n",
    "# prompt_type, task_id, name = 'passage and question', 1555804551, 'Full Passage'  # 92.97%\n",
    "# prompt_type, task_id, name = 'quotes and question', 1555823257, 'FastText'  # (5/5 filter) (77.33%)\n",
    "# prompt_type, task_id, name = 'quotes and question', 1555946647, 'RACE Cross-Ranker'  # (4 sentences incorrectly placed at end) (80.84%)\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556727396, 'Cross-Ranker'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556740293, 'Bi-Ranker'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556757043, 'TFIDF-A'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556811067, 'TFIDF-QA'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556832115, 'Predicting Search'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556892896, 'Predicting P(A)'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556938429, 'Predicting ΔP(A)'\n",
    "# dataset = 'dream'\n",
    "\n",
    "\n",
    "split = None  # 'middle', 'high', None\n",
    "\n",
    "\n",
    "# Set useful variables\n",
    "task_dir += str(task_id)\n",
    "if dataset != 'race':\n",
    "    split = None\n",
    "num_options = 3 if dataset == 'dream' else 4\n",
    "options = ['A', 'B', 'C', 'D'][:num_options]\n",
    "debate_mode_to_option = {'Ⅰ': 'A', 'Ⅱ': 'B', 'Ⅲ': 'C', 'Ⅳ': 'D', 'ⅰ': 'A', 'ⅱ': 'B', 'ⅲ': 'C', 'ⅳ': 'D', None: None}\n",
    "question_type_labels = ['a', 'c', 'l', 'm', 's'] if dataset == 'dream' else ['a', 'b', 'c', 'd', 'e']\n",
    "\n",
    "# if (dataset == 'dream') and (prompt_type == 'quote and question'):\n",
    "#     question_type_labels = []\n",
    "\n",
    "# Read HIT data\n",
    "print('# HIT Files:', len(os.listdir(task_dir)))\n",
    "hit_results = []\n",
    "num_passed_test = 0\n",
    "for hit_dir in os.listdir(task_dir):\n",
    "    if hit_dir.startswith('o_'):\n",
    "        continue\n",
    "    num_passed_test += 1\n",
    "    with open(os.path.join(task_dir, hit_dir, 'custom/data.json'), 'r') as file:\n",
    "        hit_results.append(json.load(file))\n",
    "        file.close()\n",
    "if len(os.listdir(task_dir)) != num_passed_test:\n",
    "    num_total_tested = len(os.listdir(task_dir)) - num_passed_test\n",
    "    print('# Passed Test:', num_passed_test, '/', num_total_tested, '=', round((100. * num_passed_test) / num_total_tested, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Time: 693.5 | Acc: 70 | Max Freq: 30.0 | Rate: 10 | Feedback: Make sure answers are i line with context of questions | Quote Rating: 8 | Quote Desc: Mostly helpful and in context, but some were not so great.\n",
      "| Time: 434.9 | Acc: 85 | Max Freq: 50.0 | Rate: 10 | Feedback: It was an enjoyable task. Being able to get all answers from the quotes would be better. | Quote Rating: 9 | Quote Desc: Most of them was easily understandable but a few of them did not help much with the answer.\n",
      "| Time: 999.7 | Acc: 60 | Max Freq: 35.0 | Rate: 6 | Feedback: Many of the answers seemed very similar, so maybe make them completely different. | Quote Rating: 3 | Quote Desc: Many of the passage quotes that were provided made it harder to figure out the right answer. I felt like more than one answer could have been correct,\n",
      "| Time: 1433.6 | Acc: 75 | Max Freq: 35.0 | Rate: 10 | Feedback: a little more info | Quote Rating: 7 | Quote Desc: somewhat accurate and helpfl\n",
      "| Time: 564.3 | Acc: 70 | Max Freq: 35.0 | Rate: 10 | Feedback: state how many questions we will answer total | Quote Rating: 9 | Quote Desc: mostly helpful and on topic\n",
      "| Time: 707.0 | Acc: 65 | Max Freq: 35.0 | Rate: 10 | Feedback: increase pay slightly or use more quotes that help answer the questions more easily | Quote Rating: 7 | Quote Desc: some more helpful than others\n",
      "| Time: 709.0 | Acc: 70 | Max Freq: 35.0 | Rate: 8 | Feedback: I think it's fine the way it is. I'm guessing it's intentional that some passages are vague. | Quote Rating: 6 | Quote Desc: Some of them were not detailed enough to know how to fully answer the question. The last one was a good example of not enough context.\n",
      "| Time: 505.6 | Acc: 80 | Max Freq: 45.0 | Rate: 10 | Feedback: I don't think it needs to be improved, it is pretty awesome as is. | Quote Rating: 10 | Quote Desc: I thought they were helpful to know and learn from\n",
      "| Time: 867.1 | Acc: 70 | Max Freq: 35.0 | Rate: 10 | Feedback: explain why an answer was wrong if the incorrect option was selected | Quote Rating: 10 | Quote Desc: Information taken from articles\n",
      "| Time: 1052.1 | Acc: 75 | Max Freq: 35.0 | Rate: 2 | Feedback: pay more | Quote Rating: 4 | Quote Desc: Incomplete or irrelevant\n",
      "| Time: 712.6 | Acc: 60 | Max Freq: 35.0 | Rate: 6 | Feedback: Give more information in the quotes | Quote Rating: 7 | Quote Desc: vague\n",
      "| Time: 683.2 | Acc: 55 | Max Freq: 45.0 | Rate: 10 | Feedback: awesome | Quote Rating: 10 | Quote Desc: awesome\n",
      "| Time: 737.8 | Acc: 70 | Max Freq: 35.0 | Rate: 10 | Feedback: Some of the questions were rather easy and some were very difficult. Sometimes just the wording was confusing.  | Quote Rating: 7 | Quote Desc: They were very concise descriptions and snippets of a larger paragraph. \n",
      "| Time: 376.7 | Acc: 80 | Max Freq: 30.0 | Rate: 8 | Feedback: Make some of the choices more obvious and less vague (it seemed like more than option would fit the question being asked in many cases). | Quote Rating: 8 | Quote Desc: Some of them seemed deliberately misleading and vague, but most of them were helpful.\n",
      "| Time: 1180.1 | Acc: 85 | Max Freq: 40.0 | Rate: 7 | Feedback: I'm not sure other than upping the pay slighly | Quote Rating: 7 | Quote Desc: Some of them were really vague, but still had context clues of some sort\n",
      "| Time: 499.5 | Acc: 85 | Max Freq: 50.0 | Rate: 9 | Feedback: nothing i can think of! | Quote Rating: 9 | Quote Desc: mainly useful and coherent, sometimes ambiguous \n",
      "| Time: 501.2 | Acc: 55 | Max Freq: 30.0 | Rate: 10 | Feedback: I don't know. I suspect you are testing how much I can get out of incomplete text, so I don't think you should change anything. | Quote Rating: 5 | Quote Desc: Most were well formed but some were not complete or seemingly missing information.\n",
      "| Time: 796.3 | Acc: 90 | Max Freq: 40.0 | Rate: 10 | Feedback: Cant think of anything, i thought it was great! | Quote Rating: 9 | Quote Desc: They are often broken up, yet somehow related\n",
      "| Time: 472.0 | Acc: 85 | Max Freq: 35.0 | Rate: 8 | Feedback: give better quotes for the unclear passages, or include more information that will help elucidate.  | Quote Rating: 7 | Quote Desc: scattered and inconsistent - some were very helpful, and some were not. \n",
      "| Time: 747.6 | Acc: 85 | Max Freq: 35.0 | Rate: 5 | Feedback: for some reason the chat screen was vibrating, and it was annoying.  | Quote Rating: 4 | Quote Desc: Choppy, broken English. Misleading occasionally. Sparse. \n",
      "| Time: 1019.0 | Acc: 70 | Max Freq: 35.0 | Rate: 8 | Feedback: Check some of the grammar. | Quote Rating: 6 | Quote Desc: Most gave just enough information to guess the answer, but there were a few where I have no idea how I was supposed to get it.\n",
      "| Time: 874.0 | Acc: 75 | Max Freq: 40.0 | Rate: 10 | Feedback: make answers less subjective | Quote Rating: 7 | Quote Desc: disconnected\n",
      "| Time: 1271.8 | Acc: 70 | Max Freq: 40.0 | Rate: 9 | Feedback: The task seemed pretty good but maybe try to reduce the amount of questions that require a complete guess. | Quote Rating: 7 | Quote Desc: A lot of the passage quotes helped quite a bit but there a few that asked a question about something that wasn't even mentioned in the passage so I had to completely guess.\n",
      "| Time: 884.8 | Acc: 70 | Max Freq: 40.0 | Rate: 9 | Feedback: Not for sure, it is quite good as is. | Quote Rating: 8 | Quote Desc: Sometimes ambiguous but helpful\n",
      "| Time: 1370.7 | Acc: 80 | Max Freq: 30.0 | Rate: 9 | Feedback: If not for all questions, at least for the yes/no questions I would suggest radio buttons as the drop down has a delay.  | Quote Rating: 8 | Quote Desc: disjointed\n",
      "| Time: 805.2 | Acc: 65 | Max Freq: 35.0 | Rate: 10 | Feedback: Offer a guarenteed bonus for each correct question, that would be more motivating. | Quote Rating: 7 | Quote Desc: They were short and had important details, but a few were not good at all for helping one answer the questions.\n",
      "| Time: 730.3 | Acc: 75 | Max Freq: 40.0 | Rate: 8 | Feedback: I think it's fine the way it is | Quote Rating: 7 | Quote Desc: I think the majority of them were pretty straightforward\n",
      "| Time: 937.7 | Acc: 65 | Max Freq: 35.0 | Rate: 7 | Feedback: Better quotes | Quote Rating: 7 | Quote Desc: Some were helpful while others were confusing.\n",
      "| Time: 392.6 | Acc: 55 | Max Freq: 50.0 | Rate: 4 | Feedback: Better quotes. | Quote Rating: 5 | Quote Desc: Vague and sometimes not related to the main point.\n",
      "| Time: 1020.9 | Acc: 70 | Max Freq: 30.0 | Rate: 8 | Feedback: provide more passage quotes | Quote Rating: 6 | Quote Desc: some were locking information\n",
      "| Time: 766.0 | Acc: 70 | Max Freq: 40.0 | Rate: 10 | Feedback: Don't know. | Quote Rating: 8 | Quote Desc: Some of them were pretty tough, given the limited information.\n",
      "| Time: 393.1 | Acc: 85 | Max Freq: 50.0 | Rate: 10 | Feedback: It's a pretty cool task the way it is.  It has made me think like no other HIT I've ever done. | Quote Rating: 7 | Quote Desc: They were sometimes helpful and sometimes puzzling - there were cases where the answer seemed to contradict the quotes.\n",
      "| Time: 916.4 | Acc: 85 | Max Freq: 35.0 | Rate: 0 | Feedback: The bonus incentive makes no sense. You have to have greater than average accuracy but your accuracy is dependent on how many shitty passages you get. So it feels like no matter how hard you try you will get fucked over. | Quote Rating: 6 | Quote Desc: They were either obvious or left the reader doing guesswork. I wouldn't use the system for anything important.\n",
      "| Time: 850.6 | Acc: 70 | Max Freq: 30.0 | Rate: 5 | Feedback: Pay more | Quote Rating: 8 | Quote Desc: I really dont know\n",
      "REJECTED: 0\n",
      "INCOMPLETE: 5\n",
      "VALID: 34\n",
      "Median Question Duration: 30.387500000000003\n",
      "Mean Question Duration: 33.859726102941174\n",
      "Min/Median/Mean/Max Worker Duration: 6.28 / 12.61 / 13.19 / 23.89\n",
      "Min/Median/Mean/Max Good Worker Durations: 6.28 / 12.46 / 13.09 / 23.89\n",
      "Median Worker Accuracy: 0.7\n",
      "Median Max Response Freq: 0.35\n",
      "Quote Rating: | Mean: 7.15 | Median: 7.0 | Std: 1.67\n"
     ]
    }
   ],
   "source": [
    "num_valid_hits, num_rejected_hits, num_incomplete_hits = 0, 0, 0\n",
    "metrics = {}\n",
    "task_ratings = {i: 0 for i in range(11)}\n",
    "quote_ratings = []\n",
    "durations = []\n",
    "worker_durations = []\n",
    "accuracy_by_worker = {}\n",
    "max_response_freqs = []\n",
    "worker_ids = []\n",
    "hits_by_qid = {}\n",
    "\n",
    "for hit_result in hit_results:\n",
    "    if ((len(hit_result['reject_reasons']) > 0) or\n",
    "        (len(hit_result['block_reasons']) > 0)):\n",
    "        num_rejected_hits += 1\n",
    "        print(hit_result['worker_id'], hit_result['assignment_id'],\n",
    "              '| reject_reasons:', hit_result['reject_reasons'],\n",
    "              '| block_reasons:', hit_result['block_reasons'],\n",
    "              '| bonus_reasons: ' + str(hit_result['bonus_reasons']) if 'bonus_reasons' in hit_result else '')\n",
    "    elif hit_result['feedback'] is None:\n",
    "        num_incomplete_hits += 1\n",
    "        continue\n",
    "    \n",
    "    worker_ids.append(hit_result['worker_id'])\n",
    "    num_valid_hits += 1\n",
    "    if (hit_result['task_rating'] is not None) and (hit_result['task_rating'].isdigit()):\n",
    "        task_ratings[int(hit_result['task_rating'])] += 1\n",
    "    if (hit_result.get('quote_rating') is not None) and (hit_result['quote_rating'].isdigit()):\n",
    "        quote_ratings.append(int(hit_result['quote_rating']))\n",
    "    for qtype, qtype_accuracy in hit_result['accuracy'].items():\n",
    "        accuracy_by_worker[qtype] = accuracy_by_worker.get(qtype, []) + [qtype_accuracy]\n",
    "    \n",
    "    hit_durations = []\n",
    "    response_option_counts = {option: 0 for option in options}\n",
    "    responses = []\n",
    "    for prompt in hit_result['data']:\n",
    "        qid = prompt['sample']['qid']\n",
    "        if (split is not None) and (split not in qid):\n",
    "            continue\n",
    "        hits_by_qid[qid] = hits_by_qid.get(qid, [])\n",
    "        hits_by_qid[qid].append(prompt)\n",
    "        model_stance = debate_mode_to_option[prompt['sample']['debate_mode']]\n",
    "        answer = prompt['sample']['eval_labels'][0]\n",
    "        human_correct = (prompt['response'] == answer)\n",
    "        assert answer in options, 'Answer must be in options.'\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if qid not in metrics:\n",
    "            metrics[qid] = {\n",
    "                option: {\n",
    "                    'num': 0,\n",
    "                    'num_correct': 0,\n",
    "                    'num_correct_debate_mode': 0,\n",
    "                    'num_incorrect_debate_mode': 0,\n",
    "                    'num_correct_with_correct_debate_mode': 0,\n",
    "                    'num_correct_with_incorrect_debate_mode': 0,\n",
    "                    'num_debate_mode_responses': 0,\n",
    "                    'is_debate_mode_response': []\n",
    "                }\n",
    "                for option in ([None] if model_stance is None else options)\n",
    "            }\n",
    "            for qtype in question_type_labels:\n",
    "                metrics[qid][qtype] = {\n",
    "                    'num': 0,\n",
    "                    'num_correct': 0,\n",
    "                }\n",
    "            metrics[qid]['answer'] = answer\n",
    "        for qtype in set(''.join(prompt['sample'].get('question_type_labels', []))):\n",
    "            qtype = qtype.lower()\n",
    "            metrics[qid][qtype]['num'] += 1\n",
    "            metrics[qid][qtype]['num_correct'] += human_correct\n",
    "        prompt_metrics = metrics[qid][model_stance]\n",
    "        prompt_metrics['num'] += 1\n",
    "        prompt_metrics['num_correct'] += human_correct\n",
    "        if model_stance == answer:\n",
    "            prompt_metrics['num_correct_with_correct_debate_mode'] += human_correct\n",
    "            prompt_metrics['num_correct_debate_mode'] += 1\n",
    "        else:\n",
    "            prompt_metrics['num_correct_with_incorrect_debate_mode'] += human_correct\n",
    "            prompt_metrics['num_incorrect_debate_mode'] += 1\n",
    "        prompt_metrics['num_debate_mode_responses'] += (prompt['response'] == model_stance)\n",
    "        prompt_metrics['is_debate_mode_response'].append(prompt['response'] == model_stance)\n",
    "        \n",
    "        hit_durations.append(prompt['duration'] / 1000.)\n",
    "        response_option_counts[prompt['response']] += 1\n",
    "        responses.append(prompt['response'])\n",
    "    duration = np.sum(np.array(hit_durations))\n",
    "    worker_durations.append(duration)\n",
    "    durations += hit_durations\n",
    "    response_options_array = np.array(list(response_option_counts.values()))\n",
    "    response_options_array = response_options_array / response_options_array.sum()\n",
    "    max_response_freq = response_options_array.max()\n",
    "    max_response_freqs.append(max_response_freq)\n",
    "    acc = round(100 * hit_result['accuracy'][prompt_type])\n",
    "    print('| Time:', round(duration, 1),\n",
    "          '| Acc:', acc,\n",
    "          '| Max Freq:', round(100 * max_response_freq, 1),\n",
    "          '| Rate:', hit_result['task_rating'],\n",
    "          '| Feedback:', hit_result['feedback'],\n",
    "          '| Quote Rating:', None if 'quote_rating' not in hit_result else hit_result['quote_rating'], \n",
    "          '| Quote Desc:', None if 'quote_description' not in hit_result else hit_result['quote_description'])\n",
    "\n",
    "good_worker_durations = []\n",
    "assert len(worker_durations) == len(accuracy_by_worker[prompt_type])\n",
    "for worker_duration, worker_accuracy in zip(worker_durations, accuracy_by_worker[prompt_type]):\n",
    "    if worker_accuracy > np.median(np.array(accuracy_by_worker[prompt_type])):\n",
    "        good_worker_durations.append(worker_duration)\n",
    "\n",
    "quote_ratings = np.array(quote_ratings)\n",
    "durations = np.array(durations)\n",
    "worker_durations = np.array(worker_durations)\n",
    "good_worker_durations = np.array(good_worker_durations)\n",
    "max_response_freqs = np.array(max_response_freqs)\n",
    "\n",
    "quote_ratings.sort()\n",
    "durations.sort()\n",
    "worker_durations.sort()\n",
    "good_worker_durations.sort()\n",
    "max_response_freqs.sort()\n",
    "\n",
    "for qtype in accuracy_by_worker:\n",
    "    accuracy_by_worker[qtype] = np.array(accuracy_by_worker[qtype])\n",
    "    accuracy_by_worker[qtype].sort()\n",
    "print('REJECTED:', num_rejected_hits)\n",
    "print('INCOMPLETE:', num_incomplete_hits)\n",
    "print('VALID:', num_valid_hits)\n",
    "print('Median Question Duration:', np.median(durations))\n",
    "print('Mean Question Duration:', np.mean(durations[int(durations.shape[0] / 10.):int(9. * durations.shape[0] / 10.)]))\n",
    "print('Min/Median/Mean/Max Worker Duration:',\n",
    "      round(np.min(worker_durations / 60.), 2), '/',\n",
    "      round(np.median(worker_durations / 60.), 2), '/',\n",
    "      round(np.mean(worker_durations / 60.), 2), '/',\n",
    "      round(np.max(worker_durations / 60.), 2))\n",
    "print('Min/Median/Mean/Max Good Worker Durations:',\n",
    "      round(np.min(good_worker_durations / 60.), 2), '/',\n",
    "      round(np.median(good_worker_durations / 60.), 2),'/',\n",
    "      round(np.mean(good_worker_durations / 60.), 2), '/',\n",
    "      round(np.max(good_worker_durations / 60.), 2))\n",
    "print('Median Worker Accuracy:', np.median(accuracy_by_worker[prompt_type]))\n",
    "print('Median Max Response Freq:', np.median(max_response_freqs))\n",
    "print('Quote Rating:',\n",
    "      '| Mean:', round(quote_ratings.mean(), 2),\n",
    "      '| Median:', round(np.median(quote_ratings), 2),\n",
    "      '| Std:', round(np.std(quote_ratings), 2))\n",
    "# pprint(hit_results[0]['data'][0])\n",
    "# pprint(hit_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evals per sample: 6.8\n",
      "Fraction insuffient evals: 0.0\n",
      "Convinced: nan %\n",
      "- Correct debater: nan %\n",
      "- Incorrect debater: nan %\n",
      "Accuracy: 73.22 %\n",
      "- Correct debater: nan %\n",
      "- Incorrect debater: nan %\n",
      "Extra Evals: 26.47 %\n",
      "Evals per sample distribution: [6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "Accuracy/Num-Samples by Q Type:\n",
      "{'a': (0.828125, 24),\n",
      " 'b': (0.7423735119047619, 32),\n",
      " 'c': (0.7754629629629629, 54),\n",
      " 'd': (0.6999101527403414, 53),\n",
      " 'e': (0.5528273809523809, 16)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:54: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:56: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:58: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:63: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:65: RuntimeWarning: Mean of empty slice.\n"
     ]
    }
   ],
   "source": [
    "accuracy_by_qtype = {qtype: [] for qtype in question_type_labels}\n",
    "accuracy_by_sample = []\n",
    "accuracy_by_sample_correct_debate_mode = []\n",
    "accuracy_by_sample_incorrect_debate_mode = []\n",
    "convinced_freqs = []\n",
    "convinced_freqs_with_correct_debate_mode = []\n",
    "convinced_freqs_with_incorrect_debate_mode = []\n",
    "num_evals_by_sample = []\n",
    "convinced_by_sample = []\n",
    "for qid, qid_metrics in metrics.items():\n",
    "    answer = metrics[qid]['answer']\n",
    "    for qid_metric_key, prompt in qid_metrics.items():\n",
    "        if qid_metric_key in question_type_labels:\n",
    "            qtype = qid_metric_key\n",
    "            if qid_metrics[qtype]['num'] > 0:\n",
    "                accuracy_by_qtype[qtype].append(qid_metrics[qtype]['num_correct'] / qid_metrics[qtype]['num'])\n",
    "            continue\n",
    "        if not (qid_metric_key in [None] + options):\n",
    "            continue\n",
    "        model_stance = qid_metric_key\n",
    "\n",
    "        # Q-only stats\n",
    "        prompt_metrics = metrics[qid][model_stance]\n",
    "        num_evals_by_sample.append(prompt_metrics['num'])\n",
    "        convinced_by_sample.append(prompt_metrics['is_debate_mode_response'])\n",
    "        accuracy_by_sample.append(prompt_metrics['num_correct'] / prompt_metrics['num'])\n",
    "        if model_stance is None:\n",
    "            continue\n",
    "        \n",
    "        # Debater stats\n",
    "#         if 'num_debate_mode_responses' not in prompt_metrics:\n",
    "#             print(qid_metric_key, prompt_metrics)\n",
    "        convinced_freq = prompt_metrics['num_debate_mode_responses'] / prompt_metrics['num']\n",
    "        if model_stance == answer:\n",
    "            convinced_freqs_with_correct_debate_mode.append(convinced_freq)\n",
    "            accuracy_by_sample_correct_debate_mode.append(\n",
    "                prompt_metrics['num_correct_with_correct_debate_mode'] /\n",
    "                prompt_metrics['num_correct_debate_mode'])\n",
    "        else:\n",
    "            convinced_freqs_with_incorrect_debate_mode.append(convinced_freq)\n",
    "            accuracy_by_sample_incorrect_debate_mode.append(\n",
    "                prompt_metrics['num_correct_with_incorrect_debate_mode'] /\n",
    "                prompt_metrics['num_incorrect_debate_mode'])\n",
    "        convinced_freqs.append(convinced_freq)\n",
    "\n",
    "accuracy_by_qtype = {qtype: (np.array(accuracy_by_qtype[qtype]).mean(), len(accuracy_by_qtype[qtype])) for qtype in question_type_labels}\n",
    "worker_ids = set(worker_ids)\n",
    "        \n",
    "num_evals_by_sample = np.array(num_evals_by_sample)\n",
    "print('Evals per sample:', num_evals_by_sample.mean())\n",
    "print('Fraction insuffient evals:', (num_evals_by_sample < 5).mean())\n",
    "\n",
    "convinced_freqs = np.array(convinced_freqs)\n",
    "print('Convinced:', round(100 * convinced_freqs.mean(), 2), '%')\n",
    "convinced_freqs_with_correct_debate_mode = np.array(convinced_freqs_with_correct_debate_mode)\n",
    "print('- Correct debater:', round(100 * convinced_freqs_with_correct_debate_mode.mean(), 2), '%')\n",
    "convinced_freqs_with_incorrect_debate_mode = np.array(convinced_freqs_with_incorrect_debate_mode)\n",
    "print('- Incorrect debater:', round(100 * convinced_freqs_with_incorrect_debate_mode.mean(), 2), '%')\n",
    "\n",
    "accuracy_by_sample = np.array(accuracy_by_sample)\n",
    "print('Accuracy:', round(100 * accuracy_by_sample.mean(), 2), '%')\n",
    "accuracy_by_sample_correct_debate_mode = np.array(accuracy_by_sample_correct_debate_mode)\n",
    "print('- Correct debater:', round(100 * accuracy_by_sample_correct_debate_mode.mean(), 2), '%')\n",
    "accuracy_by_sample_incorrect_debate_mode = np.array(accuracy_by_sample_incorrect_debate_mode)\n",
    "print('- Incorrect debater:', round(100 * accuracy_by_sample_incorrect_debate_mode.mean(), 2), '%')\n",
    "\n",
    "num_target_evals = 5\n",
    "print('Extra Evals:', round(((100. * (num_evals_by_sample - num_target_evals).sum()) / num_evals_by_sample.sum()), 2), '%')\n",
    "num_evals_by_sample.sort()\n",
    "print('Evals per sample distribution:', num_evals_by_sample)\n",
    "\n",
    "print('Accuracy/Num-Samples by Q Type:')\n",
    "pprint(accuracy_by_qtype)\n",
    "# 1.5*3.1*60/(917.5684545454544*26/(20*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPS, Mean: (35.29, 8.12)\n"
     ]
    }
   ],
   "source": [
    "def nps(task_ratings):\n",
    "    num_ratings = sum(list(task_ratings.values()))\n",
    "    if num_ratings == 0:\n",
    "        return None\n",
    "\n",
    "    nps_sum_ratings = 0\n",
    "    sum_ratings = 0\n",
    "    for score, num_raters in task_ratings.items():\n",
    "        sum_ratings += num_raters * score\n",
    "        if score >= 9:\n",
    "            nps_sum_ratings += num_raters\n",
    "        elif score <= 6:\n",
    "            nps_sum_ratings -= num_raters\n",
    "    return round(100 * (nps_sum_ratings / float(num_ratings)), 2), round((sum_ratings / float(num_ratings)), 2)\n",
    "\n",
    "print('NPS, Mean:', nps(task_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TFIDF-A': array([0., 0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "evals[name] = []\n",
    "for eval_no in range(num_evals):\n",
    "    evals[name].append([convinced_array[eval_no] for convinced_array in convinced_by_sample])\n",
    "\n",
    "evals[name] = 100. * np.array(evals[name]).mean(axis=1)\n",
    "pprint(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** TFIDF-A *****\n",
      "MEAN: 0.0\n",
      "STD: 0.0\n",
      "STDERR: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n, eval_values in evals.items():\n",
    "    print('*****', n, '*****')\n",
    "    print('MEAN:', round(eval_values.mean(), 1))\n",
    "    print('STD:', round(eval_values.std(), 2))\n",
    "    print('STDERR:', round(eval_values.std() / np.sqrt(eval_values.shape[0]), 2))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF-A / TFIDF-A : nan\n"
     ]
    }
   ],
   "source": [
    "for n1, eval_values1 in evals.items():\n",
    "    for n2, eval_values2 in evals.items():\n",
    "        print(n1, '/', n2, ':', round(ttest_ind(eval_values1, eval_values2, equal_var=False)[1], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "num_evals = 5\n",
    "evals = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualified: 41.77 %\n"
     ]
    }
   ],
   "source": [
    "task_dir = '/Users/ethanperez/research/ParlAI/parlai/mturk/core/run_data/live/context_evaluator_'\n",
    "\n",
    "task_setup = [\n",
    "# ## Persuading Humans (Dev)\n",
    "# ('race', 'question', 1554052233, 'No Passage'),\n",
    "# ('race', 'quote and question', 1554006689, 'TFIDF(S,[Q;A])'),\n",
    "# ('race', 'quote and question', 1554130485, 'TFIDF(S,A)'),\n",
    "# ('race', 'quote and question', 1554069931, 'BERT Base'),\n",
    "# ('race', 'quote and question', 1554072277, 'Pred search'),\n",
    "# ('race', 'quote and question', 1554132868, 'Pred Δp(A)'),\n",
    "# ## Persuading Humans\n",
    "# ('race', 'quote and question', 1556832343, 'Pred search (Almost complete)'),\n",
    "# ('race', 'quote and question', 1557155351, 'Bi-Ranker'),\n",
    "# ## Persuading Humans: FINAL\n",
    "# ('race', 'question', 1557351318, 'No Passage'),\n",
    "# ('race', 'quote and question', 1556671432, 'TFIDF(S,[Q;A])'),\n",
    "# ('race', 'quote and question', 1556725767, 'TFIDF(S,A)'),\n",
    "# ('race', 'quote and question', 1556739336, 'FastText(S,A)'),\n",
    "# ('race', 'quote and question', 1557144204, 'BERT Base'),\n",
    "# ('race', 'quote and question', 1556892630, 'Pred search'),\n",
    "# ('race', 'quote and question', 1556809031, 'Pred p(A)'),\n",
    "# ('race', 'quote and question', 1556756789, 'Pred Δp(A)'),  # (race.m=sl-sents.i.best.e)\n",
    "# ('race', 'quote and question', 1557420471, 'Human'),\n",
    "# ## Human QA on Summary\n",
    "# ('race', 'quotes and question', 1555946909, 'FastText(S,A) (lower pay)'),\n",
    "# ('race', 'quotes and question', 1555952058, 'BERT Base Best Epoch'),  # (6-10 sentence incorrectly placed at end)\n",
    "# ('race', 'quotes and question', 1556939750, 'Pred Δp(A) (lower pay)'),\n",
    "# ('race', 'quotes and question', 1557093902, 'Bi-Ranker'),\n",
    "# ## Human QA on Summary: FINAL\n",
    "# ('race', 'passage and question', 1555823963, 'Full Passage'),\n",
    "# ('race', 'quotes and question', 1557189780, 'TFIDF(S,[Q;A])'),\n",
    "# ('race', 'quotes and question', 1557164593, 'TFIDF(S,A)'),\n",
    "# ('race', 'quotes and question', 1557234076, 'FastText(S,A)'),\n",
    "# ('race', 'quotes and question', 1557085110, 'BERT Base'),  # Last Epoch\n",
    "# ('race', 'quotes and question', 1556987177, 'Pred search'),\n",
    "# ('race', 'quotes and question', 1556977072, 'Pred p(A)'),\n",
    "# ('race', 'quotes and question', 1556999857, 'Pred Δp(A)'),\n",
    "# ('race', 'quotes and question', 1557432288, 'Human'),\n",
    "# ## Persuading Humans\n",
    "# ('dream', 'quote and question', 1556670413, 'Bi-Ranker'),\n",
    "# ('dream', 'quote and question', 1555333992, 'Pred search (ToM)'),\n",
    "# ## Persuading Humans: FINAL\n",
    "# ('dream', 'question', 1554582693, 'No Passage'),\n",
    "# ('dream', 'quote and question', 1554596686, 'TFIDF(S,[Q;A])'),\n",
    "# ('dream', 'quote and question', 1554587404, 'TFIDF(S,A)'),\n",
    "# ('dream', 'quote and question', 1554662280, 'FastText(S,A)'),\n",
    "# ('dream', 'quote and question', 1554675304, 'BERT Base'),\n",
    "# ('dream', 'quote and question', 1554685131, 'Pred search'),\n",
    "# ('dream', 'quote and question', 1554692472, 'Pred p(A)'),\n",
    "# ('dream', 'quote and question', 1554729998, 'Pred Δp(A)'),\n",
    "# ## Human QA on Summary\n",
    "# ('dream', 'question, answers, and quotes', 1555707929, 'TFIDF(S,A)'),  # 64.21%: (Less filter / no feedback)\n",
    "# ('dream', 'question, answers, and quotes', 1555722489, 'BERT Base'),  # 65.38%: (Less filter / no feedback)\n",
    "# ('dream', 'question and quotes', 1555789302, 'Pred search'),  # 75.17% (4/5 filter)\n",
    "# ('dream', 'question and quotes', 1555812443, 'Pred search'),  # 79.32% Actually: quotes and question (4/5 filter)\n",
    "# ('dream', 'quotes and question', 1555946647, 'BERT Base (RACE -> DREAM)'),  # (4 sentences incorrectly placed at end) (80.84%)\n",
    "# ('dream', 'quotes and question', 1556740293, 'Bi-Ranker'),\n",
    "# ## Human QA on Summary: FINAL\n",
    "('dream', 'passage and question', 1555804551, 'Full Passage'),\n",
    "# ('dream', 'quotes and question', 1556811067, 'TFIDF(S,[Q;A])'),\n",
    "# ('dream', 'quotes and question', 1556757043, 'TFIDF(S,A)'),\n",
    "# ('dream', 'quotes and question', 1555823257, 'FastText(S,A)'),\n",
    "# ('dream', 'quotes and question', 1556727396, 'BERT Base'),\n",
    "# ('dream', 'quotes and question', 1556832115, 'Pred search'),\n",
    "# ('dream', 'quotes and question', 1556892896, 'Pred p(A)'),\n",
    "# ('dream', 'quotes and question', 1556938429, 'Pred Δp(A)'),\n",
    "]\n",
    "\n",
    "dataset, prompt_type, task_id, name = task_setup[-1]\n",
    "split = None  # 'middle', 'high', None\n",
    "\n",
    "\n",
    "# Set useful variables\n",
    "task_dir += str(task_id)\n",
    "if dataset != 'race':\n",
    "    split = None\n",
    "num_options = 3 if dataset == 'dream' else 4\n",
    "options = ['A', 'B', 'C', 'D'][:num_options]\n",
    "debate_mode_to_option = {'Ⅰ': 'A', 'Ⅱ': 'B', 'Ⅲ': 'C', 'Ⅳ': 'D', 'ⅰ': 'A', 'ⅱ': 'B', 'ⅲ': 'C', 'ⅳ': 'D', None: None}\n",
    "question_type_labels = ['a', 'c', 'l', 'm', 's'] if dataset == 'dream' else ['a', 'b', 'c', 'd', 'e']\n",
    "\n",
    "# if (dataset == 'dream') and (prompt_type == 'quote and question'):\n",
    "#     question_type_labels = []\n",
    "\n",
    "# Read HIT data\n",
    "hit_results = []\n",
    "num_passed_test = 0\n",
    "for hit_dir in os.listdir(task_dir):\n",
    "    if hit_dir.startswith('o_'):\n",
    "        continue\n",
    "    num_passed_test += 1\n",
    "    with open(os.path.join(task_dir, hit_dir, 'custom/data.json'), 'r') as file:\n",
    "        hit_results.append(json.load(file))\n",
    "        file.close()\n",
    "if len(os.listdir(task_dir)) != num_passed_test:\n",
    "    num_total_tested = len(os.listdir(task_dir)) - num_passed_test\n",
    "    print('Qualified:', round((100. * num_passed_test) / num_total_tested, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Time: 515.5 | Acc: 100 | Max Freq: 65.0 | Rate: 10 | Feedback: Shorter intro questions | Quote Rating: None | Quote Desc: None\n",
      "| Time: 418.0 | Acc: 85 | Max Freq: 45.0 | Rate: 7 | Feedback: n/a it was ok | Quote Rating: None | Quote Desc: None\n",
      "| Time: 757.7 | Acc: 100 | Max Freq: 45.0 | Rate: 10 | Feedback: I think it is fine the way it is. | Quote Rating: None | Quote Desc: None\n",
      "| Time: 368.8 | Acc: 100 | Max Freq: 60.0 | Rate: 9 | Feedback: Nothing | Quote Rating: None | Quote Desc: None\n",
      "| Time: 749.7 | Acc: 90 | Max Freq: 40.0 | Rate: 10 | Feedback: Nothing | Quote Rating: None | Quote Desc: None\n",
      "| Time: 367.5 | Acc: 95 | Max Freq: 40.0 | Rate: 10 | Feedback: It is very interesting already! | Quote Rating: None | Quote Desc: None\n",
      "| Time: 490.6 | Acc: 95 | Max Freq: 40.0 | Rate: 10 | Feedback: Fix some of the weird text? I believe I saw an underscore in the middle of one of these randomly. | Quote Rating: None | Quote Desc: None\n",
      "| Time: 380.1 | Acc: 95 | Max Freq: 40.0 | Rate: 10 | Feedback: The task is fun! | Quote Rating: None | Quote Desc: None\n",
      "| Time: 453.6 | Acc: 100 | Max Freq: 45.0 | Rate: 10 | Feedback: I can't think of many ways to improve the task. I enjoyed it how it is. | Quote Rating: None | Quote Desc: None\n",
      "| Time: 362.7 | Acc: 90 | Max Freq: 40.0 | Rate: 5 | Feedback: A couple of the questions felt like they might have had 2 correct answers, but maybe I was just tired. | Quote Rating: None | Quote Desc: None\n",
      "| Time: 681.7 | Acc: 100 | Max Freq: 60.0 | Rate: 3 | Feedback: Shorten the passages for the five trial questions at the beginning. | Quote Rating: None | Quote Desc: None\n",
      "| Time: 640.8 | Acc: 95 | Max Freq: 45.0 | Rate: 10 | Feedback: not sure | Quote Rating: None | Quote Desc: None\n",
      "| Time: 421.6 | Acc: 100 | Max Freq: 45.0 | Rate: 10 | Feedback: I think the task is great just the way it is! | Quote Rating: None | Quote Desc: None\n",
      "| Time: 753.0 | Acc: 100 | Max Freq: 60.0 | Rate: 7 | Feedback: There are minor issues with scrolling as each new text appears. The page jumps up a little so that the top portion of the text has to be scrolled down to to see it. If this can be adjusted, it would be better. | Quote Rating: None | Quote Desc: None\n",
      "| Time: 896.6 | Acc: 90 | Max Freq: 55.0 | Rate: 10 | Feedback: It worked perfectly! Maybe instead of a drop down menu to select an option just have buttons that can be clicked for efficiency. | Quote Rating: None | Quote Desc: None\n",
      "| Time: 1970.2 | Acc: 80 | Max Freq: 45.0 | Rate: 7 | Feedback: The layout shouldn't take so long to come up, and the text is a little hard to read when it's a larger passage. This comes out to 7 cents per question. | Quote Rating: None | Quote Desc: None\n",
      "| Time: 618.0 | Acc: 95 | Max Freq: 60.0 | Rate: 10 | Feedback: I can't think of anything that needs changed | Quote Rating: None | Quote Desc: None\n",
      "| Time: 359.2 | Acc: 85 | Max Freq: 55.0 | Rate: 10 | Feedback: Smaller font so easier to scroll back up and see if your answer was right on the previous question. | Quote Rating: None | Quote Desc: None\n",
      "| Time: 299.7 | Acc: 75 | Max Freq: 45.0 | Rate: 3 | Feedback: Doing one question at a time is annoying. I'd rather be able to go forward and back | Quote Rating: None | Quote Desc: None\n",
      "| Time: 280.6 | Acc: 90 | Max Freq: 50.0 | Rate: 4 | Feedback: The pay. This comes out to very little as far as wages. | Quote Rating: None | Quote Desc: None\n",
      "| Time: 433.4 | Acc: 85 | Max Freq: 40.0 | Rate: 5 | Feedback: Giving each question an individual screen so you don't scroll | Quote Rating: None | Quote Desc: None\n",
      "| Time: 476.7 | Acc: 100 | Max Freq: 60.0 | Rate: 10 | Feedback: It's fine as it is. | Quote Rating: None | Quote Desc: None\n",
      "| Time: 302.4 | Acc: 90 | Max Freq: 45.0 | Rate: 9 | Feedback: no changes | Quote Rating: None | Quote Desc: None\n",
      "| Time: 297.7 | Acc: 90 | Max Freq: 55.0 | Rate: 8 | Feedback: I'd prefer if you didn't ask me to have to put yes to continue.  | Quote Rating: None | Quote Desc: None\n",
      "| Time: 656.8 | Acc: 100 | Max Freq: 60.0 | Rate: 8 | Feedback: the time estimate said 11 minutes but it took more like 20 minutes, it might be better to let people know that it'll take this long so that they don't rush the task. | Quote Rating: None | Quote Desc: None\n",
      "| Time: 578.9 | Acc: 90 | Max Freq: 50.0 | Rate: 10 | Feedback: nothing, it was good and worth the time hopefully the 90% percent still gets me the bonus, thanks.  | Quote Rating: None | Quote Desc: None\n",
      "| Time: 736.8 | Acc: 95 | Max Freq: 35.0 | Rate: 8 | Feedback: I don't think it needs improvement. | Quote Rating: None | Quote Desc: None\n",
      "| Time: 674.5 | Acc: 95 | Max Freq: 40.0 | Rate: 5 | Feedback: The question about Mary's Grandfather had flaws. You asked what he did, then provided an answer which stated how he 'was thought to be done' - this indicates that this action was done by others and not him and therefore is an incorrect answer. Answer A for that question was at least depicting actions that could be taken by Mary's Grandfather.  | Quote Rating: None | Quote Desc: None\n",
      "| Time: 296.4 | Acc: 95 | Max Freq: 40.0 | Rate: 7 | Feedback: Nothing that I can think of | Quote Rating: None | Quote Desc: None\n",
      "| Time: 632.0 | Acc: 100 | Max Freq: 40.0 | Rate: 10 | Feedback: The task is pretty cut and dry. Very descriptive with the questions and complex enough to make you think! | Quote Rating: None | Quote Desc: None\n",
      "| Time: 444.7 | Acc: 90 | Max Freq: 40.0 | Rate: 10 | Feedback: Some shorter dialogue | Quote Rating: None | Quote Desc: None\n",
      "| Time: 244.2 | Acc: 90 | Max Freq: 60.0 | Rate: 8 | Feedback: add a progress bar | Quote Rating: None | Quote Desc: None\n",
      "| Time: 579.3 | Acc: 90 | Max Freq: 35.0 | Rate: 10 | Feedback: shorter questions? the last question actually had a typo , it says \"beating\" , i think it mean't to say boating. | Quote Rating: None | Quote Desc: None\n",
      "REJECTED: 0\n",
      "INCOMPLETE: 0\n",
      "VALID: 33\n",
      "Median Question Duration: 19.7545\n",
      "Mean Question Duration: 21.739498106060605\n",
      "Min/Median/Mean/Max Worker Duration: 4.07 / 7.94 / 9.16 / 32.84\n",
      "Min/Median/Mean/Max Good Worker Durations: 6.15 / 9.56 / 9.53 / 12.63\n",
      "Median Worker Accuracy: 0.95\n",
      "Median Max Response Freq: 0.45\n",
      "Quote Rating: | Mean: nan | Median: nan | Std: nan\n",
      "debate_mode_counts: {'Ⅰ': 0, 'Ⅱ': 0, 'Ⅲ': 0, 'Ⅳ': 0, 'ⅰ': 0, 'ⅱ': 0, 'ⅲ': 0, 'ⅳ': 0, None: 660}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:150: RuntimeWarning: Mean of empty slice.\n"
     ]
    }
   ],
   "source": [
    "num_valid_hits, num_rejected_hits, num_incomplete_hits = 0, 0, 0\n",
    "metrics = {}\n",
    "task_ratings = {i: 0 for i in range(11)}\n",
    "quote_ratings = []\n",
    "durations = []\n",
    "worker_durations = []\n",
    "accuracy_by_worker = {}\n",
    "max_response_freqs = []\n",
    "worker_ids = []\n",
    "hits_by_qid = {}\n",
    "debate_mode_counts = {debate_mode: 0 for debate_mode in debate_mode_to_option.keys()}\n",
    "\n",
    "for hit_result in hit_results:\n",
    "    if ((len(hit_result['reject_reasons']) > 0) or\n",
    "        (len(hit_result['block_reasons']) > 0)):\n",
    "        num_rejected_hits += 1\n",
    "        print(hit_result['worker_id'], hit_result['assignment_id'],\n",
    "              '| reject_reasons:', hit_result['reject_reasons'],\n",
    "              '| block_reasons:', hit_result['block_reasons'],\n",
    "              '| bonus_reasons: ' + str(hit_result['bonus_reasons']) if 'bonus_reasons' in hit_result else '')\n",
    "    elif hit_result['feedback'] is None:\n",
    "        num_incomplete_hits += 1\n",
    "        continue\n",
    "    \n",
    "    worker_ids.append(hit_result['worker_id'])\n",
    "    num_valid_hits += 1\n",
    "    if (hit_result['task_rating'] is not None) and (hit_result['task_rating'].isdigit()):\n",
    "        task_ratings[int(hit_result['task_rating'])] += 1\n",
    "    if (hit_result.get('quote_rating') is not None) and (hit_result['quote_rating'].isdigit()):\n",
    "        quote_ratings.append(int(hit_result['quote_rating']))\n",
    "    for qtype, qtype_accuracy in hit_result['accuracy'].items():\n",
    "        accuracy_by_worker[qtype] = accuracy_by_worker.get(qtype, []) + [qtype_accuracy]\n",
    "    \n",
    "    hit_durations = []\n",
    "    response_option_counts = {option: 0 for option in options}\n",
    "    responses = []\n",
    "    for prompt in hit_result['data']:\n",
    "        qid = prompt['sample']['qid']\n",
    "        if (split is not None) and (split not in qid):\n",
    "            continue\n",
    "        hits_by_qid[qid] = hits_by_qid.get(qid, [])\n",
    "        hits_by_qid[qid].append(prompt)\n",
    "        debate_mode_counts[prompt['sample']['debate_mode']] += 1\n",
    "        stance = debate_mode_to_option[prompt['sample']['debate_mode']]\n",
    "        answer = prompt['sample']['eval_labels'][0]\n",
    "        human_correct = (prompt['response'] == answer)\n",
    "        assert answer in options, 'Answer must be in options.'\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if qid not in metrics:\n",
    "            metrics[qid] = {\n",
    "                option: {\n",
    "                    'num': 0,\n",
    "                    'num_correct': 0,\n",
    "                    'num_correct_debate_mode': 0,\n",
    "                    'num_incorrect_debate_mode': 0,\n",
    "                    'num_correct_with_correct_debate_mode': 0,\n",
    "                    'num_correct_with_incorrect_debate_mode': 0,\n",
    "                    'num_debate_mode_responses': 0,\n",
    "                    'is_debate_mode_response': []\n",
    "                }\n",
    "                for option in ([None] if stance is None else options)\n",
    "            }\n",
    "#             for qtype in question_type_labels:\n",
    "#                 metrics[qid][qtype] = {\n",
    "#                     'num': 0,\n",
    "#                     'num_correct': 0,\n",
    "#                 }\n",
    "            metrics[qid]['answer'] = answer\n",
    "        metrics[qid]['qtype'] = metrics[qid].get('qtype', set([]))\n",
    "        for qtype in set(''.join(prompt['sample'].get('question_type_labels', []))):\n",
    "            qtype = qtype.lower()\n",
    "            metrics[qid]['qtype'].add(qtype)\n",
    "#             if qtype not in metrics[qid]:\n",
    "#                 print('Did you set `dataset` appropriately?')\n",
    "#             metrics[qid][qtype]['num'] += 1\n",
    "#             metrics[qid][qtype]['num_correct'] += human_correct\n",
    "        prompt_metrics = metrics[qid][stance]\n",
    "        prompt_metrics['num'] += 1\n",
    "        prompt_metrics['num_correct'] += human_correct\n",
    "        if stance == answer:\n",
    "            prompt_metrics['num_correct_with_correct_debate_mode'] += human_correct\n",
    "            prompt_metrics['num_correct_debate_mode'] += 1\n",
    "        else:\n",
    "            prompt_metrics['num_correct_with_incorrect_debate_mode'] += human_correct\n",
    "            prompt_metrics['num_incorrect_debate_mode'] += 1\n",
    "        prompt_metrics['num_debate_mode_responses'] += (prompt['response'] == stance)\n",
    "        prompt_metrics['is_debate_mode_response'].append(prompt['response'] == stance)\n",
    "        \n",
    "        hit_durations.append(prompt['duration'] / 1000.)\n",
    "        response_option_counts[prompt['response']] += 1\n",
    "        responses.append(prompt['response'])\n",
    "    duration = np.sum(np.array(hit_durations))\n",
    "    worker_durations.append(duration)\n",
    "    durations += hit_durations\n",
    "    response_options_array = np.array(list(response_option_counts.values()))\n",
    "    response_options_array = response_options_array / response_options_array.sum()\n",
    "    max_response_freq = response_options_array.max()\n",
    "    max_response_freqs.append(max_response_freq)\n",
    "    acc = round(100 * hit_result['accuracy'][prompt_type])\n",
    "    print('| Time:', round(duration, 1),\n",
    "          '| Acc:', acc,\n",
    "          '| Max Freq:', round(100 * max_response_freq, 1),\n",
    "          '| Rate:', hit_result['task_rating'],\n",
    "          '| Feedback:', hit_result['feedback'],\n",
    "          '| Quote Rating:', None if 'quote_rating' not in hit_result else hit_result['quote_rating'], \n",
    "          '| Quote Desc:', None if 'quote_description' not in hit_result else hit_result['quote_description'])\n",
    "\n",
    "debate_modes_used = list(filter(lambda x: debate_mode_counts[x] > 0, debate_mode_counts.keys()))\n",
    "\n",
    "good_worker_durations = []\n",
    "assert len(worker_durations) == len(accuracy_by_worker[prompt_type])\n",
    "for worker_duration, worker_accuracy in zip(worker_durations, accuracy_by_worker[prompt_type]):\n",
    "    if worker_accuracy > np.median(np.array(accuracy_by_worker[prompt_type])):\n",
    "        good_worker_durations.append(worker_duration)\n",
    "\n",
    "quote_ratings = np.array(quote_ratings)\n",
    "durations = np.array(durations)\n",
    "worker_durations = np.array(worker_durations)\n",
    "good_worker_durations = np.array(good_worker_durations)\n",
    "max_response_freqs = np.array(max_response_freqs)\n",
    "\n",
    "quote_ratings.sort()\n",
    "durations.sort()\n",
    "worker_durations.sort()\n",
    "good_worker_durations.sort()\n",
    "max_response_freqs.sort()\n",
    "\n",
    "for qtype in accuracy_by_worker:\n",
    "    accuracy_by_worker[qtype] = np.array(accuracy_by_worker[qtype])\n",
    "    accuracy_by_worker[qtype].sort()\n",
    "print('REJECTED:', num_rejected_hits)\n",
    "print('INCOMPLETE:', num_incomplete_hits)\n",
    "print('VALID:', num_valid_hits)\n",
    "print('Median Question Duration:', np.median(durations))\n",
    "print('Mean Question Duration:', np.mean(durations[int(durations.shape[0] / 10.):int(9. * durations.shape[0] / 10.)]))\n",
    "print('Min/Median/Mean/Max Worker Duration:',\n",
    "      round(np.min(worker_durations / 60.), 2), '/',\n",
    "      round(np.median(worker_durations / 60.), 2), '/',\n",
    "      round(np.mean(worker_durations / 60.), 2), '/',\n",
    "      round(np.max(worker_durations / 60.), 2))\n",
    "print('Min/Median/Mean/Max Good Worker Durations:',\n",
    "      round(np.min(good_worker_durations / 60.), 2), '/',\n",
    "      round(np.median(good_worker_durations / 60.), 2),'/',\n",
    "      round(np.mean(good_worker_durations / 60.), 2), '/',\n",
    "      round(np.max(good_worker_durations / 60.), 2))\n",
    "print('Median Worker Accuracy:', np.median(accuracy_by_worker[prompt_type]))\n",
    "print('Median Max Response Freq:', np.median(max_response_freqs))\n",
    "print('Quote Rating:',\n",
    "      '| Mean:', round(quote_ratings.mean(), 2),\n",
    "      '| Median:', round(np.median(quote_ratings), 2),\n",
    "      '| Std:', round(np.std(quote_ratings), 2))\n",
    "# pprint(hit_results[0]['data'][0])\n",
    "# pprint(hit_results[0])\n",
    "print('debate_mode_counts:', debate_mode_counts)\n",
    "\n",
    "qids = list(metrics.keys())\n",
    "qids.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPS, Mean: (39.39, 8.27)\n"
     ]
    }
   ],
   "source": [
    "def nps(task_ratings):\n",
    "    num_ratings = sum(list(task_ratings.values()))\n",
    "    if num_ratings == 0:\n",
    "        return None\n",
    "\n",
    "    nps_sum_ratings = 0\n",
    "    sum_ratings = 0\n",
    "    for score, num_raters in task_ratings.items():\n",
    "        sum_ratings += num_raters * score\n",
    "        if score >= 9:\n",
    "            nps_sum_ratings += num_raters\n",
    "        elif score <= 6:\n",
    "            nps_sum_ratings -= num_raters\n",
    "    return round(100 * (nps_sum_ratings / float(num_ratings)), 2), round((sum_ratings / float(num_ratings)), 2)\n",
    "\n",
    "print('NPS, Mean:', nps(task_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': (0.5375, 2),\n",
      " 'c': (0.8942857142857144, 35),\n",
      " 'l': (0.9178571428571428, 70),\n",
      " 'm': (0.9500000000000002, 12),\n",
      " 's': (0.9654017857142858, 16)}\n",
      "Evals per sample: 6.6\n",
      "Fraction insuffient evals: 0.0\n",
      "Convinced: nan %\n",
      "Convinced: nan %\n",
      "- Agent is right: nan %\n",
      "- Agent is wrong: nan %\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "Accuracy: 92.97 %\n",
      "Accuracy: 92.97142857142858 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:80: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:81: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:83: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:85: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:89: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:91: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:93: RuntimeWarning: Mean of empty slice.\n"
     ]
    }
   ],
   "source": [
    "accuracy_by_qtype = {qtype: [] for qtype in question_type_labels}\n",
    "accuracy_by_sample = []\n",
    "accuracy_by_sample_correct_debate_mode = []\n",
    "accuracy_by_sample_incorrect_debate_mode = []\n",
    "convinced_by_qtype = {qtype: [] for qtype in question_type_labels}\n",
    "convinced_freqs = []\n",
    "convinced_freqs_with_correct_debate_mode = []\n",
    "convinced_freqs_with_incorrect_debate_mode = []\n",
    "convinced_freqs_by_qtype = {qtype: [] for qtype in question_type_labels}\n",
    "convinced_freqs_with_correct_debate_mode_by_qtype = {qtype: [] for qtype in question_type_labels}\n",
    "convinced_freqs_with_incorrect_debate_mode_by_qtype = {qtype: [] for qtype in question_type_labels}\n",
    "num_evals_by_sample = []\n",
    "convinced_by_sample = []\n",
    "# qtypes = []\n",
    "# qtypes_with_correct_debate_mode = []\n",
    "# qtypes_with_incorrect_debate_mode = []\n",
    "\n",
    "for qid in qids:\n",
    "    qid_metrics = metrics[qid]\n",
    "    answer = qid_metrics['answer']\n",
    "    for qid_metric_key, prompt in qid_metrics.items():\n",
    "        if qid_metric_key in question_type_labels:\n",
    "#             qtype = qid_metric_key\n",
    "#             if qid_metrics[qtype]['num'] > 0:\n",
    "#                 accuracy_by_qtype[qtype].append(qid_metrics[qtype]['num_correct'] / qid_metrics[qtype]['num'])\n",
    "#                 convinced_by_qtype[qtype].append(qid_metrics[qtype][])\n",
    "            continue\n",
    "        if not (qid_metric_key in [None] + options):\n",
    "            continue\n",
    "        stance = qid_metric_key\n",
    "\n",
    "        # Q-only stats\n",
    "        prompt_metrics = qid_metrics[stance]\n",
    "        num_evals_by_sample.append(prompt_metrics['num'])\n",
    "        convinced_by_sample.append(prompt_metrics['is_debate_mode_response'])\n",
    "        accuracy_by_sample.append(prompt_metrics['num_correct'] / prompt_metrics['num'])\n",
    "        for qtype in qid_metrics['qtype']:\n",
    "            accuracy_by_qtype[qtype].append(prompt_metrics['num_correct'] / prompt_metrics['num'])\n",
    "        if stance is None:\n",
    "            continue\n",
    "        \n",
    "        # Debater stats\n",
    "#         if 'num_debate_mode_responses' not in prompt_metrics:\n",
    "#             print(qid_metric_key, prompt_metrics)\n",
    "        convinced_freq = prompt_metrics['num_debate_mode_responses'] / prompt_metrics['num']\n",
    "        if stance == answer:\n",
    "            convinced_freqs_with_correct_debate_mode.append(convinced_freq)\n",
    "            for qtype in qid_metrics['qtype']:\n",
    "                convinced_freqs_with_correct_debate_mode_by_qtype[qtype].append(convinced_freq)\n",
    "#             qtypes_with_correct_debate_mode.append(qid_metrics['qtype'])\n",
    "            accuracy_by_sample_correct_debate_mode.append(\n",
    "                prompt_metrics['num_correct_with_correct_debate_mode'] /\n",
    "                prompt_metrics['num_correct_debate_mode'])\n",
    "        else:\n",
    "            convinced_freqs_with_incorrect_debate_mode.append(convinced_freq)\n",
    "            for qtype in qid_metrics['qtype']:\n",
    "                convinced_freqs_with_incorrect_debate_mode_by_qtype[qtype].append(convinced_freq)\n",
    "#             qtypes_with_incorrect_debate_mode.append(qid_metrics['qtype'])\n",
    "            accuracy_by_sample_incorrect_debate_mode.append(\n",
    "                prompt_metrics['num_correct_with_incorrect_debate_mode'] /\n",
    "                prompt_metrics['num_incorrect_debate_mode'])\n",
    "        convinced_freqs.append(convinced_freq)\n",
    "        for qtype in qid_metrics['qtype']:\n",
    "            convinced_freqs_by_qtype[qtype].append(convinced_freq)\n",
    "#         qtypes.append(qid_metrics['qtype'])\n",
    "\n",
    "# qtypes = np.array(qtypes)\n",
    "# qtypes_with_correct_debate_mode = np.array(qtypes_with_correct_debate_mode)\n",
    "# qtypes_with_incorrect_debate_mode = np.array(qtypes_with_incorrect_debate_mode)\n",
    "\n",
    "# print(accuracy_by_qtype)\n",
    "accuracy_by_qtype = {qtype: (np.array(accuracy_by_qtype[qtype]).mean(), len(accuracy_by_qtype[qtype])) for qtype in question_type_labels}\n",
    "pprint(accuracy_by_qtype)\n",
    "\n",
    "num_evals_by_sample = np.array(num_evals_by_sample)\n",
    "print('Evals per sample:', num_evals_by_sample.mean())\n",
    "print('Fraction insuffient evals:', (num_evals_by_sample < 5).mean())\n",
    "\n",
    "convinced_freqs = np.array(convinced_freqs)\n",
    "print('Convinced:', round(100 * convinced_freqs.mean(), 2), '%')\n",
    "print('Convinced:', 100 * convinced_freqs.mean(), '%')\n",
    "convinced_freqs_with_correct_debate_mode = np.array(convinced_freqs_with_correct_debate_mode)\n",
    "print('- Agent is right:', round(100 * convinced_freqs_with_correct_debate_mode.mean(), 2), '%')\n",
    "convinced_freqs_with_incorrect_debate_mode = np.array(convinced_freqs_with_incorrect_debate_mode)\n",
    "print('- Agent is wrong:', round(100 * convinced_freqs_with_incorrect_debate_mode.mean(), 2), '%')\n",
    "\n",
    "for qtype in question_type_labels:\n",
    "#     print(len(convinced_freqs_by_qtype[qtype]))\n",
    "    convinced_freqs_by_qtype[qtype] = np.array(convinced_freqs_by_qtype[qtype]).mean()\n",
    "    print(len(convinced_freqs_with_correct_debate_mode_by_qtype[qtype]))\n",
    "    convinced_freqs_with_correct_debate_mode_by_qtype[qtype] = np.array(convinced_freqs_with_correct_debate_mode_by_qtype[qtype]).mean()\n",
    "#     print(len(convinced_freqs_with_incorrect_debate_mode_by_qtype[qtype]))\n",
    "    convinced_freqs_with_incorrect_debate_mode_by_qtype[qtype] = np.array(convinced_freqs_with_incorrect_debate_mode_by_qtype[qtype]).mean()\n",
    "\n",
    "accuracy_by_sample = np.array(accuracy_by_sample)\n",
    "print('Accuracy:', round(100 * accuracy_by_sample.mean(), 2), '%')\n",
    "print('Accuracy:', 100 * accuracy_by_sample.mean(), '%')\n",
    "accuracy_by_sample_correct_debate_mode = np.array(accuracy_by_sample_correct_debate_mode)\n",
    "\n",
    "num_target_evals = 5\n",
    "# print('Extra Evals:', round(((100. * (num_evals_by_sample - num_target_evals).sum()) / num_evals_by_sample.sum()), 2), '%')\n",
    "# num_evals_by_sample.sort()\n",
    "# print('Evals per sample distribution:', num_evals_by_sample)\n",
    "# 1.5*3.1*60/(917.5684545454544*26/(20*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BERT Base': array([41.42857143, 37.14285714, 36.42857143, 33.57142857, 38.57142857]),\n",
      " 'FastText(S,A)': array([35.35714286, 34.64285714, 36.07142857, 34.64285714, 36.07142857]),\n",
      " 'Full Passage': array([0., 0., 0., 0., 0.]),\n",
      " 'Human': array([34.64285714, 35.        , 36.07142857, 36.07142857, 41.07142857]),\n",
      " 'No Passage': array([0., 0., 0., 0., 0.]),\n",
      " 'Pred p(A)': array([41.42857143, 41.78571429, 41.78571429, 42.5       , 37.14285714]),\n",
      " 'Pred search': array([38.21428571, 40.35714286, 36.78571429, 37.85714286, 41.07142857]),\n",
      " 'Pred Δp(A)': array([36.07142857, 40.        , 40.35714286, 41.78571429, 39.64285714]),\n",
      " 'TFIDF(S,A)': array([36.42857143, 38.57142857, 34.64285714, 37.85714286, 34.28571429]),\n",
      " 'TFIDF(S,[Q;A])': array([32.14285714, 30.71428571, 30.35714286, 33.57142857, 33.92857143])}\n",
      "***** TFIDF(S,[Q;A]) *****\n",
      "MEAN: 32.1\n",
      "STD: 1.45\n",
      "STDERR: 0.65\n",
      "\n",
      "***** TFIDF(S,A) *****\n",
      "MEAN: 36.4\n",
      "STD: 1.7\n",
      "STDERR: 0.76\n",
      "\n",
      "***** FastText(S,A) *****\n",
      "MEAN: 35.4\n",
      "STD: 0.64\n",
      "STDERR: 0.29\n",
      "\n",
      "***** BERT Base *****\n",
      "MEAN: 37.4\n",
      "STD: 2.58\n",
      "STDERR: 1.15\n",
      "\n",
      "***** Pred search *****\n",
      "MEAN: 38.9\n",
      "STD: 1.6\n",
      "STDERR: 0.72\n",
      "\n",
      "***** Pred p(A) *****\n",
      "MEAN: 40.9\n",
      "STD: 1.92\n",
      "STDERR: 0.86\n",
      "\n",
      "***** Pred Δp(A) *****\n",
      "MEAN: 39.6\n",
      "STD: 1.9\n",
      "STDERR: 0.85\n",
      "\n",
      "***** Human *****\n",
      "MEAN: 36.6\n",
      "STD: 2.32\n",
      "STDERR: 1.04\n",
      "\n",
      "***** No Passage *****\n",
      "MEAN: 0.0\n",
      "STD: 0.0\n",
      "STDERR: 0.0\n",
      "\n",
      "***** Full Passage *****\n",
      "MEAN: 0.0\n",
      "STD: 0.0\n",
      "STDERR: 0.0\n",
      "\n",
      "TFIDF(S,[Q;A]) / TFIDF(S,[Q;A]) : 1.0\n",
      "TFIDF(S,[Q;A]) / TFIDF(S,A) : 0.0056\n",
      "TFIDF(S,[Q;A]) / FastText(S,A) : 0.0079\n",
      "TFIDF(S,[Q;A]) / BERT Base : 0.0108\n",
      "TFIDF(S,[Q;A]) / Pred search : 0.0003\n",
      "TFIDF(S,[Q;A]) / Pred p(A) : 0.0001\n",
      "TFIDF(S,[Q;A]) / Pred Δp(A) : 0.0003\n",
      "TFIDF(S,[Q;A]) / Human : 0.0152\n",
      "TFIDF(S,[Q;A]) / No Passage : 0.0\n",
      "TFIDF(S,[Q;A]) / Full Passage : 0.0\n",
      "TFIDF(S,A) / TFIDF(S,[Q;A]) : 0.0056\n",
      "TFIDF(S,A) / TFIDF(S,A) : 1.0\n",
      "TFIDF(S,A) / FastText(S,A) : 0.3191\n",
      "TFIDF(S,A) / BERT Base : 0.5102\n",
      "TFIDF(S,A) / Pred search : 0.0647\n",
      "TFIDF(S,A) / Pred p(A) : 0.0076\n",
      "TFIDF(S,A) / Pred Δp(A) : 0.0357\n",
      "TFIDF(S,A) / Human : 0.8855\n",
      "TFIDF(S,A) / No Passage : 0.0\n",
      "TFIDF(S,A) / Full Passage : 0.0\n",
      "FastText(S,A) / TFIDF(S,[Q;A]) : 0.0079\n",
      "FastText(S,A) / TFIDF(S,A) : 0.3191\n",
      "FastText(S,A) / FastText(S,A) : 1.0\n",
      "FastText(S,A) / BERT Base : 0.1863\n",
      "FastText(S,A) / Pred search : 0.0089\n",
      "FastText(S,A) / Pred p(A) : 0.003\n",
      "FastText(S,A) / Pred Δp(A) : 0.0088\n",
      "FastText(S,A) / Human : 0.3631\n",
      "FastText(S,A) / No Passage : 0.0\n",
      "FastText(S,A) / Full Passage : 0.0\n",
      "BERT Base / TFIDF(S,[Q;A]) : 0.0108\n",
      "BERT Base / TFIDF(S,A) : 0.5102\n",
      "BERT Base / FastText(S,A) : 0.1863\n",
      "BERT Base / BERT Base : 1.0\n",
      "BERT Base / Pred search : 0.3796\n",
      "BERT Base / Pred p(A) : 0.064\n",
      "BERT Base / Pred Δp(A) : 0.2205\n",
      "BERT Base / Human : 0.6347\n",
      "BERT Base / No Passage : 0.0\n",
      "BERT Base / Full Passage : 0.0\n",
      "Pred search / TFIDF(S,[Q;A]) : 0.0003\n",
      "Pred search / TFIDF(S,A) : 0.0647\n",
      "Pred search / FastText(S,A) : 0.0089\n",
      "Pred search / BERT Base : 0.3796\n",
      "Pred search / Pred search : 1.0\n",
      "Pred search / Pred p(A) : 0.138\n",
      "Pred search / Pred Δp(A) : 0.5812\n",
      "Pred search / Human : 0.1485\n",
      "Pred search / No Passage : 0.0\n",
      "Pred search / Full Passage : 0.0\n",
      "Pred p(A) / TFIDF(S,[Q;A]) : 0.0001\n",
      "Pred p(A) / TFIDF(S,A) : 0.0076\n",
      "Pred p(A) / FastText(S,A) : 0.003\n",
      "Pred p(A) / BERT Base : 0.064\n",
      "Pred p(A) / Pred search : 0.138\n",
      "Pred p(A) / Pred p(A) : 1.0\n",
      "Pred p(A) / Pred Δp(A) : 0.3444\n",
      "Pred p(A) / Human : 0.0209\n",
      "Pred p(A) / No Passage : 0.0\n",
      "Pred p(A) / Full Passage : 0.0\n",
      "Pred Δp(A) / TFIDF(S,[Q;A]) : 0.0003\n",
      "Pred Δp(A) / TFIDF(S,A) : 0.0357\n",
      "Pred Δp(A) / FastText(S,A) : 0.0088\n",
      "Pred Δp(A) / BERT Base : 0.2205\n",
      "Pred Δp(A) / Pred search : 0.5812\n",
      "Pred Δp(A) / Pred p(A) : 0.3444\n",
      "Pred Δp(A) / Pred Δp(A) : 1.0\n",
      "Pred Δp(A) / Human : 0.0817\n",
      "Pred Δp(A) / No Passage : 0.0\n",
      "Pred Δp(A) / Full Passage : 0.0\n",
      "Human / TFIDF(S,[Q;A]) : 0.0152\n",
      "Human / TFIDF(S,A) : 0.8855\n",
      "Human / FastText(S,A) : 0.3631\n",
      "Human / BERT Base : 0.6347\n",
      "Human / Pred search : 0.1485\n",
      "Human / Pred p(A) : 0.0209\n",
      "Human / Pred Δp(A) : 0.0817\n",
      "Human / Human : 1.0\n",
      "Human / No Passage : 0.0\n",
      "Human / Full Passage : 0.0\n",
      "No Passage / TFIDF(S,[Q;A]) : 0.0\n",
      "No Passage / TFIDF(S,A) : 0.0\n",
      "No Passage / FastText(S,A) : 0.0\n",
      "No Passage / BERT Base : 0.0\n",
      "No Passage / Pred search : 0.0\n",
      "No Passage / Pred p(A) : 0.0\n",
      "No Passage / Pred Δp(A) : 0.0\n",
      "No Passage / Human : 0.0\n",
      "No Passage / No Passage : nan\n",
      "No Passage / Full Passage : nan\n",
      "Full Passage / TFIDF(S,[Q;A]) : 0.0\n",
      "Full Passage / TFIDF(S,A) : 0.0\n",
      "Full Passage / FastText(S,A) : 0.0\n",
      "Full Passage / BERT Base : 0.0\n",
      "Full Passage / Pred search : 0.0\n",
      "Full Passage / Pred p(A) : 0.0\n",
      "Full Passage / Pred Δp(A) : 0.0\n",
      "Full Passage / Human : 0.0\n",
      "Full Passage / No Passage : nan\n",
      "Full Passage / Full Passage : nan\n"
     ]
    }
   ],
   "source": [
    "evals[name] = []\n",
    "for eval_no in range(num_evals):\n",
    "    evals[name].append([convinced_array[eval_no] for convinced_array in convinced_by_sample])\n",
    "\n",
    "evals[name] = 100. * np.array(evals[name]).mean(axis=1)\n",
    "pprint(evals)\n",
    "\n",
    "for n, eval_values in evals.items():\n",
    "    print('*****', n, '*****')\n",
    "    print('MEAN:', round(eval_values.mean(), 1))\n",
    "    print('STD:', round(eval_values.std(), 2))\n",
    "    print('STDERR:', round(eval_values.std() / np.sqrt(eval_values.shape[0]), 2))\n",
    "    print()\n",
    "\n",
    "for n1, eval_values1 in evals.items():\n",
    "    for n2, eval_values2 in evals.items():\n",
    "        print(n1, '/', n2, ':', round(ttest_ind(eval_values1, eval_values2, equal_var=False)[1], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.75\n",
      "89.43\n",
      "91.79\n",
      "95.0\n",
      "96.54\n"
     ]
    }
   ],
   "source": [
    "# print('Accuracy/Num-Samples by Q Type:')\n",
    "if len(accuracy_by_qtype) > 0:\n",
    "    for qtype in question_type_labels:\n",
    "        print(round(100. * accuracy_by_qtype[qtype][0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(100. * convinced_freqs.mean())\n",
    "# print(100. * convinced_freqs_with_correct_debate_mode.mean())\n",
    "# print(100. * convinced_freqs_with_incorrect_debate_mode.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for qtype in question_type_labels:\n",
    "#     print(100. * convinced_freqs[qtypes == qtype].mean(), convinced_freqs[qtypes == qtype].sum())\n",
    "    \n",
    "# for qtype in question_type_labels:\n",
    "#     print(100. * convinced_freqs_with_correct_debate_mode[qtypes_with_correct_debate_mode == qtype].mean(), convinced_freqs_with_correct_debate_mode[qtypes_with_correct_debate_mode == qtype].sum())\n",
    "    \n",
    "# for qtype in question_type_labels:\n",
    "#     print(100. * convinced_freqs_with_incorrect_debate_mode[qtypes_with_incorrect_debate_mode == qtype].mean(), convinced_freqs_with_incorrect_debate_mode[qtypes_with_incorrect_debate_mode == qtype].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.179421768707485\n",
      "35.60817805383022\n",
      "38.56632653061225\n",
      "38.315781440781436\n",
      "33.431919642857146\n",
      "80.64625850340137\n",
      "65.2536231884058\n",
      "78.39455782312926\n",
      "69.21855921855924\n",
      "56.607142857142854\n",
      "25.357142857142854\n",
      "25.726363008971703\n",
      "25.290249433106577\n",
      "28.014855514855512\n",
      "25.706845238095237\n"
     ]
    }
   ],
   "source": [
    "for qtype in question_type_labels:\n",
    "    print(100. * convinced_freqs_by_qtype[qtype])\n",
    "for qtype in question_type_labels:\n",
    "    print(100. * convinced_freqs_with_correct_debate_mode_by_qtype[qtype])\n",
    "for qtype in question_type_labels:\n",
    "    print(100. * convinced_freqs_with_incorrect_debate_mode_by_qtype[qtype])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.43154761904762\n",
      "54.058248299319736\n",
      "53.369565217391305\n",
      "52.782312925170075\n",
      "51.573565323565326\n",
      "43.80952380952381\n"
     ]
    }
   ],
   "source": [
    "print(100. * accuracy_by_sample.mean())\n",
    "for qtype in question_type_labels:\n",
    "    print(100. * accuracy_by_qtype[qtype][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37.12414965986394\n"
     ]
    }
   ],
   "source": [
    "print(100 * convinced_freqs.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# HIT Files: 127\n",
      "# Passed Test: 35 / 92 = 38.04 %\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "task_dir = '/Users/ethanperez/research/ParlAI/parlai/mturk/core/run_data/live/context_evaluator_'\n",
    "\n",
    "### RACE: Unfiltered Workers\n",
    "# prompt_type, task_id = 'question', '1553982706'  # Q-only\n",
    "# prompt_type, task_id = 'context_question', 1553790696  # TFIDF\n",
    "# prompt_type, task_id = 'context_question', 1553901953  # FastText\n",
    "### RACE: Filtered Workers\n",
    "# prompt_type, task_id = 'question', '1554052233'  # Q-only\n",
    "# prompt_type, task_id = 'quote and question', 1554006689  # TFIDF-QA\n",
    "# prompt_type, task_id = 'quote and question', 1554130485  # TFIDF-A\n",
    "# prompt_type, task_id = 'quote and question', 1554069931  # Cross-Ranker\n",
    "# prompt_type, task_id = 'quote and question', 1554072277  # Predicting Search\n",
    "# prompt_type, task_id = 'quote and question', 1554132868  # Predicting ΔP(A)\n",
    "\n",
    "### RACE Test\n",
    "## Convinced\n",
    "# prompt_type, task_id = 'quote and question', 1556671432  # TFIDF-QA\n",
    "# prompt_type, task_id = 'quote and question', 1556725767  # TFIDF-A\n",
    "# prompt_type, task_id = 'quote and question', 1556739336  # BoW-A\n",
    "# prompt_type, task_id = 'quote and question', 1556756789  # ΔP(A) (race.m=sl-sents.i.best.e)\n",
    "# prompt_type, task_id = 'quote and question', 1556809031  # P(A)\n",
    "# prompt_type, task_id = 'quote and question', 1556832343  # Cross-Ranker (Almost complete)\n",
    "# prompt_type, task_id = 'quote and question', 1556892630  # Cross-Ranker\n",
    "## Acc on Summary\n",
    "# prompt_type, task_id = 'passage and question', 1555823963  # Full Passage\n",
    "# prompt_type, task_id = 'quotes and question', 1555946909  # FastText\n",
    "# prompt_type, task_id = 'quotes and question', 1555952058  # Cross-Ranker (Best Epoch. 6-10 sentence incorrectly placed at end)\n",
    "# prompt_type, task_id = 'quotes and question', 1556939750  # Predicting ΔP(A) (lower pay)\n",
    "# prompt_type, task_id = 'quotes and question', 1556977072  # Predicting P(A)\n",
    "# prompt_type, task_id = 'quotes and question', 1556987177  # Predicting Search\n",
    "prompt_type, task_id = 'quotes and question', 1556999857  # Predicting ΔP(A)\n",
    "dataset = 'race'\n",
    "\n",
    "### DREAM\n",
    "# prompt_type, task_id = 'question', 1554582693  # Q-only\n",
    "# prompt_type, task_id = 'quote and question', 1554596686  # TFIDF-QA\n",
    "# prompt_type, task_id = 'quote and question', 1554587404  # TFIDF-A\n",
    "# prompt_type, task_id = 'quote and question', 1554662280  # BoW-A\n",
    "# prompt_type, task_id = 'quote and question', 1556670413  # Bi-Ranker\n",
    "# prompt_type, task_id = 'quote and question', 1554675304  # Cross-Ranker\n",
    "# prompt_type, task_id = 'quote and question', 1554685131  # SL\n",
    "# prompt_type, task_id = 'quote and question', 1554692472  # SL-Sents\n",
    "# prompt_type, task_id = 'quote and question', 1554729998  # SL-Sents-Influence\n",
    "# prompt_type, task_id = 'quote and question', 1555333992  # SL-Theory-of-Mind\n",
    "## All answers at once\n",
    "# prompt_type, task_id = 'question, answers, and quotes', 1555707929  # TFIDF(O): 64.21%: (Less filter / no feedback)\n",
    "# prompt_type, task_id = 'question, answers, and quotes', 1555722489  # Search: 65.38%: (Less filter / no feedback)\n",
    "# prompt_type, task_id = 'question and quotes', 1555789302  # SL: 75.17% (4/5 filter)\n",
    "# prompt_type, task_id = 'question and quotes', 1555812443  # SL: 79.32% Actually: quotes and question (4/5 filter)\n",
    "# prompt_type, task_id = 'passage and question', 1555804551  # Full Passage: 92.97%\n",
    "# prompt_type, task_id = 'quotes and question', 1555823257  # FastText (5/5 filter) (77.33%)\n",
    "# prompt_type, task_id = 'quotes and question', 1555946647  # RACE Cross-Ranker (4 sentences incorrectly placed at end) (80.84%)\n",
    "# prompt_type, task_id = 'quotes and question', 1556727396  # Cross-Ranker\n",
    "# prompt_type, task_id = 'quotes and question', 1556740293  # Bi-Ranker\n",
    "# prompt_type, task_id = 'quotes and question', 1556757043  # TFIDF-A\n",
    "# prompt_type, task_id = 'quotes and question', 1556811067  # TFIDF-QA\n",
    "# prompt_type, task_id = 'quotes and question', 1556832115  # Predicting Search\n",
    "# prompt_type, task_id = 'quotes and question', 1556892896  # Predicting P(A)\n",
    "# prompt_type, task_id = 'quotes and question', 1556938429  # Predicting ΔP(A)\n",
    "# dataset = 'dream'\n",
    "\n",
    "\n",
    "split = None  # 'middle', 'high', None\n",
    "\n",
    "\n",
    "# Set useful variables\n",
    "task_dir += str(task_id)\n",
    "if dataset != 'race':\n",
    "    split = None\n",
    "num_options = 3 if dataset == 'dream' else 4\n",
    "options = ['A', 'B', 'C', 'D'][:num_options]\n",
    "debate_mode_to_option = {'Ⅰ': 'A', 'Ⅱ': 'B', 'Ⅲ': 'C', 'Ⅳ': 'D', 'ⅰ': 'A', 'ⅱ': 'B', 'ⅲ': 'C', 'ⅳ': 'D', None: None}\n",
    "question_type_labels = ['a', 'c', 'l', 'm', 's'] if dataset == 'dream' else ['a', 'b', 'c', 'd', 'e']\n",
    "\n",
    "# Read HIT data\n",
    "print('# HIT Files:', len(os.listdir(task_dir)))\n",
    "hit_results = []\n",
    "num_passed_test = 0\n",
    "for hit_dir in os.listdir(task_dir):\n",
    "    if hit_dir.startswith('o_'):\n",
    "        continue\n",
    "    num_passed_test += 1\n",
    "    with open(os.path.join(task_dir, hit_dir, 'custom/data.json'), 'r') as file:\n",
    "        hit_results.append(json.load(file))\n",
    "        file.close()\n",
    "if len(os.listdir(task_dir)) != num_passed_test:\n",
    "    num_total_tested = len(os.listdir(task_dir)) - num_passed_test\n",
    "    print('# Passed Test:', num_passed_test, '/', num_total_tested, '=', round((100. * num_passed_test) / num_total_tested, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Time: 1352.7 | Acc: 75 | Max Freq: 50.0 | Rate: 6 | Feedback: Some questions are worded quite strangely | Quote Rating: 7 | Quote Desc: Some offered very little or confusing information that required very close reading to get the answers.\n",
      "| Time: 696.6 | Acc: 55 | Max Freq: 35.0 | Rate: 8 | Feedback: Maybe give  more information to get the answer from. | Quote Rating: 7 | Quote Desc: A lot of them I was able to understand and find the answer from however some were very difficult to find the answer to based on the information given.  Tried my best with the task though.\n",
      "| Time: 1273.5 | Acc: 80 | Max Freq: 35.0 | Rate: 10 | Feedback: The only thing I noticed is that some of the questions use incorrect names, which is where I made several of my errors. Other than that it was good - it made me think!  | Quote Rating: 7 | Quote Desc: I would describe them as being slightly cryptic. \n",
      "| Time: 850.8 | Acc: 75 | Max Freq: 30.0 | Rate: 10 | Feedback: I can't think of anything to improve. I really enjoy doing these! | Quote Rating: 8 | Quote Desc: They were parts of larger quotes\n",
      "| Time: 1133.8 | Acc: 60 | Max Freq: 35.0 | Rate: 8 | Feedback: nothing comes to mind | Quote Rating: 6 | Quote Desc: Some were pretty clear and gave enough information to select a good answer, others required some extrapolations and a bit of guessing\n",
      "| Time: 1378.6 | Acc: 75 | Max Freq: 30.0 | Rate: 9 | Feedback: I don't think there are any areas that need improvement.  | Quote Rating: 8 | Quote Desc: Some took a lot of rereading to fully comprehend the meaning, and answer. The passages were overall very useful.\n",
      "| Time: 2210.3 | Acc: 55 | Max Freq: 30.0 | Rate: 9 | Feedback: unsure | Quote Rating: 6 | Quote Desc: Many of them were pretty easy to figure out (or at least narrow down). Some were nearly impossible to figure out as the quotes didn't provide enough information.  Fun and interesting task, though\n",
      "| Time: 1751.3 | Acc: 75 | Max Freq: 30.0 | Rate: 9 | Feedback: I think it works fine. | Quote Rating: 7 | Quote Desc: I think some of them were easy to comprehend and a few were confusing me.\n",
      "| Time: 601.5 | Acc: 65 | Max Freq: 30.0 | Rate: 10 | Feedback: Honestly? Use fewer quotes in which the answer could possibly be two or three of the given choices. | Quote Rating: 8 | Quote Desc: Easy to read, but not always easy to parse for an answer.\n",
      "| Time: 1111.4 | Acc: 65 | Max Freq: 30.0 | Rate: 9 | Feedback: I didn't think the quote for problem #15 was very helpful. | Quote Rating: 7 | Quote Desc: Most of them were helpful, though the quote for #15 wasn't. Some of them were interesting.\n",
      "| Time: 1075.0 | Acc: 65 | Max Freq: 30.0 | Rate: 6 | Feedback: more details | Quote Rating: 6 | Quote Desc: minimally detailed\n",
      "| Time: 1320.2 | Acc: 60 | Max Freq: 35.0 | Rate: 9 | Feedback: provide more clarity with respect to what sort of inferences can be made to answer the questions. Maybe have a two attempt allowance for the answers | Quote Rating: 7 | Quote Desc: Some passage quotes required the reader to make more than one assumption to arrive at the answer while some others were capable of more than a single right response.\n",
      "| Time: 2193.3 | Acc: 65 | Max Freq: 30.0 | Rate: 6 | Feedback: na | Quote Rating: 5 | Quote Desc: some of them didn't make sense.\n",
      "| Time: 380.5 | Acc: 35 | Max Freq: 35.0 | Rate: 2 | Feedback: More clear | Quote Rating: 2 | Quote Desc: Extremely vague and frustrating\n",
      "| Time: 796.8 | Acc: 85 | Max Freq: 40.0 | Rate: 8 | Feedback: Not sure. | Quote Rating: 8 | Quote Desc: Most of them seemed reasonable, but a few really did not help much for answering the question.\n",
      "| Time: 1254.2 | Acc: 60 | Max Freq: 35.0 | Rate: 9 | Feedback: It's good as it is. I realize the point isn't to make it too obvious, so as it stands now it's very challenging. | Quote Rating: 5 | Quote Desc: Some made lots of sense and lead to an obvious choice while others lead to two good choices, and I seemed to pick the wrong one. It was interesting overall, but also frustrating.\n",
      "| Time: 596.1 | Acc: 60 | Max Freq: 40.0 | Rate: 5 | Feedback: Provide more information. | Quote Rating: 2 | Quote Desc: The passages were confusing and often misleading and vague.\n",
      "| Time: 652.7 | Acc: 50 | Max Freq: 35.0 | Rate: 10 | Feedback: it is fine the wa it is | Quote Rating: 6 | Quote Desc: they were oj but could be answered differerntly\n",
      "| Time: 1082.5 | Acc: 85 | Max Freq: 35.0 | Rate: 8 | Feedback: I think that providing slightly more information on some of them would be helpful, considering that the alternative is to use common sense or commonly known expert consensus. The pitfall in employing the latter is that, for example, in the last question, the answer runs counterintuitive to the consensus of some actual neurologists actually, & there is no indication within the passage that one recommendation is prioritized over another to achieve the outcome of having a good memory and learning better.  | Quote Rating: 7 | Quote Desc: Snippets from a larger body of information.\n",
      "| Time: 1099.7 | Acc: 50 | Max Freq: 45.0 | Rate: 7 | Feedback: Make the texts more relevant but streamlined where its not obvious half the time. | Quote Rating: 7 | Quote Desc: Swing wildly from either spelling out the answers or not helping at all. \n",
      "| Time: 1125.7 | Acc: 75 | Max Freq: 40.0 | Rate: 4 | Feedback: Provide more useful info | Quote Rating: 6 | Quote Desc: Some were very vague \n",
      "| Time: 1158.9 | Acc: 45 | Max Freq: 45.0 | Rate: 10 | Feedback: no need for improvement | Quote Rating: 8 | Quote Desc: challenging\n",
      "| Time: 1138.9 | Acc: 50 | Max Freq: 40.0 | Rate: 7 | Feedback: clearer answers | Quote Rating: 6 | Quote Desc: sometimes confusing\n",
      "| Time: 858.2 | Acc: 85 | Max Freq: 40.0 | Rate: 9 | Feedback: That's hard to say. Perhaps give advice that if the passage is inadequate to be sure of an answer, the reader should use common sense. | Quote Rating: 8 | Quote Desc: They appeared to be snippets taken at various points from a larger, more cohesive passage.\n",
      "| Time: 820.1 | Acc: 80 | Max Freq: 35.0 | Rate: 10 | Feedback: Perhaps explain in slightly more detail in the start that some of the quotes might not give you enough info to provide an accurate response but to still try your best.  | Quote Rating: 7 | Quote Desc: Helpful for establishing contextual clues, but hard to establish fine details. \n",
      "| Time: 572.9 | Acc: 50 | Max Freq: 30.0 | Rate: 8 | Feedback: Nothing really I enjoyed it | Quote Rating: 7 | Quote Desc: Sometimes there is not enough information to get the correct answer\n",
      "| Time: 939.6 | Acc: 60 | Max Freq: 30.0 | Rate: 10 | Feedback: na | Quote Rating: 8 | Quote Desc: They were good. But guessing the Title of the passage was difficult.\n",
      "| Time: 805.3 | Acc: 75 | Max Freq: 35.0 | Rate: 9 | Feedback: I think it worked the way it should. The only thing would be making it known how many of these hits a single person can do. Nowhere does it mention that. | Quote Rating: 8 | Quote Desc: They were generally helpful but some were too vague and the answer could have easily been one of the others, a few of them didn't have enough information or context.\n",
      "| Time: 694.7 | Acc: 70 | Max Freq: 30.0 | Rate: 9 | Feedback: Make some of the passages more clear in order to answer the questions correctly. | Quote Rating: 7 | Quote Desc: Informative\n",
      "| Time: 1227.4 | Acc: 75 | Max Freq: 40.0 | Rate: 10 | Feedback: leave it as it is as it is great just as it is and it really makes you have to think hard to get the correct answers | Quote Rating: 8 | Quote Desc: They gave some information but also didn't give enough information to completely know the correct answer every time\n",
      "| Time: 1242.4 | Acc: 65 | Max Freq: 40.0 | Rate: 10 | Feedback: The quotes selected really impact our ability to answer, making it difficult to earn a bonus reward, so really we aren't being paid double if we do well since it is set up so we can't do well. | Quote Rating: 3 | Quote Desc: It was actually impossible to answer several questions based only on the quotes provided.\n",
      "| Time: 1041.1 | Acc: 65 | Max Freq: 45.0 | Rate: 10 | Feedback: It is already good based on what you all want to accomplish with the task. | Quote Rating: 6 | Quote Desc: 5\n",
      "REJECTED: 0\n",
      "INCOMPLETE: 3\n",
      "VALID: 32\n",
      "Median Question Duration: 40.548500000000004\n",
      "Mean Question Duration: 44.55064453125\n",
      "Min/Median/Mean/Max Worker Duration: 6.34 / 18.18 / 17.94 / 36.84\n",
      "Min/Median/Mean/Max Good Worker Durations: 11.58 / 18.04 / 17.97 / 29.19\n",
      "Median Worker Accuracy: 0.65\n",
      "Median Max Response Freq: 0.35\n",
      "Quote Rating: | Mean: 6.5 | Median: 7.0 | Std: 1.6\n"
     ]
    }
   ],
   "source": [
    "num_valid_hits, num_rejected_hits, num_incomplete_hits = 0, 0, 0\n",
    "metrics = {}\n",
    "task_ratings = {i: 0 for i in range(11)}\n",
    "quote_ratings = []\n",
    "durations = []\n",
    "worker_durations = []\n",
    "accuracy_by_worker = {}\n",
    "max_response_freqs = []\n",
    "worker_ids = []\n",
    "hits_by_qid = {}\n",
    "\n",
    "for hit_result in hit_results:\n",
    "    if ((len(hit_result['reject_reasons']) > 0) or\n",
    "        (len(hit_result['block_reasons']) > 0)):\n",
    "        num_rejected_hits += 1\n",
    "        print(hit_result['worker_id'], hit_result['assignment_id'],\n",
    "              '| reject_reasons:', hit_result['reject_reasons'],\n",
    "              '| block_reasons:', hit_result['block_reasons'],\n",
    "              '| bonus_reasons: ' + str(hit_result['bonus_reasons']) if 'bonus_reasons' in hit_result else '')\n",
    "    elif hit_result['feedback'] is None:\n",
    "        num_incomplete_hits += 1\n",
    "        continue\n",
    "    \n",
    "    worker_ids.append(hit_result['worker_id'])\n",
    "    num_valid_hits += 1\n",
    "    if (hit_result['task_rating'] is not None) and (hit_result['task_rating'].isdigit()):\n",
    "        task_ratings[int(hit_result['task_rating'])] += 1\n",
    "    if (hit_result.get('quote_rating') is not None) and (hit_result['quote_rating'].isdigit()):\n",
    "        quote_ratings.append(int(hit_result['quote_rating']))\n",
    "    for qtype, qtype_accuracy in hit_result['accuracy'].items():\n",
    "        accuracy_by_worker[qtype] = accuracy_by_worker.get(qtype, []) + [qtype_accuracy]\n",
    "    \n",
    "    hit_durations = []\n",
    "    response_option_counts = {option: 0 for option in options}\n",
    "    responses = []\n",
    "    for prompt in hit_result['data']:\n",
    "        qid = prompt['sample']['qid']\n",
    "        if (split is not None) and (split not in qid):\n",
    "            continue\n",
    "        hits_by_qid[qid] = hits_by_qid.get(qid, [])\n",
    "        hits_by_qid[qid].append(prompt)\n",
    "        model_stance = debate_mode_to_option[prompt['sample']['debate_mode']]\n",
    "        answer = prompt['sample']['eval_labels'][0]\n",
    "        human_correct = (prompt['response'] == answer)\n",
    "        assert answer in options, 'Answer must be in options.'\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if qid not in metrics:\n",
    "            metrics[qid] = {\n",
    "                option: {\n",
    "                    'num': 0,\n",
    "                    'num_correct': 0,\n",
    "                    'num_correct_debate_mode': 0,\n",
    "                    'num_incorrect_debate_mode': 0,\n",
    "                    'num_correct_with_correct_debate_mode': 0,\n",
    "                    'num_correct_with_incorrect_debate_mode': 0,\n",
    "                    'num_debate_mode_responses': 0,\n",
    "                }\n",
    "                for option in ([None] if model_stance is None else options)\n",
    "            }\n",
    "            for qtype in question_type_labels:\n",
    "                metrics[qid][qtype] = {\n",
    "                    'num': 0,\n",
    "                    'num_correct': 0,\n",
    "                }\n",
    "            metrics[qid]['answer'] = answer\n",
    "        for qtype in set(''.join(prompt['sample'].get('question_type_labels', []))):\n",
    "            qtype = qtype.lower()\n",
    "            metrics[qid][qtype]['num'] += 1\n",
    "            metrics[qid][qtype]['num_correct'] += human_correct\n",
    "        prompt_metrics = metrics[qid][model_stance]\n",
    "        prompt_metrics['num'] += 1\n",
    "        prompt_metrics['num_correct'] += human_correct\n",
    "        if model_stance == answer:\n",
    "            prompt_metrics['num_correct_with_correct_debate_mode'] += human_correct\n",
    "            prompt_metrics['num_correct_debate_mode'] += 1\n",
    "        else:\n",
    "            prompt_metrics['num_correct_with_incorrect_debate_mode'] += human_correct\n",
    "            prompt_metrics['num_incorrect_debate_mode'] += 1\n",
    "        prompt_metrics['num_debate_mode_responses'] += (prompt['response'] == model_stance)\n",
    "        \n",
    "        hit_durations.append(prompt['duration'] / 1000.)\n",
    "        response_option_counts[prompt['response']] += 1\n",
    "        responses.append(prompt['response'])\n",
    "    duration = np.sum(np.array(hit_durations))\n",
    "    worker_durations.append(duration)\n",
    "    durations += hit_durations\n",
    "    response_options_array = np.array(list(response_option_counts.values()))\n",
    "    response_options_array = response_options_array / response_options_array.sum()\n",
    "    max_response_freq = response_options_array.max()\n",
    "    max_response_freqs.append(max_response_freq)\n",
    "    acc = round(100 * hit_result['accuracy'][prompt_type])\n",
    "    print('| Time:', round(duration, 1),\n",
    "          '| Acc:', acc,\n",
    "          '| Max Freq:', round(100 * max_response_freq, 1),\n",
    "          '| Rate:', hit_result['task_rating'],\n",
    "          '| Feedback:', hit_result['feedback'],\n",
    "          '| Quote Rating:', None if 'quote_rating' not in hit_result else hit_result['quote_rating'], \n",
    "          '| Quote Desc:', None if 'quote_description' not in hit_result else hit_result['quote_description'])\n",
    "\n",
    "good_worker_durations = []\n",
    "assert len(worker_durations) == len(accuracy_by_worker[prompt_type])\n",
    "for worker_duration, worker_accuracy in zip(worker_durations, accuracy_by_worker[prompt_type]):\n",
    "    if worker_accuracy > np.median(np.array(accuracy_by_worker[prompt_type])):\n",
    "        good_worker_durations.append(worker_duration)\n",
    "\n",
    "quote_ratings = np.array(quote_ratings)\n",
    "durations = np.array(durations)\n",
    "worker_durations = np.array(worker_durations)\n",
    "good_worker_durations = np.array(good_worker_durations)\n",
    "max_response_freqs = np.array(max_response_freqs)\n",
    "\n",
    "quote_ratings.sort()\n",
    "durations.sort()\n",
    "worker_durations.sort()\n",
    "good_worker_durations.sort()\n",
    "max_response_freqs.sort()\n",
    "\n",
    "for qtype in accuracy_by_worker:\n",
    "    accuracy_by_worker[qtype] = np.array(accuracy_by_worker[qtype])\n",
    "    accuracy_by_worker[qtype].sort()\n",
    "print('REJECTED:', num_rejected_hits)\n",
    "print('INCOMPLETE:', num_incomplete_hits)\n",
    "print('VALID:', num_valid_hits)\n",
    "print('Median Question Duration:', np.median(durations))\n",
    "print('Mean Question Duration:', np.mean(durations[int(durations.shape[0] / 10.):int(9. * durations.shape[0] / 10.)]))\n",
    "print('Min/Median/Mean/Max Worker Duration:',\n",
    "      round(np.min(worker_durations / 60.), 2), '/',\n",
    "      round(np.median(worker_durations / 60.), 2), '/',\n",
    "      round(np.mean(worker_durations / 60.), 2), '/',\n",
    "      round(np.max(worker_durations / 60.), 2))\n",
    "print('Min/Median/Mean/Max Good Worker Durations:',\n",
    "      round(np.min(good_worker_durations / 60.), 2), '/',\n",
    "      round(np.median(good_worker_durations / 60.), 2),'/',\n",
    "      round(np.mean(good_worker_durations / 60.), 2), '/',\n",
    "      round(np.max(good_worker_durations / 60.), 2))\n",
    "print('Median Worker Accuracy:', np.median(accuracy_by_worker[prompt_type]))\n",
    "print('Median Max Response Freq:', np.median(max_response_freqs))\n",
    "print('Quote Rating:',\n",
    "      '| Mean:', round(quote_ratings.mean(), 2),\n",
    "      '| Median:', round(np.median(quote_ratings), 2),\n",
    "      '| Std:', round(np.std(quote_ratings), 2))\n",
    "# pprint(hit_results[0]['data'][0])\n",
    "# pprint(hit_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evals per sample: 6.4\n",
      "Fraction insuffient evals: 0.0\n",
      "Convinced: nan %\n",
      "- Correct debater: nan %\n",
      "- Incorrect debater: nan %\n",
      "Accuracy: 65.67 %\n",
      "- Correct debater: nan %\n",
      "- Incorrect debater: nan %\n",
      "Extra Evals: 21.88 %\n",
      "Evals per sample distribution: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "Accuracy/Num-Samples by Q Type:\n",
      "{'a': (0.7243551587301588, 24),\n",
      " 'b': (0.6207217261904762, 32),\n",
      " 'c': (0.726873897707231, 54),\n",
      " 'd': (0.6087376460017969, 53),\n",
      " 'e': (0.5747767857142858, 16)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:52: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:54: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:56: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:61: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:63: RuntimeWarning: Mean of empty slice.\n"
     ]
    }
   ],
   "source": [
    "accuracy_by_qtype = {qtype: [] for qtype in question_type_labels}\n",
    "accuracy_by_sample = []\n",
    "accuracy_by_sample_correct_debate_mode = []\n",
    "accuracy_by_sample_incorrect_debate_mode = []\n",
    "convinced_freqs = []\n",
    "convinced_freqs_with_correct_debate_mode = []\n",
    "convinced_freqs_with_incorrect_debate_mode = []\n",
    "num_evals_by_sample = []\n",
    "for qid, qid_metrics in metrics.items():\n",
    "    answer = metrics[qid]['answer']\n",
    "    for qid_metric_key, prompt in qid_metrics.items():\n",
    "        if qid_metric_key in question_type_labels:\n",
    "            qtype = qid_metric_key\n",
    "            if qid_metrics[qtype]['num'] > 0:\n",
    "                accuracy_by_qtype[qtype].append(qid_metrics[qtype]['num_correct'] / qid_metrics[qtype]['num'])\n",
    "            continue\n",
    "        if not (qid_metric_key in [None] + options):\n",
    "            continue\n",
    "        model_stance = qid_metric_key\n",
    "\n",
    "        # Q-only stats\n",
    "        prompt_metrics = metrics[qid][model_stance]\n",
    "        num_evals_by_sample.append(prompt_metrics['num'])\n",
    "        accuracy_by_sample.append(prompt_metrics['num_correct'] / prompt_metrics['num'])\n",
    "        if model_stance is None:\n",
    "            continue\n",
    "        \n",
    "        # Debater stats\n",
    "#         if 'num_debate_mode_responses' not in prompt_metrics:\n",
    "#             print(qid_metric_key, prompt_metrics)\n",
    "        convinced_freq = prompt_metrics['num_debate_mode_responses'] / prompt_metrics['num']\n",
    "        if model_stance == answer:\n",
    "            convinced_freqs_with_correct_debate_mode.append(convinced_freq)\n",
    "            accuracy_by_sample_correct_debate_mode.append(\n",
    "                prompt_metrics['num_correct_with_correct_debate_mode'] /\n",
    "                prompt_metrics['num_correct_debate_mode'])\n",
    "        else:\n",
    "            convinced_freqs_with_incorrect_debate_mode.append(convinced_freq)\n",
    "            accuracy_by_sample_incorrect_debate_mode.append(\n",
    "                prompt_metrics['num_correct_with_incorrect_debate_mode'] /\n",
    "                prompt_metrics['num_incorrect_debate_mode'])\n",
    "        convinced_freqs.append(convinced_freq)\n",
    "\n",
    "accuracy_by_qtype = {qtype: (np.array(accuracy_by_qtype[qtype]).mean(), len(accuracy_by_qtype[qtype])) for qtype in question_type_labels}\n",
    "worker_ids = set(worker_ids)\n",
    "        \n",
    "num_evals_by_sample = np.array(num_evals_by_sample)\n",
    "print('Evals per sample:', num_evals_by_sample.mean())\n",
    "print('Fraction insuffient evals:', (num_evals_by_sample < 5).mean())\n",
    "\n",
    "convinced_freqs = np.array(convinced_freqs)\n",
    "print('Convinced:', round(100 * convinced_freqs.mean(), 2), '%')\n",
    "convinced_freqs_with_correct_debate_mode = np.array(convinced_freqs_with_correct_debate_mode)\n",
    "print('- Correct debater:', round(100 * convinced_freqs_with_correct_debate_mode.mean(), 2), '%')\n",
    "convinced_freqs_with_incorrect_debate_mode = np.array(convinced_freqs_with_incorrect_debate_mode)\n",
    "print('- Incorrect debater:', round(100 * convinced_freqs_with_incorrect_debate_mode.mean(), 2), '%')\n",
    "\n",
    "accuracy_by_sample = np.array(accuracy_by_sample)\n",
    "print('Accuracy:', round(100 * accuracy_by_sample.mean(), 2), '%')\n",
    "accuracy_by_sample_correct_debate_mode = np.array(accuracy_by_sample_correct_debate_mode)\n",
    "print('- Correct debater:', round(100 * accuracy_by_sample_correct_debate_mode.mean(), 2), '%')\n",
    "accuracy_by_sample_incorrect_debate_mode = np.array(accuracy_by_sample_incorrect_debate_mode)\n",
    "print('- Incorrect debater:', round(100 * accuracy_by_sample_incorrect_debate_mode.mean(), 2), '%')\n",
    "\n",
    "num_target_evals = 5\n",
    "print('Extra Evals:', round(((100. * (num_evals_by_sample - num_target_evals).sum()) / num_evals_by_sample.sum()), 2), '%')\n",
    "num_evals_by_sample.sort()\n",
    "print('Evals per sample distribution:', num_evals_by_sample)\n",
    "\n",
    "print('Accuracy/Num-Samples by Q Type:')\n",
    "pprint(accuracy_by_qtype)\n",
    "# 1.5*3.1*60/(917.5684545454544*26/(20*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPS, Mean: (40.62, 8.25)\n"
     ]
    }
   ],
   "source": [
    "def nps(task_ratings):\n",
    "    num_ratings = sum(list(task_ratings.values()))\n",
    "    if num_ratings == 0:\n",
    "        return None\n",
    "\n",
    "    nps_sum_ratings = 0\n",
    "    sum_ratings = 0\n",
    "    for score, num_raters in task_ratings.items():\n",
    "        sum_ratings += num_raters * score\n",
    "        if score >= 9:\n",
    "            nps_sum_ratings += num_raters\n",
    "        elif score <= 6:\n",
    "            nps_sum_ratings -= num_raters\n",
    "    return round(100 * (nps_sum_ratings / float(num_ratings)), 2), round((sum_ratings / float(num_ratings)), 2)\n",
    "\n",
    "print('NPS, Mean:', nps(task_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(ttest_ind(convinced_freqs, convinced_freqs2, equal_var=False))\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

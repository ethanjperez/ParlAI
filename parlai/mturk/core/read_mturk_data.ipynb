{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from pprint import pprint\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "num_evals = 5\n",
    "evals = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# HIT Files: 110\n",
      "# Passed Test: 37 / 73 = 50.68 %\n"
     ]
    }
   ],
   "source": [
    "task_dir = '/Users/ethanperez/research/ParlAI/parlai/mturk/core/run_data/live/context_evaluator_'\n",
    "\n",
    "### RACE: Unfiltered Workers\n",
    "# prompt_type, task_id = 'question', '1553982706'  # Q-only\n",
    "# prompt_type, task_id = 'context_question', 1553790696  # TFIDF\n",
    "# prompt_type, task_id = 'context_question', 1553901953  # FastText\n",
    "### RACE: Filtered Workers\n",
    "# prompt_type, task_id = 'question', '1554052233'  # Q-only\n",
    "# prompt_type, task_id = 'quote and question', 1554006689  # TFIDF-QA\n",
    "# prompt_type, task_id = 'quote and question', 1554130485  # TFIDF-A\n",
    "# prompt_type, task_id = 'quote and question', 1554069931  # Cross-Ranker\n",
    "# prompt_type, task_id = 'quote and question', 1554072277  # Predicting Search\n",
    "# prompt_type, task_id = 'quote and question', 1554132868  # Predicting ΔP(A)\n",
    "\n",
    "### RACE Test\n",
    "## Persuading\n",
    "# prompt_type, task_id, name = 'quote and question', 1556671432, 'TFIDF-QA'\n",
    "# prompt_type, task_id, name = 'quote and question', 1556725767, 'TFIDF-A'\n",
    "# prompt_type, task_id, name = 'quote and question', 1556739336, 'BoW-A'\n",
    "# prompt_type, task_id, name = 'quote and question', 1556756789, 'Pred ΔP(A)'  # (race.m=sl-sents.i.best.e)\n",
    "# prompt_type, task_id, name = 'quote and question', 1556809031, 'Pred P(A)'\n",
    "# prompt_type, task_id, name = 'quote and question', 1556832343, 'Pred Search (Almost complete)'\n",
    "# prompt_type, task_id, name = 'quote and question', 1556892630, 'Pred Search'\n",
    "## Acc on Summary\n",
    "# prompt_type, task_id, name = 'passage and question', 1555823963, 'Full Passage'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1555946909, 'BoW-A'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1555952058, 'Cross-Ranker Best Epoch'  # (6-10 sentence incorrectly placed at end)\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556939750, 'Pred ΔP(A) (lower pay)'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556977072, 'Pred P(A)'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556987177, 'Pred Search'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556999857, 'Pred ΔP(A)'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1557085110, 'Cross-Ranker'  # Last Epoch\n",
    "prompt_type, task_id, name = 'quotes and question', 1557093902, 'Bi-Ranker'\n",
    "dataset = 'race'\n",
    "\n",
    "### DREAM\n",
    "## Persuading\n",
    "# prompt_type, task_id, name = 'question', 1554582693, 'Q-only'\n",
    "# prompt_type, task_id, name = 'quote and question', 1554596686, 'TFIDF-QA'\n",
    "# prompt_type, task_id, name = 'quote and question', 1554587404, 'TFIDF-A'\n",
    "# prompt_type, task_id, name = 'quote and question', 1554662280, 'BoW-A'\n",
    "# prompt_type, task_id, name = 'quote and question', 1556670413, 'Bi-Ranker'\n",
    "# prompt_type, task_id, name = 'quote and question', 1554675304, 'Cross-Ranker'\n",
    "# prompt_type, task_id, name = 'quote and question', 1554685131, 'Pred Search'\n",
    "# prompt_type, task_id, name = 'quote and question', 1554692472, 'Pred P(A)'\n",
    "# prompt_type, task_id, name = 'quote and question', 1554729998, 'Pred ΔP(A)'\n",
    "# prompt_type, task_id, name = 'quote and question', 1555333992, 'Pred Search (ToM)'\n",
    "## Acc on Summary\n",
    "# prompt_type, task_id, name = 'question, answers, and quotes', 1555707929, 'TFIDF-A'  # 64.21%: (Less filter / no feedback)\n",
    "# prompt_type, task_id, name = 'question, answers, and quotes', 1555722489, 'Cross-Ranker'  # 65.38%: (Less filter / no feedback)\n",
    "# prompt_type, task_id, name = 'question and quotes', 1555789302, 'Pred Search'  # 75.17% (4/5 filter)\n",
    "# prompt_type, task_id, name = 'question and quotes', 1555812443, 'Pred Search'  # 79.32% Actually: quotes and question (4/5 filter)\n",
    "# prompt_type, task_id, name = 'passage and question', 1555804551, 'Full Passage'  # 92.97%\n",
    "# prompt_type, task_id, name = 'quotes and question', 1555823257, 'FastText'  # (5/5 filter) (77.33%)\n",
    "# prompt_type, task_id, name = 'quotes and question', 1555946647, 'RACE Cross-Ranker'  # (4 sentences incorrectly placed at end) (80.84%)\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556727396, 'Cross-Ranker'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556740293, 'Bi-Ranker'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556757043, 'TFIDF-A'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556811067, 'TFIDF-QA'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556832115, 'Predicting Search'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556892896, 'Predicting P(A)'\n",
    "# prompt_type, task_id, name = 'quotes and question', 1556938429, 'Predicting ΔP(A)'\n",
    "# dataset = 'dream'\n",
    "\n",
    "\n",
    "split = None  # 'middle', 'high', None\n",
    "\n",
    "\n",
    "# Set useful variables\n",
    "task_dir += str(task_id)\n",
    "if dataset != 'race':\n",
    "    split = None\n",
    "num_options = 3 if dataset == 'dream' else 4\n",
    "options = ['A', 'B', 'C', 'D'][:num_options]\n",
    "debate_mode_to_option = {'Ⅰ': 'A', 'Ⅱ': 'B', 'Ⅲ': 'C', 'Ⅳ': 'D', 'ⅰ': 'A', 'ⅱ': 'B', 'ⅲ': 'C', 'ⅳ': 'D', None: None}\n",
    "question_type_labels = ['a', 'c', 'l', 'm', 's'] if dataset == 'dream' else ['a', 'b', 'c', 'd', 'e']\n",
    "\n",
    "# if (dataset == 'dream') and (prompt_type == 'quote and question'):\n",
    "#     question_type_labels = []\n",
    "\n",
    "# Read HIT data\n",
    "print('# HIT Files:', len(os.listdir(task_dir)))\n",
    "hit_results = []\n",
    "num_passed_test = 0\n",
    "for hit_dir in os.listdir(task_dir):\n",
    "    if hit_dir.startswith('o_'):\n",
    "        continue\n",
    "    num_passed_test += 1\n",
    "    with open(os.path.join(task_dir, hit_dir, 'custom/data.json'), 'r') as file:\n",
    "        hit_results.append(json.load(file))\n",
    "        file.close()\n",
    "if len(os.listdir(task_dir)) != num_passed_test:\n",
    "    num_total_tested = len(os.listdir(task_dir)) - num_passed_test\n",
    "    print('# Passed Test:', num_passed_test, '/', num_total_tested, '=', round((100. * num_passed_test) / num_total_tested, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Time: 588.3 | Acc: 50 | Max Freq: 30.0 | Rate: 10 | Feedback: Try to choose passages that are more clear and concise | Quote Rating: 7 | Quote Desc: I think some were random, but most of them explained things well\n",
      "| Time: 1459.8 | Acc: 75 | Max Freq: 35.0 | Rate: 8 | Feedback: Don't know | Quote Rating: 8 | Quote Desc: Interesting\n",
      "| Time: 1073.9 | Acc: 50 | Max Freq: 35.0 | Rate: 10 | Feedback: Maybe provide an additional one word clue for each question | Quote Rating: 7 | Quote Desc: Some of the provided quotes made some answers obvious but there were a few questions that I felt like not one choice was right so I had to guess.\n",
      "| Time: 403.2 | Acc: 55 | Max Freq: 35.0 | Rate: 8 | Feedback: Make the quotes better | Quote Rating: 5 | Quote Desc: They were very vague and kind of unhelpful\n",
      "| Time: 635.5 | Acc: 75 | Max Freq: 45.0 | Rate: 7 | Feedback: some quotes gave no clues for which answer was correct | Quote Rating: 6 | Quote Desc: vague and sometimes off topic\n",
      "| Time: 671.2 | Acc: 75 | Max Freq: 40.0 | Rate: 9 | Feedback: Provide less vague quotes | Quote Rating: 7 | Quote Desc: Some were really vague and made me guess. Others really helped me find the answer.\n",
      "| Time: 684.6 | Acc: 50 | Max Freq: 30.0 | Rate: 10 | Feedback: Not sure you can, the pay is good, although I'm not sure if the $3.10 is the base pay and then you get a bonus, or the money is up to $3.10. If you can earn up to $3.10, then the HIT reward is misleading and you should definitely change that to say whatever the base pay is and then explain there's an option to earn up to $x amount more. | Quote Rating: 5 | Quote Desc: Some of them had pretty much zero clue as to what the correct answer was, so I was just winging it. Some had a tiny clue and others it was very obvious. It really was a mixed bag of how detailed and useful the passages were.\n",
      "| Time: 535.2 | Acc: 55 | Max Freq: 35.0 | Rate: 7 | Feedback: make all passages more complete so that determining the most correct answer will be easier | Quote Rating: 7 | Quote Desc: some gave enough context; others were incomplete and therefore it was difficult to determine the most correct answer\n",
      "| Time: 498.2 | Acc: 55 | Max Freq: 35.0 | Rate: 4 | Feedback: IDK. | Quote Rating: 2 | Quote Desc: Intentionally confusing and concise; the quotes did not provide information helpful to applying critical thinking skills.\n",
      "| Time: 384.7 | Acc: 75 | Max Freq: 30.0 | Rate: 8 | Feedback: N/A- it was easy to understand and execute | Quote Rating: 7 | Quote Desc: Mostly they were helpful because I could infer which information was most likely to be correct.  Occasionally, though, they seemed to have nothing to do with the answers and I had to make a random guess\n",
      "| Time: 469.7 | Acc: 45 | Max Freq: 40.0 | Rate: 9 | Feedback: Having passages that overlap with the questions more in some cases. | Quote Rating: 8 | Quote Desc: Some were very useful, others had no information related to the question.\n",
      "| Time: 1382.1 | Acc: 35 | Max Freq: 35.0 | Rate: 8 | Feedback: Learn Extra | Quote Rating: 10 | Quote Desc: Its very good\n",
      "| Time: 927.3 | Acc: 70 | Max Freq: 30.0 | Rate: 9 | Feedback: I  enjoyed it as it was. | Quote Rating: 8 | Quote Desc: They were some that is was very hard to extract the information from.\n",
      "| Time: 833.7 | Acc: 80 | Max Freq: 35.0 | Rate: 9 | Feedback: By making them clearer | Quote Rating: 10 | Quote Desc: Interesting\n",
      "| Time: 1022.2 | Acc: 50 | Max Freq: 30.0 | Rate: 10 | Feedback: it was pretty good, but maybe stick to whats in the passage instead of being tricky | Quote Rating: 8 | Quote Desc: some where tricky , i read and read them over and over to make sure i  was right \n",
      "| Time: 1055.3 | Acc: 65 | Max Freq: 35.0 | Rate: 10 | Feedback: Perhaps change some of the quotes that more difficult to determine and answer to.  | Quote Rating: 8 | Quote Desc: I think some of them were easy to infer and some quotes were straightforward. Some of the quotes also didn't do a great job at helping to infer the answers. But overall, the quotes were helpful. \n",
      "| Time: 385.4 | Acc: 35 | Max Freq: 40.0 | Rate: 0 | Feedback: By having it make sense. | Quote Rating: 0 | Quote Desc: Barely related to the answers. Basically useless.\n",
      "| Time: 1800.3 | Acc: 55 | Max Freq: 35.0 | Rate: 8 | Feedback: It does not need improvement | Quote Rating: 5 | Quote Desc: Sometimes useful sometimes nonsensical\n",
      "| Time: 1195.7 | Acc: 50 | Max Freq: 45.0 | Rate: 8 | Feedback: By making it a bit less vague and making it a bit more specific on the quotes to gelp anwser some questions more easily. | Quote Rating: 4 | Quote Desc: They are very vague but it is designed to make you think.\n",
      "| Time: 991.2 | Acc: 70 | Max Freq: 35.0 | Rate: 7 | Feedback: Make sure all questions can be gotten correct from the passage excerpts. | Quote Rating: 7 | Quote Desc: Most gave good to decent context clues, however, many gave zero clue as to what the answer was. \n",
      "| Time: 935.5 | Acc: 50 | Max Freq: 55.0 | Rate: 5 | Feedback: I don't know. | Quote Rating: 5 | Quote Desc: sometimes they were helpful, sometimes they weren't.\n",
      "| Time: 456.9 | Acc: 55 | Max Freq: 30.0 | Rate: 10 | Feedback: No way to improve.  It is very good.  | Quote Rating: 5 | Quote Desc: They are good but hard. \n",
      "| Time: 784.2 | Acc: 60 | Max Freq: 40.0 | Rate: 10 | Feedback: im not sure but the task is very fun i would say just keep it lighthearted and fun | Quote Rating: 5 | Quote Desc: some were very helpful and some were completely useless when trying to answer the questions\n",
      "| Time: 802.6 | Acc: 50 | Max Freq: 45.0 | Rate: 5 | Feedback: I guess you could make some of the passages less cryptic and a little more understandable when it relates to the question beneath it. Other then that I thought the task was fine for what it was. It was just a few passages that I thought was unfair, others where I got the answer wrong became obvious after the correct answer was given. | Quote Rating: 5 | Quote Desc: Some of them were useful and gave the the information I needed to pick the correct answer. Others had small hints that weren't obvious unless you really thought about it, then it was easy enough to figure out what the answer was. The rest, which was a few, were just nonsense to me and I couldn't figure out the answer for the life of me. And when I say nonsense, I mean that I couldn't figure out how the answer was related to the passage quotes given, it seemed totally random. I hope I explained that last part well.\n",
      "| Time: 575.2 | Acc: 60 | Max Freq: 40.0 | Rate: 8 | Feedback: Make sure the answer to the question is in the passage provided. | Quote Rating: 6 | Quote Desc: Sometimes the incorrect part of the passage is included or the passage is incomplete. It is difficult to answer the question when the passage is not relevant or complete.\n",
      "| Time: 409.4 | Acc: 55 | Max Freq: 40.0 | Rate: 9 | Feedback: Provide more clues in the quotes | Quote Rating: 5 | Quote Desc: Vague at times\n",
      "| Time: 825.1 | Acc: 60 | Max Freq: 40.0 | Rate: 9 | Feedback: Some of the quotes are just really difficult to infer from. | Quote Rating: 8 | Quote Desc: Informative\n",
      "| Time: 974.0 | Acc: 65 | Max Freq: 35.0 | Rate: 5 | Feedback: Ask less questions. | Quote Rating: 7 | Quote Desc: Some were somewhat ambiguous.  Most were moderately easy to figure out.\n",
      "| Time: 423.2 | Acc: 55 | Max Freq: 30.0 | Rate: 8 | Feedback: i'm not sure because I don't know the purpose | Quote Rating: 4 | Quote Desc: they were mostly vague\n",
      "| Time: 2500.4 | Acc: 80 | Max Freq: 35.0 | Rate: 9 | Feedback: For about 3 or 4 of the questions, it seemed not quite enough information was provided in the quotes. Most of them were perfect though. | Quote Rating: 8 | Quote Desc: The majority of them were easy enough to decipher once reading them over a couple of times and then thinking about each possible answer. However, a few of the questions and correct answers left me scratching my head in confusion.\n",
      "| Time: 470.7 | Acc: 55 | Max Freq: 40.0 | Rate: 5 | Feedback: Choosing answers with at least some relevancy. | Quote Rating: 4 | Quote Desc: Often had nothing to do with the answers.\n",
      "REJECTED: 0\n",
      "INCOMPLETE: 6\n",
      "VALID: 31\n",
      "Median Question Duration: 29.5155\n",
      "Mean Question Duration: 34.10094354838709\n",
      "Min/Median/Mean/Max Worker Duration: 6.41 / 13.07 / 14.06 / 41.67\n",
      "Min/Median/Mean/Max Good Worker Durations: 6.41 / 13.9 / 16.18 / 41.67\n",
      "Median Worker Accuracy: 0.55\n",
      "Median Max Response Freq: 0.35\n",
      "Quote Rating: | Mean: 6.16 | Median: 7.0 | Std: 2.11\n"
     ]
    }
   ],
   "source": [
    "num_valid_hits, num_rejected_hits, num_incomplete_hits = 0, 0, 0\n",
    "metrics = {}\n",
    "task_ratings = {i: 0 for i in range(11)}\n",
    "quote_ratings = []\n",
    "durations = []\n",
    "worker_durations = []\n",
    "accuracy_by_worker = {}\n",
    "max_response_freqs = []\n",
    "worker_ids = []\n",
    "hits_by_qid = {}\n",
    "\n",
    "for hit_result in hit_results:\n",
    "    if ((len(hit_result['reject_reasons']) > 0) or\n",
    "        (len(hit_result['block_reasons']) > 0)):\n",
    "        num_rejected_hits += 1\n",
    "        print(hit_result['worker_id'], hit_result['assignment_id'],\n",
    "              '| reject_reasons:', hit_result['reject_reasons'],\n",
    "              '| block_reasons:', hit_result['block_reasons'],\n",
    "              '| bonus_reasons: ' + str(hit_result['bonus_reasons']) if 'bonus_reasons' in hit_result else '')\n",
    "    elif hit_result['feedback'] is None:\n",
    "        num_incomplete_hits += 1\n",
    "        continue\n",
    "    \n",
    "    worker_ids.append(hit_result['worker_id'])\n",
    "    num_valid_hits += 1\n",
    "    if (hit_result['task_rating'] is not None) and (hit_result['task_rating'].isdigit()):\n",
    "        task_ratings[int(hit_result['task_rating'])] += 1\n",
    "    if (hit_result.get('quote_rating') is not None) and (hit_result['quote_rating'].isdigit()):\n",
    "        quote_ratings.append(int(hit_result['quote_rating']))\n",
    "    for qtype, qtype_accuracy in hit_result['accuracy'].items():\n",
    "        accuracy_by_worker[qtype] = accuracy_by_worker.get(qtype, []) + [qtype_accuracy]\n",
    "    \n",
    "    hit_durations = []\n",
    "    response_option_counts = {option: 0 for option in options}\n",
    "    responses = []\n",
    "    for prompt in hit_result['data']:\n",
    "        qid = prompt['sample']['qid']\n",
    "        if (split is not None) and (split not in qid):\n",
    "            continue\n",
    "        hits_by_qid[qid] = hits_by_qid.get(qid, [])\n",
    "        hits_by_qid[qid].append(prompt)\n",
    "        model_stance = debate_mode_to_option[prompt['sample']['debate_mode']]\n",
    "        answer = prompt['sample']['eval_labels'][0]\n",
    "        human_correct = (prompt['response'] == answer)\n",
    "        assert answer in options, 'Answer must be in options.'\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if qid not in metrics:\n",
    "            metrics[qid] = {\n",
    "                option: {\n",
    "                    'num': 0,\n",
    "                    'num_correct': 0,\n",
    "                    'num_correct_debate_mode': 0,\n",
    "                    'num_incorrect_debate_mode': 0,\n",
    "                    'num_correct_with_correct_debate_mode': 0,\n",
    "                    'num_correct_with_incorrect_debate_mode': 0,\n",
    "                    'num_debate_mode_responses': 0,\n",
    "                    'is_debate_mode_response': []\n",
    "                }\n",
    "                for option in ([None] if model_stance is None else options)\n",
    "            }\n",
    "            for qtype in question_type_labels:\n",
    "                metrics[qid][qtype] = {\n",
    "                    'num': 0,\n",
    "                    'num_correct': 0,\n",
    "                }\n",
    "            metrics[qid]['answer'] = answer\n",
    "        for qtype in set(''.join(prompt['sample'].get('question_type_labels', []))):\n",
    "            qtype = qtype.lower()\n",
    "            metrics[qid][qtype]['num'] += 1\n",
    "            metrics[qid][qtype]['num_correct'] += human_correct\n",
    "        prompt_metrics = metrics[qid][model_stance]\n",
    "        prompt_metrics['num'] += 1\n",
    "        prompt_metrics['num_correct'] += human_correct\n",
    "        if model_stance == answer:\n",
    "            prompt_metrics['num_correct_with_correct_debate_mode'] += human_correct\n",
    "            prompt_metrics['num_correct_debate_mode'] += 1\n",
    "        else:\n",
    "            prompt_metrics['num_correct_with_incorrect_debate_mode'] += human_correct\n",
    "            prompt_metrics['num_incorrect_debate_mode'] += 1\n",
    "        prompt_metrics['num_debate_mode_responses'] += (prompt['response'] == model_stance)\n",
    "        prompt_metrics['is_debate_mode_response'].append(prompt['response'] == model_stance)\n",
    "        \n",
    "        hit_durations.append(prompt['duration'] / 1000.)\n",
    "        response_option_counts[prompt['response']] += 1\n",
    "        responses.append(prompt['response'])\n",
    "    duration = np.sum(np.array(hit_durations))\n",
    "    worker_durations.append(duration)\n",
    "    durations += hit_durations\n",
    "    response_options_array = np.array(list(response_option_counts.values()))\n",
    "    response_options_array = response_options_array / response_options_array.sum()\n",
    "    max_response_freq = response_options_array.max()\n",
    "    max_response_freqs.append(max_response_freq)\n",
    "    acc = round(100 * hit_result['accuracy'][prompt_type])\n",
    "    print('| Time:', round(duration, 1),\n",
    "          '| Acc:', acc,\n",
    "          '| Max Freq:', round(100 * max_response_freq, 1),\n",
    "          '| Rate:', hit_result['task_rating'],\n",
    "          '| Feedback:', hit_result['feedback'],\n",
    "          '| Quote Rating:', None if 'quote_rating' not in hit_result else hit_result['quote_rating'], \n",
    "          '| Quote Desc:', None if 'quote_description' not in hit_result else hit_result['quote_description'])\n",
    "\n",
    "good_worker_durations = []\n",
    "assert len(worker_durations) == len(accuracy_by_worker[prompt_type])\n",
    "for worker_duration, worker_accuracy in zip(worker_durations, accuracy_by_worker[prompt_type]):\n",
    "    if worker_accuracy > np.median(np.array(accuracy_by_worker[prompt_type])):\n",
    "        good_worker_durations.append(worker_duration)\n",
    "\n",
    "quote_ratings = np.array(quote_ratings)\n",
    "durations = np.array(durations)\n",
    "worker_durations = np.array(worker_durations)\n",
    "good_worker_durations = np.array(good_worker_durations)\n",
    "max_response_freqs = np.array(max_response_freqs)\n",
    "\n",
    "quote_ratings.sort()\n",
    "durations.sort()\n",
    "worker_durations.sort()\n",
    "good_worker_durations.sort()\n",
    "max_response_freqs.sort()\n",
    "\n",
    "for qtype in accuracy_by_worker:\n",
    "    accuracy_by_worker[qtype] = np.array(accuracy_by_worker[qtype])\n",
    "    accuracy_by_worker[qtype].sort()\n",
    "print('REJECTED:', num_rejected_hits)\n",
    "print('INCOMPLETE:', num_incomplete_hits)\n",
    "print('VALID:', num_valid_hits)\n",
    "print('Median Question Duration:', np.median(durations))\n",
    "print('Mean Question Duration:', np.mean(durations[int(durations.shape[0] / 10.):int(9. * durations.shape[0] / 10.)]))\n",
    "print('Min/Median/Mean/Max Worker Duration:',\n",
    "      round(np.min(worker_durations / 60.), 2), '/',\n",
    "      round(np.median(worker_durations / 60.), 2), '/',\n",
    "      round(np.mean(worker_durations / 60.), 2), '/',\n",
    "      round(np.max(worker_durations / 60.), 2))\n",
    "print('Min/Median/Mean/Max Good Worker Durations:',\n",
    "      round(np.min(good_worker_durations / 60.), 2), '/',\n",
    "      round(np.median(good_worker_durations / 60.), 2),'/',\n",
    "      round(np.mean(good_worker_durations / 60.), 2), '/',\n",
    "      round(np.max(good_worker_durations / 60.), 2))\n",
    "print('Median Worker Accuracy:', np.median(accuracy_by_worker[prompt_type]))\n",
    "print('Median Max Response Freq:', np.median(max_response_freqs))\n",
    "print('Quote Rating:',\n",
    "      '| Mean:', round(quote_ratings.mean(), 2),\n",
    "      '| Median:', round(np.median(quote_ratings), 2),\n",
    "      '| Std:', round(np.std(quote_ratings), 2))\n",
    "# pprint(hit_results[0]['data'][0])\n",
    "# pprint(hit_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evals per sample: 6.2\n",
      "Fraction insuffient evals: 0.0\n",
      "Convinced: nan %\n",
      "- Correct debater: nan %\n",
      "- Incorrect debater: nan %\n",
      "Accuracy: 58.25 %\n",
      "- Correct debater: nan %\n",
      "- Incorrect debater: nan %\n",
      "Extra Evals: 19.35 %\n",
      "Evals per sample distribution: [5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6\n",
      " 6 6 6 6 6 6 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "Accuracy/Num-Samples by Q Type:\n",
      "{'a': (0.49444444444444446, 24),\n",
      " 'b': (0.5919270833333334, 32),\n",
      " 'c': (0.6166666666666666, 54),\n",
      " 'd': (0.629874213836478, 53),\n",
      " 'e': (0.5359375, 16)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:54: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:56: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:58: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:63: RuntimeWarning: Mean of empty slice.\n",
      "/anaconda3/envs/main/lib/python3.6/site-packages/ipykernel_launcher.py:65: RuntimeWarning: Mean of empty slice.\n"
     ]
    }
   ],
   "source": [
    "accuracy_by_qtype = {qtype: [] for qtype in question_type_labels}\n",
    "accuracy_by_sample = []\n",
    "accuracy_by_sample_correct_debate_mode = []\n",
    "accuracy_by_sample_incorrect_debate_mode = []\n",
    "convinced_freqs = []\n",
    "convinced_freqs_with_correct_debate_mode = []\n",
    "convinced_freqs_with_incorrect_debate_mode = []\n",
    "num_evals_by_sample = []\n",
    "convinced_by_sample = []\n",
    "for qid, qid_metrics in metrics.items():\n",
    "    answer = metrics[qid]['answer']\n",
    "    for qid_metric_key, prompt in qid_metrics.items():\n",
    "        if qid_metric_key in question_type_labels:\n",
    "            qtype = qid_metric_key\n",
    "            if qid_metrics[qtype]['num'] > 0:\n",
    "                accuracy_by_qtype[qtype].append(qid_metrics[qtype]['num_correct'] / qid_metrics[qtype]['num'])\n",
    "            continue\n",
    "        if not (qid_metric_key in [None] + options):\n",
    "            continue\n",
    "        model_stance = qid_metric_key\n",
    "\n",
    "        # Q-only stats\n",
    "        prompt_metrics = metrics[qid][model_stance]\n",
    "        num_evals_by_sample.append(prompt_metrics['num'])\n",
    "        convinced_by_sample.append(prompt_metrics['is_debate_mode_response'])\n",
    "        accuracy_by_sample.append(prompt_metrics['num_correct'] / prompt_metrics['num'])\n",
    "        if model_stance is None:\n",
    "            continue\n",
    "        \n",
    "        # Debater stats\n",
    "#         if 'num_debate_mode_responses' not in prompt_metrics:\n",
    "#             print(qid_metric_key, prompt_metrics)\n",
    "        convinced_freq = prompt_metrics['num_debate_mode_responses'] / prompt_metrics['num']\n",
    "        if model_stance == answer:\n",
    "            convinced_freqs_with_correct_debate_mode.append(convinced_freq)\n",
    "            accuracy_by_sample_correct_debate_mode.append(\n",
    "                prompt_metrics['num_correct_with_correct_debate_mode'] /\n",
    "                prompt_metrics['num_correct_debate_mode'])\n",
    "        else:\n",
    "            convinced_freqs_with_incorrect_debate_mode.append(convinced_freq)\n",
    "            accuracy_by_sample_incorrect_debate_mode.append(\n",
    "                prompt_metrics['num_correct_with_incorrect_debate_mode'] /\n",
    "                prompt_metrics['num_incorrect_debate_mode'])\n",
    "        convinced_freqs.append(convinced_freq)\n",
    "\n",
    "accuracy_by_qtype = {qtype: (np.array(accuracy_by_qtype[qtype]).mean(), len(accuracy_by_qtype[qtype])) for qtype in question_type_labels}\n",
    "worker_ids = set(worker_ids)\n",
    "        \n",
    "num_evals_by_sample = np.array(num_evals_by_sample)\n",
    "print('Evals per sample:', num_evals_by_sample.mean())\n",
    "print('Fraction insuffient evals:', (num_evals_by_sample < 5).mean())\n",
    "\n",
    "convinced_freqs = np.array(convinced_freqs)\n",
    "print('Convinced:', round(100 * convinced_freqs.mean(), 2), '%')\n",
    "convinced_freqs_with_correct_debate_mode = np.array(convinced_freqs_with_correct_debate_mode)\n",
    "print('- Correct debater:', round(100 * convinced_freqs_with_correct_debate_mode.mean(), 2), '%')\n",
    "convinced_freqs_with_incorrect_debate_mode = np.array(convinced_freqs_with_incorrect_debate_mode)\n",
    "print('- Incorrect debater:', round(100 * convinced_freqs_with_incorrect_debate_mode.mean(), 2), '%')\n",
    "\n",
    "accuracy_by_sample = np.array(accuracy_by_sample)\n",
    "print('Accuracy:', round(100 * accuracy_by_sample.mean(), 2), '%')\n",
    "accuracy_by_sample_correct_debate_mode = np.array(accuracy_by_sample_correct_debate_mode)\n",
    "print('- Correct debater:', round(100 * accuracy_by_sample_correct_debate_mode.mean(), 2), '%')\n",
    "accuracy_by_sample_incorrect_debate_mode = np.array(accuracy_by_sample_incorrect_debate_mode)\n",
    "print('- Incorrect debater:', round(100 * accuracy_by_sample_incorrect_debate_mode.mean(), 2), '%')\n",
    "\n",
    "num_target_evals = 5\n",
    "print('Extra Evals:', round(((100. * (num_evals_by_sample - num_target_evals).sum()) / num_evals_by_sample.sum()), 2), '%')\n",
    "num_evals_by_sample.sort()\n",
    "print('Evals per sample distribution:', num_evals_by_sample)\n",
    "\n",
    "print('Accuracy/Num-Samples by Q Type:')\n",
    "pprint(accuracy_by_qtype)\n",
    "# 1.5*3.1*60/(917.5684545454544*26/(20*60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NPS, Mean: (25.81, 7.81)\n"
     ]
    }
   ],
   "source": [
    "def nps(task_ratings):\n",
    "    num_ratings = sum(list(task_ratings.values()))\n",
    "    if num_ratings == 0:\n",
    "        return None\n",
    "\n",
    "    nps_sum_ratings = 0\n",
    "    sum_ratings = 0\n",
    "    for score, num_raters in task_ratings.items():\n",
    "        sum_ratings += num_raters * score\n",
    "        if score >= 9:\n",
    "            nps_sum_ratings += num_raters\n",
    "        elif score <= 6:\n",
    "            nps_sum_ratings -= num_raters\n",
    "    return round(100 * (nps_sum_ratings / float(num_ratings)), 2), round((sum_ratings / float(num_ratings)), 2)\n",
    "\n",
    "print('NPS, Mean:', nps(task_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Bi-Ranker': array([0., 0., 0., 0., 0.]),\n",
      " 'Cross-Ranker': array([0., 0., 0., 0., 0.])}\n"
     ]
    }
   ],
   "source": [
    "evals[name] = []\n",
    "for eval_no in range(num_evals):\n",
    "    evals[name].append([convinced_array[eval_no] for convinced_array in convinced_by_sample])\n",
    "\n",
    "evals[name] = 100. * np.array(evals[name]).mean(axis=1)\n",
    "pprint(evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Cross-Ranker *****\n",
      "MEAN: 0.0\n",
      "STD: 0.0\n",
      "STDERR: 0.0\n",
      "\n",
      "***** Bi-Ranker *****\n",
      "MEAN: 0.0\n",
      "STD: 0.0\n",
      "STDERR: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n, eval_values in evals.items():\n",
    "    print('*****', n, '*****')\n",
    "    print('MEAN:', round(eval_values.mean(), 2))\n",
    "    print('STD:', round(eval_values.std(), 2))\n",
    "    print('STDERR:', round(eval_values.std() / np.sqrt(eval_values.shape[0]), 2))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Ranker / Cross-Ranker : nan\n"
     ]
    }
   ],
   "source": [
    "for n1, eval_values1 in evals.items():\n",
    "    for n2, eval_values2 in evals.items():\n",
    "        print(n1, '/', n2, ':', round(ttest_ind(eval_values1, eval_values2, equal_var=False)[1], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
